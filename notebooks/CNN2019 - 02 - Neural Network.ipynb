{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"CNN2019 - 02 - Neural Network.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fqiFD42_92oE","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","\n","# Task 2 - Artificial Neural Network\n","\n","\n","In this assignment you will practice putting together a simple image classification pipeline, based on the Two-layer Neural Network classifier. The goals of this assignment are as follows:\n","* understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)\n","* understand the train/val/test splits and the use of validation data for hyperparameter tuning.\n","* develop proficiency in writing efficient vectorized code with numpy\n","\n","* implement and apply various activation functions\n","* implement and apply hyperparameter finetuning strategy\n","\n","* understand the differences and tradeoffs between these classifiers\n","* the use of feature extraction to boost neural net performance\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"JLQflrey92oM","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.set_printoptions(precision=7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bLZgxpOj92of","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"DdVpN5a392on","colab_type":"code","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = 123456\n","Nama = \"\"\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oPQaktKa92oy","colab_type":"text"},"source":["# [Part 1] Simple One-Layer Neural Network"]},{"cell_type":"markdown","metadata":{"id":"H-w7dUV-92o0","colab_type":"text"},"source":["## 1 - Load Dataset\n","\n","For this exercise, we will use binary class data to recognize `cats` and `not cats`. \n","The images are $64\\times64$ in dimension.\n"]},{"cell_type":"markdown","metadata":{"id":"c_mXeRo_92o2","colab_type":"text"},"source":["---\n","### a. Load Cat Dataset\n","first, load the dataset\n"]},{"cell_type":"code","metadata":{"id":"1PAp0FCP92o5","colab_type":"code","colab":{}},"source":["import h5py    \n","    \n","def load_dataset():\n","    !wget 'https://raw.githubusercontent.com/cnn-adf/Task2019/master/resources/catvnoncat.h5'\n","    dataset = h5py.File('catvnoncat.h5', \"r\")\n","\n","    train_set_x_orig = np.array(dataset[\"X_train\"][:]) # your train set features\n","    train_set_y_orig = np.array(dataset[\"y_train\"][:]) # your train set labels\n","    val_set_x_orig = np.array(dataset[\"X_val\"][:]) # your val set features\n","    val_set_y_orig = np.array(dataset[\"y_val\"][:]) # your val set labels\n","    classes = np.array(dataset[\"classes\"][:]) # the list of classes\n","    \n","    return train_set_x_orig, train_set_y_orig, val_set_x_orig, val_set_y_orig, classes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fliDuquz92o-","colab_type":"code","colab":{}},"source":["X_train_ori, y_train, X_val_ori, y_val, classes = load_dataset()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KzllPE1-92pK","colab_type":"code","colab":{}},"source":["print(X_train_ori.shape)\n","print(y_train.shape)\n","print(X_val_ori.shape)\n","print(y_val.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rlJTqCwf92pb","colab_type":"text"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","(209, 64, 64, 3)\n","(209, 1)\n","(50, 64, 64, 3)\n","(50, 1)"]},{"cell_type":"markdown","metadata":{"id":"FTP4J31iBS9G","colab_type":"text"},"source":["View some data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"xF0d5gc092pf","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(18,5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train_ori[i+j*10])\n","        ax[j,i].set_title(classes[y_train[i+j*10,0]].decode(\"utf-8\") )\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qG_vGaXa92pq","colab_type":"text"},"source":["---\n","### b. Reshape and Normalize Data\n","<br>\n","\n","<font color='red'>**EXERCISE**: </font>\n","* Reshape `X_train_ori` and `X_val_ori` into 1-dimensional matrix, \n","* Store it as `X_train` and `X_val`\n","* the `X_train_ori` and `X_val_ori` shape should still be `(209, 64, 64, 3)` and `(50, 64, 64, 3)`\n","\n","<br>\n","\n","*Hint: use `np.reshape()`*"]},{"cell_type":"code","metadata":{"id":"TBZDD3K392pt","colab_type":"code","colab":{}},"source":["X_train = ??\n","X_val = ??"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggTFmgxc92p4","colab_type":"code","colab":{}},"source":["print('before')\n","print(X_train_ori.shape)\n","print(X_val_ori.shape)\n","print('\\nafter')\n","print(X_train.shape)\n","print(X_val.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_0DDZmAw92p_","colab_type":"text"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","before\n","(209, 64, 64, 3)\n","(50, 64, 64, 3)\n","\n","after\n","(209, 12288)\n","(50, 12288)"]},{"cell_type":"markdown","metadata":{"id":"3gDM9TnZ92qE","colab_type":"text"},"source":["\n","<br>\n","\n","<font color='red'>**EXERCISE**: </font>\n","* Since for this dataset using sigmoid and regression is enough, standarize the dataset into a `range of 0-1` by dividing it with `255`"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"pT3thgML92qF","colab_type":"code","colab":{}},"source":["print('before',X_train[0,:6])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"at6lTm-D92qM","colab_type":"text"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","before [17 31 56 22 33 59]"]},{"cell_type":"code","metadata":{"id":"n1vwifEp92qM","colab_type":"code","colab":{}},"source":["X_train = ??\n","X_val = ??"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sU4qOipk92qT","colab_type":"code","colab":{}},"source":["print('after',X_train[0,:6])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79aD_qGt92qZ","colab_type":"text"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","after [0.0666667 0.1215686 0.2196078 0.0862745 0.1294118 0.2313725]"]},{"cell_type":"markdown","metadata":{"id":"UV6I5RpY92qa","colab_type":"text"},"source":["---\n","## 2 - Basic Neurons\n","\n","Standard neuron is basically the same as previous linear function. \n","\n","So in here, we've already provide you with the forward-backward implementations"]},{"cell_type":"markdown","metadata":{"id":"kre6pR7J92qc","colab_type":"text"},"source":["---\n","### a. Forward and Backward Affine Function\n","\n","Implement Affine forward function:\n","\n","$$\n","\\begin{align}\n","f(x, W, b) = x.W + b\n","\\end{align}\n","$$\n"]},{"cell_type":"code","metadata":{"id":"wUZykTok92qd","colab_type":"code","colab":{}},"source":["def affine_forward(x, W, b):   \n","    v = np.dot(x, W) + b    \n","    return v"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJak83yX92qj","colab_type":"text"},"source":["Implement affine backward function:\n","\n","\n","$$\n","\\begin{align*}\n","\\partial W & = x^T.\\partial out \\\\\n","\\partial b & = \\sum \\partial out \\\\\n","\\partial x & = \\partial out.W^T \\\\\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"k2Fnd_Sm92qn","colab_type":"code","colab":{}},"source":["def affine_backward(dout, x, W, b):\n","    dW = np.dot(x.T,dout)\n","    db = np.sum(dout, axis=0, keepdims=True)\n","    dx = dout.dot(W.T)\n","    \n","    return dW, db, dx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i9ghf2tn92qr","colab_type":"text"},"source":["---\n","### b. Forward and Backward Sigmoid Function\n","\n","For the activation function, though, you need to make it yourself\n","<br>\n","\n","<font color='red'>**EXERCISE**: </font>\n","* Implement Sigmoid forward function:\n","\n","$$\n","\\begin{align}\n","f(x) = \\sigma(x) = \\frac{1}{1+e^{-v}}\n","\\end{align}\n","$$\n"]},{"cell_type":"code","metadata":{"id":"NGRKY2nw92qr","colab_type":"code","colab":{}},"source":["def sigmoid_forward(x):  \n","  out = ??\n","  return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2413rY9h92qw","colab_type":"code","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","ds = sigmoid_forward(x)\n","\n","print(ds)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VSnRFi992q4","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[0.1192029 0.2689414 0.5       0.7310586 0.8807971 0.9525741]\n"]},{"cell_type":"markdown","metadata":{"id":"LrsactKq92q7","colab_type":"text"},"source":["---\n","<br>\n","\n","<font color='red'>**EXERCISE**: </font>\n","* Implement Sigmoid backward function\n","$$\n","\\begin{align*}\n","\\sigma'(x) = \\sigma(x) \\ (1 - \\sigma(x))\\\\\\\\\n","\\partial out = \\partial out . \\sigma'(x)\n","\\end{align*}\n","$$\n","<br>"]},{"cell_type":"code","metadata":{"id":"mipLpvTu92q9","colab_type":"code","colab":{}},"source":["def sigmoid_backward(dout, ds):\n","    \"\"\"\n","    Argument:\n","        ds: sigmoid forward result\n","        dout: gradient error\n","    \"\"\"\n","    di = ??\n","    dout = ??\n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHSGzegX92rD","colab_type":"code","colab":{}},"source":["np.random.seed(10)\n","dout = np.random.random((6,)) \n","dout = sigmoid_backward(dout, ds)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VDVWCLn692rI","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[0.0809837 0.0040801 0.1584121 0.1472238 0.05234   0.0101556]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5XSSziqF92rK","colab_type":"text"},"source":["---\n","### c. Forward and Backward Tanh Function\n","<br>\n","\n","<font color='red'>**EXERCISE**: </font>\n","* Implement `Tanh` forward function\n","*hint: use `np.tanh(x)`\n","\n","$$\n","\\begin{align}\n","f(x) = tanh(x)\n","\\end{align}\n","$$\n"]},{"cell_type":"code","metadata":{"id":"Qjajerxs92rL","colab_type":"code","colab":{}},"source":["def tanh_forward(x):  \n","    out = ??\n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OVdzDgl92rP","colab_type":"code","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","dt = tanh_forward(x)\n","\n","print(dt)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gC2lOp092rT","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[ -0.9640276 -0.7615942  0.         0.7615942  0.9640276  0.9950548]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qZYKxELm92rX","colab_type":"text"},"source":["---\n","<br>\n","\n","<font color='red'>**EXERCISE**: </font>\n","* Implement Tanh backward function\n","$$\n","\\begin{align*}\n","f'(x) = 1-f(v)^2\\\\\n","\\\\\n","\\partial out = \\partial out . f'(x)\n","\\end{align*}\n","$$\n","<br>"]},{"cell_type":"code","metadata":{"id":"oOX1RX_-92rZ","colab_type":"code","colab":{}},"source":["def tanh_backward(dout, dt):\n","    \"\"\"\n","    Argument:\n","        dt: tanh forward result\n","        dout: gradient error\n","    \"\"\"\n","    di = ??\n","    dout = ??\n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"EnASa_P092rc","colab_type":"code","colab":{}},"source":["np.random.seed(10)\n","dout = np.random.random((6,)) \n","dout = tanh_backward(dout, dt)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0os4dMq92rh","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[0.0544944 0.0087153 0.6336482 0.3144784 0.0352199 0.0022179]\n"]},{"cell_type":"markdown","metadata":{"id":"AA897jo6ZFv6","colab_type":"text"},"source":["---\n","## 3 - Train a One-Layer Sigmoid\n"]},{"cell_type":"markdown","metadata":{"id":"vZ29WzMY92ri","colab_type":"text"},"source":["\n","### a. Training Function\n","\n","The network architecture should be: <br>\n","<pre><b>Input - FC layer - Sigmoid</b></pre>\n","\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>\n","Implement Training Function\n","\n","* `call affine forward function`\n","* `call sigmoid forward function`\n","* `apply L2 loss regression (MSE)`\n","* `call affine backward function`\n","* `implement weight update`\n","\n","<br>\n","\n","**Note** that we do not calculate `sigmoid backward` as in Single Layer Perceptron `sigmoid` is used directly as output activation"]},{"cell_type":"code","metadata":{"id":"-qVsxC3w92rj","colab_type":"code","colab":{}},"source":["def train_one_layer(X, y, W=None, b=None, learning_rate=0.005, num_iters=100, verbose=True):\n","    \n","    \n","    num_train, dim = X.shape\n","    \n","    num_classes = 1\n","    \n","    if W is None:\n","        W = 0.02*np.random.rand(dim, num_classes)\n","    if b is None:\n","        b = np.zeros((1, num_classes))\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","                     \n","    for it in range(num_iters):\n","        \n","\n","        # calculate 1st layer score by calling affine forward function using X, W, and b\n","        layer1 = ??\n","        \n","        # calculate 1st activation score by calling sigmoid forward function using layer1 score\n","        act1 = ??\n","        \n","        # calculate error by subtracting act1 with y\n","        error = ??\n","        \n","        # calculate L2 Loss (MSE) by averaging the squared error\n","        loss = ??  \n","        \n","        # divide error by num_train\n","        error = ??\n","    \n","        # calculate layer 1 weights gradient by calling affine backward function using error, X, W, and b\n","        dW, db, _ = ??\n","\n","        \n","        # perform parameter update by subtracting each W and b with a fraction of dW and db\n","        # according to the learning rate\n","        W -= ??\n","        b -= ??\n","        \n","        if verbose and it % 100 == 0:\n","            print ('iteration', it,'/',num_iters, ': loss =', loss)\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","    return loss_history, W, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ye8c9cZH92rn","colab_type":"text"},"source":["### b. Train the Softmax Classifier\n","\n","Try the training Function using the initial parameter"]},{"cell_type":"code","metadata":{"id":"k6LGU9zl92rn","colab_type":"code","colab":{}},"source":["loss, W, b = train_one_layer(X_train, y_train, num_iters=1000, learning_rate=0.005)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7x7U1LUT92rr","colab_type":"text"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"id":"eppOYFhA92rr","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [12, 5]\n","plt.plot(loss)\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FvQ7aiRT92rv","colab_type":"text"},"source":["\n","### c. Predict Function\n","\n","The network architecture should be: <br>\n","<pre><b>Input - FC layer - Sigmoid</b></pre>\n","\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>\n","Implement Predict Function\n","\n","* `call forward function`"]},{"cell_type":"code","metadata":{"id":"aB-Mcp8g92rw","colab_type":"code","colab":{}},"source":["def predict_one_layer(X, W, b):    \n","    y_pred = np.zeros(X.shape[1])\n","    \n","    # calculate 1st layer score by calling affine forward function using X, W, and b\n","    layer1 = ??\n","\n","    # calculate 1st activation score by calling sigmoid forward function using layer1 score\n","    act1 = ??\n","    \n","    # since it's a binary class, round the score to get the class\n","    y_pred = np.round(act1)\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QJdIRG2G92ry","colab_type":"text"},"source":["Calculate the Training Accuracy"]},{"cell_type":"code","metadata":{"id":"e8psteqc92rz","colab_type":"code","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_one_layer(X_train, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15].ravel())\n","print('Predicted label =',y_pred[:15].astype('int').ravel())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2yzbf36o92r4","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should get about <b>~95%</b> accuracy on training set using the initial run</pre>"]},{"cell_type":"markdown","metadata":{"id":"ei9VFwzL92r5","colab_type":"text"},"source":["Calculate the Validation Accuracy"]},{"cell_type":"code","metadata":{"id":"bxCkHhzp92r6","colab_type":"code","colab":{}},"source":["y_pred = predict_one_layer(X_val, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label  =',y_val[:15].ravel())\n","print('Predicted label =',y_pred[:15].astype('int').ravel())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MMh1rxBj92r9","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should also get about <b>~74%</b> accuracy on validation set</pre>\n","\n","<br>\n","\n","You can retrain further the weights by adding the pre-trained W and b to the arguments when calling training function\n","\n"]},{"cell_type":"code","metadata":{"id":"gDDBQVUgM_OT","colab_type":"code","colab":{}},"source":["# loss, W, b = train_one_layer(X_train, y_train, W=W, b=b, num_iters=1000, learning_rate=0.005)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2piJd0x92r9","colab_type":"text"},"source":["---\n","---\n","# [Part 2] Two-Layer Neural Network on CIFAR-10\n","\n","Now, let's build a two-layered Neural Network and train it using CIFAR-10 dataset"]},{"cell_type":"markdown","metadata":{"id":"zcyRACfL92r-","colab_type":"text"},"source":["---\n","## 1 - Load CIFAR-10 Dataset\n","\n","CIFAR-10 dataset is a image classification dataset, consisting of 10 classes. The images are $32\\times32$ color image with 50,000 data train and 10,000 data test"]},{"cell_type":"markdown","metadata":{"id":"M2X2LYjB92r_","colab_type":"text"},"source":["---\n","### a. Import Data ***CIFAR-10***"]},{"cell_type":"code","metadata":{"id":"rvv-wROB92r_","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)\n","print('X_test.shape  =',X_test.shape)\n","print('y_test.shape  =',y_test.shape)\n","\n","X_test_ori = X_test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ta7ftQi-92sC","colab_type":"text"},"source":["---\n","### b. Visualizing Data\n","Show the first 20 images from `X_train`"]},{"cell_type":"code","metadata":{"id":"RgmpnQx092sD","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train[i+j*10])\n","        ax[j,i].set_title(classes[y_train[i+j*10,0]])\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JP_N9cAB92sG","colab_type":"text"},"source":["---\n","### c. Split Training Data\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>\n","* `Get the last 1000 data from Training Set as Validation Set`"]},{"cell_type":"code","metadata":{"id":"fQsc4Wa192sI","colab_type":"code","colab":{}},"source":["X_val = ??\n","y_val = ??\n","\n","X_train = ??\n","y_train = ??\n","\n","print('X_val.shape   =',X_val.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BpcwDu1dNmfU","colab_type":"text"},"source":["**EXPECTED OUTPUT:**\n","<pre>X_val.shape   = (1000, 32, 32, 3)\n","y_val.shape   = (1000, 1)\n","X_train.shape = (49000, 32, 32, 3)\n","y_train.shape = (49000, 1)"]},{"cell_type":"markdown","metadata":{"id":"XLpqKlTJ92sL","colab_type":"text"},"source":["---\n","### d. Normalizing Data\n","Normalize `X_train`, `X_val` and `X_test` by *zero-centering* them:"]},{"cell_type":"code","metadata":{"id":"YPJiHpgC92sM","colab_type":"code","colab":{}},"source":["X_train = X_train.astype('float32')\n","X_val = X_val.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","mean_image = np.mean(X_train, axis = 0)\n","X_train -= mean_image\n","X_val -= mean_image\n","X_test -= mean_image\n","\n","print('np.mean(X_train) =',np.mean(X_train))\n","print('np.mean(X_val)   =',np.mean(X_val))\n","print('np.mean(X_test)  =',np.mean(X_test))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGEk3WMy92sX","colab_type":"text"},"source":["---\n","### e. Reshape Data\n","Reshape each data in `X_train`, `X_val` and `X_test` into 1-dimensional matrix\n"]},{"cell_type":"code","metadata":{"id":"WmW3BZn692sZ","colab_type":"code","colab":{}},"source":["X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]*X_train.shape[3]))\n","X_val = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]*X_val.shape[3]))\n","X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]*X_test.shape[3]))\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fMvowg3l92sd","colab_type":"text"},"source":[" Reshape `y_train`, `y_val` and `y_test` into a vector \n"]},{"cell_type":"code","metadata":{"id":"9pE9Pgsz92sd","colab_type":"code","colab":{}},"source":["y_train = y_train.ravel()\n","y_val = y_val.ravel()\n","y_test = y_test.ravel()\n","\n","print('y_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U3M4090292sg","colab_type":"text"},"source":["---\n","## 2 - Advanced Activation Functions\n","\n","For this part, you need to implement several advanced activation functions that became popular recently"]},{"cell_type":"markdown","metadata":{"id":"b_9cX36A92sh","colab_type":"text"},"source":["---\n","### a. Rectified Linear Unit (ReLU)\n","\n","Implement the forward and backward function of the infamous Rectified Linear Unit (ReLU) activation function\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>\n","* Implement ReLU forward function:\n","\n","\n","$$\n","\\begin{align}\n","f(x) = \n","\\begin{cases}\n","0, & \\text{for } x<0\\\\\n","x, & \\text{for } x\\geq0\n","\\end{cases}\n","\\end{align}\n","$$\n","\n","*hint: use `np.maximum()`"]},{"cell_type":"code","metadata":{"id":"eBu0Q6-Y92si","colab_type":"code","colab":{}},"source":["def relu_forward(x):  \n","    out = ??\n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRZtpSAP92sk","colab_type":"code","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","dr = relu_forward(x) \n","\n","print(dr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_pwj_6xn92so","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[0, 0, 0, 1, 2, 3]\n"]},{"cell_type":"markdown","metadata":{"id":"2t_YjdFX92sp","colab_type":"text"},"source":["---\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>\n","* Implement ReLU backward function\n","$$\n","\\begin{align*}\n","f'(v) = \n","\\begin{cases}\n","0, & \\text{for } x<0\\\\\n","1, & \\text{for } x\\geq0\n","\\end{cases}\\\\\n","\\\\\n","\\partial out = \\partial out . f'(x)\n","\\end{align*}\n","$$\n","\n","*hint: use `np.where(condition, if true, if false)`"]},{"cell_type":"code","metadata":{"id":"fjRGgdT-92sq","colab_type":"code","colab":{}},"source":["def relu_backward(dout, x):\n","    \"\"\"\n","    Argument:\n","        x: relu input\n","        dout: gradient error\n","    \"\"\"\n","    di   = ??\n","    dout = ??\n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j07YdL5i92ss","colab_type":"code","colab":{}},"source":["np.random.seed(10)\n","dout = np.random.random((6,)) \n","dout = relu_backward(dout, x)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMTHkOiR92sw","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[0.        0.        0.        0.7488039 0.498507  0.2247966]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SovOoekP92sw","colab_type":"text"},"source":["---\n","### b. Parametric ReLU (Leaky ReLU)\n","\n","Implement the forward and backward function of parametric Rectified Linear Unit (Leaky ReLU) activation function\n","\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>\n","* Implement Parametric ReLU forward function:\n","\n","\n","$$\n","\\begin{align}\n","f(x, \\alpha) = \n","\\begin{cases}\n","\\alpha x, & \\text{for } x<0\\\\\n","x, & \\text{for } x\\geq0\n","\\end{cases}\n","\\end{align}\n","$$\n","\n","*hint: use `np.where(condition, if true, if false)`"]},{"cell_type":"code","metadata":{"id":"YPdGZzEP92sw","colab_type":"code","colab":{}},"source":["def prelu_forward(x, alpha):  \n","    out = ??\n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QAzZPL-92sz","colab_type":"code","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","alpha = 0.01\n","dp = prelu_forward(x, alpha) \n","\n","print(dp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FCsWPV1792s0","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[-0.02 -0.01  0.    1.    2.    3.  ]\n"]},{"cell_type":"markdown","metadata":{"id":"DN54FbAv92s1","colab_type":"text"},"source":["---\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>\n","* Implement PReLU backward function\n","$$\n","\\begin{align*}\n","f'(x, \\alpha) = \n","\\begin{cases}\n","\\alpha, & \\text{for } x<0\\\\\n","1, & \\text{for } x\\geq0\n","\\end{cases}\\\\\n","\\\\\n","\\partial out = \\partial out . f'(x)\n","\\end{align*}\n","$$\n","\n","*hint: use `np.where(condition, if true, if false)`"]},{"cell_type":"code","metadata":{"id":"6KLXiutj92s1","colab_type":"code","colab":{}},"source":["def prelu_backward(dout, x, alpha):\n","    \"\"\"\n","    Argument:\n","        x: prelu input\n","        dout: gradient error\n","    \"\"\"\n","    di   = ??\n","    dout = ??\n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHn_dvZq92s4","colab_type":"code","colab":{}},"source":["np.random.seed(10)\n","dout = np.random.random((6,)) \n","dout = prelu_backward(dout, x, alpha)\n","\n","\n","np.set_printoptions(precision=5)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aDAhyVY_92s7","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[7.71321e-03 2.07519e-04 6.33648e-03 7.48804e-01 4.98507e-01 2.24797e-01]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uOEsPa5Y92s8","colab_type":"text"},"source":["---\n","### c. ELU Function\n","\n","Implement the forward and backward function of the new Exponential Linear Unit (ELU) activation function\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>\n","* Implement ELU forward function:\n","\n","\n","$$\n","\\begin{align}\n","f(x, \\alpha) = \n","\\begin{cases}\n","\\alpha (e^x-1), & \\text{for } x<0\\\\\n","x, & \\text{for } x\\geq0\n","\\end{cases}\n","\\end{align}\n","$$\n","\n","*hint: use `np.where(condition, if true, if false)`"]},{"cell_type":"code","metadata":{"id":"dxoUn19E92s_","colab_type":"code","colab":{}},"source":["def elu_forward(x, alpha):  \n","    out = ??\n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQEAKYcQ92tB","colab_type":"code","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","alpha = 1.0\n","de = elu_forward(x, alpha) \n","\n","np.set_printoptions(precision=7)\n","print(de)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-EtJ07v-92tD","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[-0.8646647 -0.6321206  0.         1.         2.         3.       ]\n"]},{"cell_type":"markdown","metadata":{"id":"-8RnowdB92tD","colab_type":"text"},"source":["---\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>\n","* Implement `ELU` backward function\n","$$\n","\\begin{align*}\n","f'(x, \\alpha) = \n","\\begin{cases}\n","f(x,\\alpha)+\\alpha, & \\text{for } x<0\\\\\n","1, & \\text{for } x\\geq0\n","\\end{cases}\\\\\n","\\\\\n","\\partial out = \\partial out . f'(x)\n","\\end{align*}\n","$$\n","\n","*hint: use `np.where(condition, if true, if false)`"]},{"cell_type":"code","metadata":{"id":"RJUGzh5u92tE","colab_type":"code","colab":{}},"source":["def elu_backward(dout, x, alpha):\n","    \"\"\"\n","    Argument:\n","        x: elu input\n","        dout: gradient error\n","    \"\"\"\n","    di   = ??\n","    dout = ??\n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l6Jmcj9992tG","colab_type":"code","colab":{}},"source":["np.random.seed(10)\n","dout = np.random.random((6,)) \n","dout = elu_backward(dout, x, alpha)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfSRcG9d92tI","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>[0.1043869 0.0076342 0.6336482 0.7488039 0.498507  0.2247966]\n","\n","</pre?\n"]},{"cell_type":"markdown","metadata":{"id":"BfTpR-9h92tI","colab_type":"text"},"source":["---\n","## 3 - Softmax Function \n"]},{"cell_type":"markdown","metadata":{"id":"S1Q0Yrag92tJ","colab_type":"text"},"source":["### a. Softmax Score\n","\n","The implementation is the same as previous Task, so in here we already provide you with the implementation\n"]},{"cell_type":"code","metadata":{"id":"p5n9QYlx92tK","colab_type":"code","colab":{}},"source":["def softmax(x):  \n","    \n","    # shift x by subtracting with its maximum value . Use np.max(...)\n","    x -= np.max(x)\n","    \n","    # Apply exp() element-wise to x. Use np.exp(...).    \n","    x_exp = np.exp(x)\n","    \n","    # Create a vector X_sum that sums each row of X_exp. Use np.sum(..., axis = 1, keepdims = True).\n","    x_sum = np.sum(x_exp, axis = 1, keepdims = True)  \n","    \n","    # Compute softmax(x) score by dividing X_exp by X_sum. It should automatically use numpy broadcasting.\n","    score = x_exp / x_sum\n","    \n","    return score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Yxwqh0m92tM","colab_type":"text"},"source":["### b. Softmax Loss\n","\n","Implement a softmax loss function using numpy. The implementation is the same as previous Task, so in here we already provide you with the implementation\n"]},{"cell_type":"code","metadata":{"id":"py4nnXFO92tM","colab_type":"code","colab":{}},"source":["def softmax_loss(score, y):\n","   \n","    num_examples = score.shape[0]\n","    \n","    #make a number list containing [1 2 3 ... n]\n","    number_list = range(num_examples)\n","    \n","    # calculate the correct log probability of score[number_list,y] by applycing -np.log(...)\n","    corect_logprobs = -np.log(score[number_list,y])\n","    \n","    # average the correct log probability, use np.sum then divide it by num_examples\n","    loss = np.sum(corect_logprobs)/num_examples\n","    \n","    \n","    # 3. COMPUTE THE GRADIENT ON SCORES\n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    \n","    return loss, dscores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zpdiX12ndORu"},"source":["---\n","---\n","# [Part 3] Train Sigmoid vs ReLU\n","\n","Let's train the network to CIFAR-10 dataset, and compare the result between using Sigmoid and ReLU activation function"]},{"cell_type":"markdown","metadata":{"id":"rCisaNV8Yz30","colab_type":"text"},"source":["---\n","## 1 - Train a Two-Layer Sigmoid with Softmax"]},{"cell_type":"markdown","metadata":{"id":"0A_RVWT692tO","colab_type":"text"},"source":["---\n","\n","### a. Training Function\n","\n","The network architecture should be: **Input - FC layer - Sigmoid - FC Layer - Softmax**\n","\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font>Implement Training Function\n","\n","* `call affine forward function`\n","* `call sigmoid forward function`\n","* `call affine forward function`\n","* `call softmax function`\n","* `call softmax_loss function`\n","* `call affine backward function`\n","* `call sigmoid backward function`\n","* `call affine backward function`\n","* `implement weight update`"]},{"cell_type":"code","metadata":{"id":"5MPcksBs92tO","colab_type":"code","colab":{}},"source":["def train_two_layer_sigmoid(X, y, hidden_size, W=None, b=None, learning_rate=1e-4, lr_decay=0.95, reg=0.5, num_iters=100, \n","                    batch_size=200, verbose=True):\n","    num_train, dim = X.shape\n","    iterations_per_epoch = max(num_train / batch_size, 1)\n","    \n","    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","    \n","    if W is None:\n","        W0 = 1e-4 * np.random.randn(dim, hidden_size)\n","        W1 = 1e-4 * np.random.randn(hidden_size, num_classes)\n","        W = [W0, W1]\n","    if b is None:\n","        b0 = np.zeros((1,hidden_size))\n","        b1 = np.zeros((1,num_classes))\n","        b = [b0, b1]\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","                     \n","    for it in range(num_iters):\n","        X_batch = None\n","        y_batch = None\n","\n","        # Randomly select indices from training examples\n","        train_rows = np.arange(num_train)\n","        idxs = np.random.choice(train_rows, batch_size, replace=False)\n","  \n","        X_batch = X[idxs]\n","        y_batch = y[idxs]\n","\n","\n","        # calculate 1st layer score by calling affine forward function using X_batch, W[0], and b[0]\n","        layer1 = ??\n","        \n","        # calculate 1st activation score by calling sigmoid forward function using layer1 score\n","        act1 = ??\n","        \n","        # calculate 2nd layer score by calling affine forward function using act1, W[1], and b[1]\n","        layer2 = ??\n","                \n","        # calculate softmax score by calling softmax function using layer2 score\n","        softmax_score = ??\n","        \n","        # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n","        loss, dout = ??\n","        \n","        # add regularization to the loss\n","        loss+= 0.5 * reg * np.sum(W[0] * W[0]) + 0.5 * reg * np.sum(W[1] * W[1])\n","    \n","        # append the loss history\n","        loss_history.append(loss)\n","\n","        # calculate layer 2 weights gradient by calling affine backward function using dout, act1, W[1], and b[1]\n","        dW1, db1, dact1 = ??\n","        \n","        # calculate sigmoid gradient by calling sigmoid backward function using dact1 and act1 score\n","        dlayer1 = ??\n","    \n","        # calculate layer 1 weights gradient by calling affine backward function using dlayer1, X_batch, W[0], and b[0]\n","        dW0, db0, _ = ??\n","        \n","        # perform regulatization gradient\n","        dW1 += reg*W[1]\n","        dW0 += reg*W[0]\n","        \n","        # perform parameter update by subtracting W and b with a fraction of dW and db\n","        # according to the learning rate\n","        W[0] -= ??\n","        b[0] -= ??\n","        W[1] -= ??\n","        b[1] -= ??\n","        \n","        if verbose and it % 100 == 0:\n","            print ('iteration', it,'/',num_iters, ': loss =', loss)\n","            \n","        if it % iterations_per_epoch == 0:\n","            # Decay learning rate\n","            learning_rate *= lr_decay\n","        \n","    return loss_history, W, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_BCDToT92tQ","colab_type":"text"},"source":["### b. Train the Softmax Classifier\n","\n","Try the training Function using the initial parameter"]},{"cell_type":"code","metadata":{"id":"nACDxIkF92tQ","colab_type":"code","colab":{}},"source":["loss, W_sigm, b_sigm = train_two_layer_sigmoid(X_train, y_train, hidden_size=50, num_iters=1000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2jFi4gF92tR","colab_type":"text"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"id":"41z1XYZ_92tT","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [12, 5]\n","plt.plot(loss)\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HHZUjG9V92tU","colab_type":"text"},"source":["\n","### c. Predict Function\n","\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :** </font> Implement Predict Function\n","* call forward function\n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input - FC layer - Sigmoid - FC Layer - argmax\n","\n"]},{"cell_type":"code","metadata":{"id":"unreYTQZ92tW","colab_type":"code","colab":{}},"source":["def predict_two_layer_sigmoid(X, W, b):    \n","    y_pred = np.zeros(X.shape[1])\n","\n","    \n","    # calculate 1st layer score by calling affine forward function using X_batch, W[0], and b[0]\n","    layer1 = ??\n","\n","    # calculate 1st activation score by calling sigmoid forward function using layer1 score\n","    act1 = ??\n","\n","    # calculate 2nd layer score by calling affine forward function using act1, W[1], and b[1]\n","    layer2 = ??\n","    \n","    # take the maximum prediction from layer 2 and use that column to get the class   \n","    # use np.argmax\n","    y_pred = ??\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2g7l0Aa92tX","colab_type":"code","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_two_layer_sigmoid(X_train, W_sigm, b_sigm)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Cdus5LE92ta","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should get about <b>~20%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"id":"qEotzZEw92ta","colab_type":"code","colab":{}},"source":["y_pred = predict_two_layer_sigmoid(X_val, W_sigm, b_sigm)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yRcDOPQf92tc","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should also get about <b>~20%</b> accuracy on validation set</pre>\n","\n","<br>\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"markdown","metadata":{"id":"3Dry2JfV92td","colab_type":"text"},"source":["---\n","## 2 - Train a Two-Layer ReLU with Softmax"]},{"cell_type":"markdown","metadata":{"id":"G-3Yniwi92td","colab_type":"text"},"source":["\n","### a. Predict Function\n","\n","This time, we implement the predict function first, because we are going to use `predict` function inside the `training` function to track the `validation` accuracy \n","\n","The network architecture should be: \n","<pre><b>Input - FC layer - ReLU - FC Layer - argmax</b></pre>\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :**</font> Implement Predict Function\n","\n","* call forward function"]},{"cell_type":"code","metadata":{"id":"MT-1-YzN92te","colab_type":"code","colab":{}},"source":["def predict_two_layer_relu(X, W, b):    \n","    y_pred = np.zeros(X.shape[1])\n","\n","    \n","    # calculate 1st layer score by calling affine forward function using X_batch, W[0], and b[0]\n","    layer1 = ??\n","\n","    # calculate 1st activation score by calling relu forward function using layer1 score\n","    act1 = ??\n","\n","    # calculate 2nd layer score by calling affine forward function using act1, W[1], and b[1]\n","    layer2 = ??\n","    \n","    # take the maximum prediction from layer 2 and use that column to get the class     \n","    y_pred = ??\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FHlDx27a92tg","colab_type":"text"},"source":["### b. Training Function\n","\n","The network architecture should be: \n","<pre><b>Input - FC layer - ReLU - FC Layer - Softmax</b></pre>\n","\n","\n","<br>\n","\n","<font color='red'>**EXERCISE :**</font> Implement Training Function\n","\n","* `call affine forward function`\n","* `call relu forward function`\n","* `call affine forward function`\n","* `call softmax function`\n","* `call softmax_loss function`\n","* `call affine backward function`\n","* `call relu backward function`\n","* `call affine backward function`\n","* `implement weight update`\n","* `calculate the training and validation accuracy`"]},{"cell_type":"code","metadata":{"id":"SclS-t5w92tg","colab_type":"code","colab":{}},"source":["def train_two_layer_relu(X, y, X_val, y_val, hidden_size, W=None, b=None, learning_rate=1e-4, lr_decay=0.9, reg=0.5, num_iters=100, \n","                    batch_size=200, verbose=True):\n","    num_train, dim = X.shape\n","    iterations_per_epoch = max(num_train / batch_size, 1)\n","    \n","    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","    \n","    if W is None:\n","        W0 = 1e-4 * np.random.randn(dim, hidden_size)\n","        W1 = 1e-4 * np.random.randn(hidden_size, num_classes)\n","        W = [W0, W1]\n","    if b is None:\n","        b0 = np.zeros((1,hidden_size))\n","        b1 = np.zeros((1,num_classes))\n","        b = [b0, b1]\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","                     \n","    for it in range(num_iters):\n","        X_batch = None\n","        y_batch = None\n","\n","        # Randomly select indices from training examples\n","        train_rows = np.arange(num_train)\n","        idxs = np.random.choice(train_rows, batch_size, replace=False)\n","  \n","        X_batch = X[idxs]\n","        y_batch = y[idxs]\n","\n","\n","        # calculate 1st layer score by calling affine forward function using X_batch, W[0], and b[0]\n","        layer1 = ??\n","        \n","        # calculate 1st activation score by calling relu forward function using layer1 score\n","        act1 = ??\n","        \n","        # calculate 2nd layer score by calling affine forward function using act1, W[1], and b[1]\n","        layer2 = ??\n","                \n","        # calculate softmax score by calling softmax function using layer2 score\n","        softmax_score = ??\n","        \n","        # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n","        loss, dout = ??\n","        \n","        # add regularization to the loss\n","        loss+= 0.5 * reg * np.sum(W[0] * W[0]) + 0.5 * reg * np.sum(W[1] * W[1])\n","    \n","        # append the loss history\n","        loss_history.append(loss)\n","\n","        # calculate layer 2 weights gradient by calling affine backward function using dout, act1, W[1], and b[1]\n","        dW1, db1, dact1 = ??\n","        \n","        # calculate sigmoid gradient by calling relu backward function using dact1 and act1 score\n","        dlayer1 = ??\n","    \n","        # calculate layer 1 weights gradient by calling affine backward function using dlayer1, X_batch, W[0], and b[0]\n","        dW0, db0, _ = ?? \n","        \n","        # perform regulatization gradient\n","        dW1 += reg*W[1]\n","        dW0 += reg*W[0]\n","        \n","        # perform parameter update by subtracting W and b with a fraction of dW and db\n","        # according to the learning rate\n","        W[0] -= ??\n","        b[0] -= ??\n","        W[1] -= ??\n","        b[1] -= ??\n","        \n","        if verbose and it % 100 == 0:\n","            print ('iteration', it,'/',num_iters, ': loss =', loss)\n","            \n","        if it % iterations_per_epoch == 0:\n","            # Check accuracy\n","            # calculate the training accuracy by calling predict_two_layer_relu function on X_batch\n","            # and compare it tu y_batch. Then calculate the mean correct (accuracy in range 0-1)\n","            train_acc = (predict_two_layer_relu(X_batch, W, b) == y_batch).mean()\n","            \n","            # calculate the training accuracy by calling predict_two_layer_relu function on X_val\n","            # and compare it tu y_val. Then calculate the mean correct (accuracy in range 0-1)\n","            val_acc = (predict_two_layer_relu(X_val, W, b) == y_val).mean()\n","            \n","            train_acc_history.append(train_acc)\n","            val_acc_history.append(val_acc)\n","            \n","            # Decay learning rate\n","            learning_rate *= lr_decay\n","        \n","    return loss_history, W, b, train_acc_history, val_acc_history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghdyqYYl92th","colab_type":"text"},"source":["### c. Train the Softmax Classifier\n","\n","Try the training Function using the initial parameter"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"3_kOrJKK92ti","colab_type":"code","colab":{}},"source":["loss, W_relu, b_relu, train_acc, val_acc = train_two_layer_relu(X_train, y_train, X_val, y_val, hidden_size=50, num_iters=1000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5WADncXh92tj","colab_type":"text"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"DJwhsE6S92tk","colab_type":"code","colab":{}},"source":["plt.plot(loss)\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qm2t91O092tn","colab_type":"text"},"source":["Visualize the training and validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"IyHQ4wAK92tp","colab_type":"code","colab":{}},"source":["plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.title('Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzvMhF-p92tr","colab_type":"code","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_two_layer_relu(X_train, W_relu, b_relu)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kquphISw92ts","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should be able to get about <b>~28%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"id":"DSJkTObc92ts","colab_type":"code","colab":{}},"source":["y_pred = predict_two_layer_relu(X_val, W_relu, b_relu)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-aGoPbB92tv","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should also be able to get about <b>~29%</b> accuracy on validation set</pre>\n","\n","<br>\n","\n","You can retrain further the weights by adding the pre-trained W and b to the arguments when calling training function\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"1LsoAvV492tv","colab_type":"text"},"source":["## 3 - First Layer Visualization\n"]},{"cell_type":"code","metadata":{"id":"OiaTO7K192tv","colab_type":"code","colab":{}},"source":["## run this to turn off the scrolling effect in jupyter notebook\n","\n","from IPython.core.magics.display import Javascript\n","Javascript(\"\"\"\n","  IPython.OutputArea.prototype._should_scroll = function(lines) {\n","      return false;\n","  }\"\"\"\n",")\n","\n","## set return true to turn on the scrolling effect"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7T0XX9vRaqLN","colab_type":"code","colab":{}},"source":["!wget 'https://raw.githubusercontent.com/CNN-ADF/Task2019/master/resources/vis_utils.py'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"91xGTPaj92ty","colab_type":"code","colab":{}},"source":["from vis_utils import visualize_grid\n","\n","plt.rcParams['figure.figsize'] = [10, 10]\n","\n","plt.imshow(visualize_grid(W_relu[0].reshape(32, 32, 3, -1).transpose(3, 0, 1, 2), padding=3).astype('uint8'))\n","\n","plt.gca().axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MX7mAsBx92tz","colab_type":"text"},"source":["<pre>You should see that if you re-train the network, the weight result visualization will be different."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WH0hh11zdqcY"},"source":["---\n","---\n","# [Optional] Hyperparameter Tuning\n","\n","Now, let's build a two-layered Neural Network and train it using CIFAR-10 dataset"]},{"cell_type":"markdown","metadata":{"id":"Wp7QWSwS92t0","colab_type":"text"},"source":["**What's wrong?**. \n","* Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. \n","* Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. \n","* On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n","\n","<br>\n","\n","**Tuning**. \n","* Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice.\n","* Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. \n","* You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n","\n","**Approximate results**. \n","* You should be aim to achieve a classification accuracy of greater than 48% on the validation set. Our best network gets over 52% on the validation set.\n","\n","<br>\n","\n","**EXERCISE :**\n","* `Use the validation set to tune hyperparameters (regularization strength and learning rate)`\n","* `find the best learning rate and regularization strength using staged random search, (**Coarse-to-Fine Search**)`\n","* `try to gradually decrease the random range to find the best learning rate and regularization strength`\n","* `use only few epochs or iteration`"]},{"cell_type":"code","metadata":{"id":"_CRrHw2a92t0","colab_type":"code","colab":{}},"source":["import warnings\n","import datetime\n","warnings.filterwarnings('ignore')\n","\n","results = {}\n","best_val = -1\n","best_reg = 0\n","best_lr = 0\n","\n","best_W = None\n","best_b = None\n","max_iter = 1000\n","max_trial = 30\n","\n","for trial in range(max_trial):\n","    \n","    reg = 10**np.random.uniform(-2, 1)    # <---------- you can try and change this\n","    lr = 10**np.random.uniform(-2,-4)     # <---------- you can try and change this    \n","    \n","    loss, W, b, _, _ = train_two_layer_relu(X_train, y_train, X_val, y_val, 50,\n","                                      num_iters=max_iter, batch_size=200, \n","                                      learning_rate=lr, lr_decay=0.95, \n","                                      reg=reg, verbose=False) \n","    val_acc = (predict_two_layer_relu(X_val, W, b) == y_val).mean() \n","    if val_acc > best_val: \n","        best_W = W \n","        best_b = b \n","        best_val = val_acc \n","        best_lr  = lr \n","        best_reg = reg \n","    print(str(datetime.datetime.now()), 'val_acc:', val_acc, '\\tlr:', lr, \n","          '\\treg:', reg, '\\t', str(trial)+'/'+str(max_trial))\n","    \n","print (\"best_reg: \", best_reg)\n","print (\"best_lr: \", best_lr)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dTMH_Wlc92t1","colab_type":"text"},"source":["* You can try different strategy to find the hyperparameter\n","* You can also try to finetune the other hyperparameter such as number of hidden neuron and lr_decay\n","* You can also try other architectures such as changing the activation function"]},{"cell_type":"markdown","metadata":{"id":"DMDeUKJ192t2","colab_type":"text"},"source":["---\n","## 1 - Train the Network Fully\n","\n","When you are done experimenting,\n","\n","Train the network for longer epochs using the `best learning rate` and ` best regularization strength`\n","\n","---"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"JcBCr4t-92t3","colab_type":"code","colab":{}},"source":["print('lr =',best_lr)\n","print('reg =',best_reg)\n","loss, best_W, best_b, train_acc, val_acc = train_two_layer_relu(X_train, y_train, X_val, y_val,\n","                                                      W = best_W, b = best_b,\n","                                                      hidden_size=50,\n","                                                      num_iters=5000,\n","                                                      learning_rate = best_lr,\n","                                                      reg = best_reg)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e7fucZM_92t8","colab_type":"text"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ud-yWUve92t-","colab_type":"code","colab":{}},"source":["plt.plot(loss)\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mKyrEWK92uB","colab_type":"text"},"source":["Visualize the training and validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"_CPvhV6092uB","colab_type":"code","colab":{}},"source":["plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.title('Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"li8UULY092uC","colab_type":"code","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_two_layer_relu(X_train, best_W, best_b)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uV6rk80a92uE","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>If you're careful, You should be able to get about <b>~60%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"id":"NtgFFdM492uE","colab_type":"code","colab":{}},"source":["y_pred = predict_two_layer_relu(X_val, best_W, best_b)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8rEVaW_C92uF","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should also be able to get about <b>~50%</b> accuracy on validation set</pre>\n","\n","<br>\n","\n","You can retrain further the weights by adding the pre-trained W and b to the arguments when calling training function\n","\n","---\n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"kEGRYolZ92uF","colab_type":"code","colab":{}},"source":["plt.imshow(visualize_grid(best_W[0].reshape(32, 32, 3, -1).transpose(3, 0, 1, 2), padding=3).astype('uint8'))\n","\n","plt.gca().axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRx-kvEc92uH","colab_type":"text"},"source":["## 2 - Test the Trained Weights\n","\n","Evaluate your final trained network on the test set; you should be able get **above 48%.**"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"hWA3_ob492uH","colab_type":"code","colab":{}},"source":["y_pred = predict_two_layer_relu(X_test, best_W, best_b)\n","\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label      =',y_test[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7GkxoGT92uK","colab_type":"text"},"source":[" An important way to gain intuition about how an algorithm works is to visualize the mistakes that it makes. \n"," \n"," In this visualization, we show examples of images that are misclassified by our current system. \n"," \n"," The first column  shows images that our system labeled as \"plane\" but whose true label is  something other than \"plane\"."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"J1h57y3h92uL","colab_type":"code","colab":{}},"source":["examples_per_class = 8\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for cls, cls_name in enumerate(classes):\n","    idxs = np.where((y_test != cls) & (y_pred == cls))[0]\n","    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n","    for i, idx in enumerate(idxs):\n","        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n","        plt.imshow(X_test_ori[idx].astype('uint8'))\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls_name)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzG3ZWZq92uM","colab_type":"text"},"source":["---\n","---\n","# [Optional] Two-Layer NeuralNet on Feature Space\n","\n","We have seen that we can achieve reasonable performance on an image classification task by training a linear classifier on the pixels of the input image. In this exercise we will show that we can improve our classification performance by training linear classifiers not on raw pixels but on features that are computed from the raw pixels.\n","\n","All of your work for this exercise will be done in this notebook."]},{"cell_type":"markdown","metadata":{"id":"I97mZ8U_92uM","colab_type":"text"},"source":["## 1 - Feature Extraction Functions\n","* For each image we will compute a Histogram of Oriented Gradients (HOG) as well as a color histogram using the hue channel in HSV color space. We form our final feature vector for each image by concatenating the HOG and color histogram feature vectors.\n","\n","* Roughly speaking, HOG should capture the texture of the image while ignoring color information, and the color histogram represents the color of the input image while ignoring texture. \n","* As a result, we expect that using both together ought to work better than using either alone. Verifying this assumption would be a good thing to try for your interests.\n","\n","* The `hog_feature` and `color_histogram_hsv` functions both operate on a single\n","image and return a feature vector for that image.\n","* The `extract_features` function takes a set of images and a list of feature functions and evaluates each feature function on each image, storing the results in a matrix where\n","each column is the concatenation of all feature vectors for a single image."]},{"cell_type":"code","metadata":{"id":"D3p77eRO92uN","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","\n","import matplotlib\n","from scipy.ndimage import uniform_filter\n","\n","def extract_features(imgs, feature_fns, verbose=False):\n","    num_images = imgs.shape[0]\n","    if num_images == 0:\n","        return np.array([])\n","\n","    # Use the first image to determine feature dimensions\n","    feature_dims = []\n","    first_image_features = []\n","    for feature_fn in feature_fns:\n","        feats = feature_fn(imgs[0].squeeze())\n","        assert len(feats.shape) == 1, 'Feature functions must be one-dimensional'\n","        feature_dims.append(feats.size)\n","        first_image_features.append(feats)\n","\n","    # Now that we know the dimensions of the features, we can allocate a single\n","    # big array to store all features as columns.\n","    total_feature_dim = sum(feature_dims)\n","    imgs_features = np.zeros((num_images, total_feature_dim))\n","    imgs_features[0] = np.hstack(first_image_features).T\n","\n","    # Extract features for the rest of the images.\n","    for i in range(1, num_images):\n","        idx = 0\n","        for feature_fn, feature_dim in zip(feature_fns, feature_dims):\n","            next_idx = idx + feature_dim\n","            imgs_features[i, idx:next_idx] = feature_fn(imgs[i].squeeze())\n","            idx = next_idx\n","        if verbose and i % 1000 == 0:\n","            print('Done extracting features for %d / %d images' % (i, num_images))\n","\n","    return imgs_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWJfslUI92uO","colab_type":"code","colab":{}},"source":["\n","def rgb2gray(rgb):\n","    return np.dot(rgb[..., :3], [0.299, 0.587, 0.144])\n","\n","\n","def hog_feature(im):\n","\n","    # convert rgb to grayscale if needed\n","    if im.ndim == 3:\n","        image = rgb2gray(im)\n","    else:\n","        image = np.at_least_2d(im)\n","\n","    sx, sy = image.shape  # image size\n","    orientations = 9  # number of gradient bins\n","    cx, cy = (8, 8)  # pixels per cell\n","\n","    gx = np.zeros(image.shape)\n","    gy = np.zeros(image.shape)\n","    gx[:, :-1] = np.diff(image, n=1, axis=1)  # compute gradient on x-direction\n","    gy[:-1, :] = np.diff(image, n=1, axis=0)  # compute gradient on y-direction\n","    grad_mag = np.sqrt(gx ** 2 + gy ** 2)  # gradient magnitude\n","    grad_ori = np.arctan2(gy, (gx + 1e-15)) * (180 / np.pi) + 90  # gradient orientation\n","\n","    n_cellsx = int(np.floor(sx / cx))  # number of cells in x\n","    n_cellsy = int(np.floor(sy / cy))  # number of cells in y\n","    # compute orientations integral images\n","    orientation_histogram = np.zeros((n_cellsx, n_cellsy, orientations))\n","    for i in range(orientations):\n","        # create new integral image for this orientation\n","        # isolate orientations in this range\n","        temp_ori = np.where(grad_ori < 180 / orientations * (i + 1),\n","                            grad_ori, 0)\n","        temp_ori = np.where(grad_ori >= 180 / orientations * i,\n","                            temp_ori, 0)\n","        # select magnitudes for those orientations\n","        cond2 = temp_ori > 0\n","        temp_mag = np.where(cond2, grad_mag, 0)\n","        orientation_histogram[:, :, i] = uniform_filter(temp_mag, size=(cx, cy))[int(cx / 2)::cx, int(cy / 2)::cy].T\n","\n","    return orientation_histogram.ravel()\n","\n","\n","def color_histogram_hsv(im, nbin=10, xmin=0, xmax=255, normalized=True):\n","    ndim = im.ndim\n","    bins = np.linspace(xmin, xmax, nbin + 1)\n","    hsv = matplotlib.colors.rgb_to_hsv(im / xmax) * xmax\n","    imhist, bin_edges = np.histogram(hsv[:, :, 0], bins=bins, density=normalized)\n","    imhist = imhist * np.diff(bin_edges)\n","\n","    # return histogram\n","    return imhist"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUMpJIDQ92uQ","colab_type":"text"},"source":["## 2 - Reload the CIFAR-10 dataset"]},{"cell_type":"code","metadata":{"id":"OnrJGipX92uQ","colab_type":"code","colab":{}},"source":["try:\n","    del X_train, y_train\n","    del X_test, y_test\n","    print('Clear previously loaded data.')\n","except:\n","    pass\n","\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","mask = list(range(49000, 50000))\n","X_val = X_train[mask]\n","y_val = y_train[mask]\n","mask = list(range(49000))\n","X_train = X_train[mask]\n","y_train = y_train[mask]\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'forse', 'ship', 'truck']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlO2BiZ792uR","colab_type":"text"},"source":["## 3 - Extract Features"]},{"cell_type":"code","metadata":{"id":"5FNOS5pg92uR","colab_type":"code","colab":{}},"source":["num_color_bins = 20 # Number of bins in the color histogram\n","feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n","X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n","X_val_feats = extract_features(X_val, feature_fns)\n","X_test_feats = extract_features(X_test, feature_fns)\n","\n","# Preprocessing: Subtract the mean feature\n","mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n","X_train_feats -= mean_feat\n","X_val_feats -= mean_feat\n","X_test_feats -= mean_feat\n","\n","# Preprocessing: Divide by standard deviation. This ensures that each feature\n","# has roughly the same scale.\n","std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n","X_train_feats /= std_feat\n","X_val_feats /= std_feat\n","X_test_feats /= std_feat\n","\n","# Preprocessing: Add a bias dimension\n","X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n","X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n","X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rBfgwQ992uT","colab_type":"code","colab":{}},"source":["y_train = y_train.ravel()\n","y_val = y_val.ravel()\n","y_test = y_test.ravel()\n","\n","print('X_train_feats.shape =', X_train_feats.shape)\n","print('y_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NwUjiS2A92uV","colab_type":"text"},"source":["## 4 - Train a Two-Layer Neural Network"]},{"cell_type":"markdown","metadata":{"id":"aQ6n1u5C92uV","colab_type":"text"},"source":["Again, fine tune the network, and find the best hyperparameter (learning rate, regularizations, bins, hidden neuron, etc)\n","\n","Then train the network once again using feature space CIFAR10 dataset"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"bEt5bFZm92uV","colab_type":"code","colab":{}},"source":["loss, W, b, _, _ = train_two_layer_relu(X_train_feats, y_train, X_val_feats, y_val,\n","                                  hidden_size=100,\n","                                  num_iters=1000,\n","                                  learning_rate = 0.9,\n","                                  reg = 0.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwRICvC692uX","colab_type":"text"},"source":["This approach should outperform all previous approaches: you should easily be able to achieve over 55% classification accuracy on the test set; \n","\n","our best model achieves **about 60% classification accuracy**."]},{"cell_type":"code","metadata":{"id":"LnVQn2aS92uX","colab_type":"code","colab":{}},"source":["y_pred = predict_two_layer_relu(X_val_feats, W, b)\n","\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","\n","print('Validation Accuracy =', accuracy*100,'%')\n","print('Test label      =',y_val[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"R9aMIIpv92uY","colab_type":"code","colab":{}},"source":["y_pred = predict_two_layer_relu(X_test_feats, W, b)\n","\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label      =',y_test[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZtAodYw092uZ","colab_type":"text"},"source":[" An important way to gain intuition about how an algorithm works is to visualize the mistakes that it makes. \n"," \n"," In this visualization, we show examples of images that are misclassified by our current system. \n"," \n"," The first column  shows images that our system labeled as \"plane\" but whose true label is  something other than \"plane\"."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"SsrzrAtt92uZ","colab_type":"code","colab":{}},"source":["examples_per_class = 8\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for cls, cls_name in enumerate(classes):\n","    idxs = np.where((y_test != cls) & (y_pred == cls))[0]\n","    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n","    for i, idx in enumerate(idxs):\n","        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n","        plt.imshow(X_test[idx].astype('uint8'))\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls_name)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQqQ_gcO92ub","colab_type":"text"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 2\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"bvBk-u7_92ub","colab_type":"text"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}