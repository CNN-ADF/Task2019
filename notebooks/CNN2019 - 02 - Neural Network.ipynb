{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNN2019 - 02 - Neural Network.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fqiFD42_92oE"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","\n","# Task 2 - Artificial Neural Network\n","\n","\n","In this assignment you will practice putting together a simple image classification pipeline, based on the Two-layer Neural Network classifier. The goals of this assignment are as follows:\n","\n","    * understand the basic Image Classification pipeline using Multi-layered Neural Network\n","    * understand the train/val/test splits and the use of validation data for hyperparameter tuning.\n","    * develop proficiency in writing efficient vectorized code with numpy\n","\n","    * implement and apply various classic activation functions\n","    * implement and apply mini-batch gradient descent\n","    * implement and apply regularization\n","    * implement and apply hyperparameter finetuning strategy\n","\n","    * understand the differences and tradeoffs between these classifiers\n","    * the use of feature extraction to boost neural net performance\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vXUed3x_ROCm","colab_type":"text"},"source":["# [Part 0] Import Libraries"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JLQflrey92oM","colab":{}},"source":["import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.set_printoptions(precision=7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bLZgxpOj92of"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DdVpN5a392on","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = 123456\n","Nama = \"\"\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oPQaktKa92oy"},"source":["# [Part 1] Binary Single Layer Perceptron"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H-w7dUV-92o0"},"source":["## 1 - Load Dataset\n","\n","For this exercise, we will use binary class data to recognize `cats` and `not cats`. \n","The images are $64\\times64$ in dimension.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"c_mXeRo_92o2"},"source":["---\n","### a. Load Cat Dataset\n","first, load the dataset\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1PAp0FCP92o5","colab":{}},"source":["import h5py    \n","    \n","def load_dataset():\n","    !wget 'https://raw.githubusercontent.com/cnn-adf/Task2019/master/resources/catvnoncat.h5'\n","    dataset = h5py.File('catvnoncat.h5', \"r\")\n","\n","    train_set_x_orig = np.array(dataset[\"X_train\"][:]) # your train set features\n","    train_set_y_orig = np.array(dataset[\"y_train\"][:]) # your train set labels\n","    val_set_x_orig = np.array(dataset[\"X_val\"][:]) # your val set features\n","    val_set_y_orig = np.array(dataset[\"y_val\"][:]) # your val set labels\n","    classes = np.array(dataset[\"classes\"][:]) # the list of classes\n","    \n","    return train_set_x_orig, train_set_y_orig, val_set_x_orig, val_set_y_orig, classes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fliDuquz92o-","colab":{}},"source":["X_train_ori, y_train, X_val_ori, y_val, classes = load_dataset()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KzllPE1-92pK","colab":{}},"source":["print(X_train_ori.shape)\n","print(y_train.shape)\n","print(X_val_ori.shape)\n","print(y_val.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rlJTqCwf92pb"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","(209, 64, 64, 3)\n","(209, 1)\n","(50, 64, 64, 3)\n","(50, 1)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FTP4J31iBS9G"},"source":["View some data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xF0d5gc092pf","scrolled":true,"colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(18,5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train_ori[i+j*10])\n","        ax[j,i].set_title(classes[y_train[i+j*10,0]].decode(\"utf-8\") )\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MqIf_DT2Rd8h","colab_type":"text"},"source":["---\n","### b. Reshape and Normalize Data"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qG_vGaXa92pq"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","    * Reshape `X_train_ori` and `X_val_ori` into 1-dimensional matrix, \n","    * Store it as `X_train` and `X_val`\n","    * the `X_train_ori` and `X_val_ori` shape should still be \n","      (209, 64, 64, 3) and (50, 64, 64, 3)\n","\n","<br>\n","\n","*Hint: use `np.reshape()`*"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TBZDD3K392pt","colab":{}},"source":["X_train = ??\n","X_val = ??\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ggTFmgxc92p4","colab":{}},"source":["print('before')\n","print(X_train_ori.shape)\n","print(X_val_ori.shape)\n","\n","print('\\nafter')\n","print(X_train.shape)\n","print(X_val.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_0DDZmAw92p_"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","before\n","(209, 64, 64, 3)\n","(50, 64, 64, 3)\n","\n","after\n","(209, 12288)\n","(50, 12288)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3gDM9TnZ92qE"},"source":["\n","<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","    * Since for this dataset using sigmoid and regression is enough, \n","    * standarize the dataset into a `range of 0-1` by dividing it with `255`"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pT3thgML92qF","scrolled":true,"colab":{}},"source":["print('before',X_train[0,:6])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"at6lTm-D92qM"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","before [17 31 56 22 33 59]"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n1vwifEp92qM","colab":{}},"source":["# divide with `255`\n","# X_train = ??\n","# X_val = ??\n","\n","X_train = X_train/ 255.\n","X_val = X_val/255."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sU4qOipk92qT","colab":{}},"source":["print('after',X_train[0,:6])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"79aD_qGt92qZ"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","after [0.0666667 0.1215686 0.2196078 0.0862745 0.1294118 0.2313725]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UV6I5RpY92qa"},"source":["---\n","## 2 - Basic Neurons\n","\n","Standard neuron is basically the same as previous linear function. \n"]},{"cell_type":"markdown","metadata":{"id":"F6UJH2vRRga3","colab_type":"text"},"source":["---\n","### a. Forward and Backward Affine Function\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kre6pR7J92qc"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","    * Implement Affine forward function:\n","\n","$$\n","\\begin{align}\n","f(x, W, b) = x.W + b\n","\\end{align}\n","$$\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wUZykTok92qd","colab":{}},"source":["def affine_forward(x, W, b):  \n","  \n","    v = ??            # x dot W + b   \n","    \n","    return v"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aJak83yX92qj"},"source":["\n","\n","<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","    * Implement affine backward function:\n","\n","\n","$$\n","\\begin{align*}\n","\\partial W & = x^T.\\partial out \\\\\n","\\partial b & = \\sum \\partial out \\\\\n","\\partial x & = \\partial out.W^T \\\\\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k2Fnd_Sm92qn","colab":{}},"source":["def affine_backward(dout, x, W, b):\n","  \n","    dW = ??           # x.T dot dout\n","    db = ??           # sum dout, axis=0, keepdims=True\n","    dx = ??           # dout dot W.T\n","    \n","    return dW, db, dx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Sz-H5feRj9U","colab_type":"text"},"source":["---\n","### b. Forward and Backward Sigmoid Function\n","Also implement the activation function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i9ghf2tn92qr"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","    * Implement Sigmoid forward function:\n","\n","$$\n","\\begin{align}\n","f(x) = \\sigma(x) = \\frac{1}{1+e^{-v}}\n","\\end{align}\n","$$\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NGRKY2nw92qr","colab":{}},"source":["def sigmoid_forward(x):  \n","  \n","    out = ??\n","\n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5P8liX-zSu5K","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2413rY9h92qw","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","ds = sigmoid_forward(x)\n","\n","print(ds)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3VSnRFi992q4"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [0.1192029 0.2689414 0.5       0.7310586 0.8807971 0.9525741]\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LrsactKq92q7"},"source":["---\n","<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","    * Implement Sigmoid backward function\n","$$\n","\\begin{align*}\n","\\sigma'(x) = \\sigma(x) \\ (1 - \\sigma(x))\\\\\\\\\n","\\partial out = \\partial out . \\sigma'(x)\n","\\end{align*}\n","$$\n","<br>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mipLpvTu92q9","colab":{}},"source":["def sigmoid_backward(dout, ds):\n","    \"\"\"\n","    Argument:\n","        ds: sigmoid forward result\n","        dout: gradient error\n","    \"\"\"\n","    \n","    # calculate the local gradient of sigmoid\n","    ds_prime = ??\n","    \n","    # calculate the gradient propagation\n","    dout = ??\n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z06Z-t0bS2tg"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BHSGzegX92rD","colab":{}},"source":["np.random.seed(10)\n","dout = np.random.random((6,)) \n","dout = sigmoid_backward(dout, ds)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VDVWCLn692rI"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [0.0809837 0.0040801 0.1584121 0.1472238 0.05234   0.0101556]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YGVmVkzkRm-a","colab_type":"text"},"source":["---\n","### c. Forward and Backward Tanh Function\n","\n","We haven't discussed aboout activation function yet in class, but the implementation of these functions are quite easy, so you should be able to handle it"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5XSSziqF92rK"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","    * Implement `Tanh` forward function\n","\n","**hint: use `np.tanh(x)`*\n","\n","$$\n","\\begin{align}\n","f(x) = tanh(x)\n","\\end{align}\n","$$\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Qjajerxs92rL","colab":{}},"source":["def tanh_forward(x):  \n","  \n","    out = ??\n","    \n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KV4PuTfNS78R"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2OVdzDgl92rP","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","dt = tanh_forward(x)\n","\n","print(dt)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9gC2lOp092rT"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [ -0.9640276 -0.7615942  0.         0.7615942  0.9640276  0.9950548]\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qZYKxELm92rX"},"source":["---\n","<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","    * Implement Tanh backward function\n","$$\n","\\begin{align*}\n","f'(x) = 1-f(v)^2\\\\\n","\\\\\n","\\partial out = \\partial out . f'(x)\n","\\end{align*}\n","$$\n","<br>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oOX1RX_-92rZ","colab":{}},"source":["def tanh_backward(dout, dt):\n","    \"\"\"\n","    Argument:\n","        dt: tanh forward result\n","        dout: gradient error\n","    \"\"\"\n","    \n","    # calculate the local gradient of tanh\n","    dt_prime = ??           # 1- dt^2\n","    \n","    # calculate the gradient propagation\n","    dout = ??\n","    \n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Z5NZVgcoS9bg"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EnASa_P092rc","scrolled":false,"colab":{}},"source":["np.random.seed(10)\n","dout = np.random.random((6,)) \n","dout = tanh_backward(dout, dt)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y0os4dMq92rh"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [0.0544944 0.0087153 0.6336482 0.3144784 0.0352199 0.0022179]\n"]},{"cell_type":"markdown","metadata":{"id":"4Ye2Qs9Hvh69","colab_type":"text"},"source":["---\n","### d. Forward and Backward ReLU Function\n","\n","Next we implement ReLU activation function\n","\n","We haven't discussed aboout activation function yet in class, but the implementation of these functions are quite easy, so you should be able to handle it\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b_9cX36A92sh"},"source":["#### <font color='red'>**EXERCISE :** </font>\n","* Implement ReLU forward function:\n","\n","\n","$$\n","\\begin{align}\n","f(x) = \n","\\begin{cases}\n","0, & \\text{for } x<0\\\\\n","x, & \\text{for } x\\geq0\n","\\end{cases}\n","\\end{align}\n","$$\n","\n","**hint: use `np.maximum()`*"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eBu0Q6-Y92si","colab":{}},"source":["def relu_forward(x): \n","  \n","    out = ??\n","    \n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p90PC09F3y7w"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mRZtpSAP92sk","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","dr = relu_forward(x) \n","\n","print(dr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_pwj_6xn92so"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [0, 0, 0, 1, 2, 3]\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2t_YjdFX92sp"},"source":["---\n","\n","<br>\n","\n","#### <font color='red'>**EXERCISE :** </font>\n","* Implement ReLU backward function\n","$$\n","\\begin{align*}\n","f'(v) = \n","\\begin{cases}\n","0, & \\text{for } x<0\\\\\n","1, & \\text{for } x\\geq0\n","\\end{cases}\\\\\n","\\\\\n","\\partial out = \\partial out . f'(x)\n","\\end{align*}\n","$$\n","\n","**hint: use `np.where(condition, if true, if false)`*"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fjRGgdT-92sq","colab":{}},"source":["def relu_backward(dout, x):\n","    \"\"\"\n","    Argument:\n","        x: relu input\n","        dout: gradient error\n","    \"\"\"\n","    \n","    # calculate the local gradient of relu\n","    dr_prime = ??\n","    \n","    # calculate the gradient propagation\n","    dout = ??\n","    \n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YWS4ZUDv3zTX"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j07YdL5i92ss","colab":{}},"source":["np.random.seed(10)\n","dout = np.random.random((6,)) \n","dout = relu_backward(dout, x)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nMTHkOiR92sw"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [0.        0.        0.        0.7488039 0.498507  0.2247966]\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AA897jo6ZFv6"},"source":["---\n","## 3 - One-Layer Sigmoid\n","\n","We'll train a Single Layer Perceptron with Sigmoid for binary Classification using Full batch **Gradient Descent**\n"]},{"cell_type":"markdown","metadata":{"id":"_DJOVwlSRtqG","colab_type":"text"},"source":["\n","### a. Training Function\n","\n","The network architecture should be: <br>\n","<pre><font color=\"blue\"><b>Input - FC layer - Sigmoid</b></font></pre>\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vZ29WzMY92ri"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Implement Training Function\n","\n","    * call affine forward function\n","    * call sigmoid forward function\n","    * apply L2 loss regression (MSE)\n","    * call affine backward function\n","    * implement weight update\n","\n","<br>\n","\n","**Note** that we do not calculate `sigmoid backward` as in Single Layer Perceptron `sigmoid` is used directly as output activation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-qVsxC3w92rj","colab":{}},"source":["def train_one_layer(X, y, W=None, b=None, lr=0.005, epochs=100, verbose=True):\n","    \n","    \n","    num_train, dim = X.shape\n","    \n","    num_classes = 1\n","    \n","    # initialize weights if not provided\n","    if W is None:\n","        W = 0.02*np.random.rand(dim, num_classes)\n","    if b is None:\n","        b = np.zeros((1, num_classes))\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","                     \n","    for ep in range(epochs):\n","        \n","\n","        # calculate 1st layer score by calling affine forward function using X, W, and b\n","        layer1 = ??\n","        \n","        # calculate 1st activation score by calling sigmoid forward function using layer1 score\n","        act1 = ??\n","        \n","        # calculate error by subtracting act1 with y\n","        error = ??\n","        \n","        # calculate L2 Loss (MSE) by averaging the squared error (mean)\n","        loss = ??  \n","        \n","        # divide error by num_train\n","        error /= num_train\n","    \n","        # calculate layer 1 weights gradient by calling affine backward function using error, X, W, and b\n","        dW, db, _ = ??\n","\n","        \n","        # perform parameter update by subtracting each W and b with a fraction of dW and db\n","        # according to the learning rate\n","        W = ??                         # W-lr*dW\n","        b = ??                         # b-lr*db\n","\n","        if verbose and ep % 100 == 0:\n","            print ('epoch',ep,'/',epochs, ': loss =', loss)\n","            \n","            # append the loss history\n","            loss_history.append(loss)\n","\n","    print('Training done')\n","    \n","    return W, b, loss_history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ye8c9cZH92rn"},"source":["### b. Train the Binary Classifier\n","\n","Try the training Function using the initial parameter"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k6LGU9zl92rn","colab":{}},"source":["W, b, loss = train_one_layer(X_train, y_train, epochs=1000, lr=0.005)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sdYOq5TW8Xwm"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>The loss should starts around 0.6 and ends around 0.05"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7x7U1LUT92rr"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eppOYFhA92rr","colab":{}},"source":["plt.rcParams['figure.figsize'] = [12, 5]\n","plt.plot(loss)\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IR9hWMFBR6Sr","colab_type":"text"},"source":["\n","### c. Predict Function\n","\n","The network architecture should be: <br>\n","<pre><font color=\"blue\"><b>Input - FC layer - Sigmoid</b></font></pre>\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FvQ7aiRT92rv"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Implement Predict Function\n","\n","    * call forward function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aB-Mcp8g92rw","colab":{}},"source":["def predict_one_layer(X, W, b):    \n","    y_pred = np.zeros(X.shape[1])\n","    \n","    # calculate 1st layer score by calling affine forward function using X, W, and b\n","    layer1 = ??\n","\n","    # calculate 1st activation score by calling sigmoid forward function using layer1 score\n","    act1 = ??\n","    \n","    # since it's a binary class, round the score to get the class\n","    y_pred = np.round(act1)\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QJdIRG2G92ry"},"source":["### d. Training Accuracy\n","Calculate the Training Accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e8psteqc92rz","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_one_layer(X_train, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15].ravel())\n","print('Predicted label =',y_pred[:15].astype('int').ravel())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2yzbf36o92r4"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should get about <b>~95%</b> accuracy on training set using the initial run</pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ei9VFwzL92r5"},"source":["Calculate the Validation Accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bxCkHhzp92r6","colab":{}},"source":["y_pred = predict_one_layer(X_val, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15].ravel())\n","print('Predicted label  =',y_pred[:15].astype('int').ravel())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MMh1rxBj92r9"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should also get about <b>~74%</b> accuracy on validation set</pre>\n","\n","<br>\n","\n","You can retrain further the weights by adding the pre-trained W and b to the arguments when calling training function\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gDDBQVUgM_OT","colab":{}},"source":["# loss, W, b = train_one_layer(X_train, y_train, W=W, b=b, num_iters=1000, learning_rate=0.005)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X2piJd0x92r9"},"source":["---\n","---\n","# [Part 2] Multi-Layered Perceptron\n","\n","Now, let's build some multi-layered Neural Networks and train it using CIFAR-10 dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zcyRACfL92r-"},"source":["---\n","## 1 - Load CIFAR-10 Dataset\n","\n","CIFAR-10 dataset is a image classification dataset, consisting of 10 classes. \n","\n","The images are $32\\times32$ color image with 50,000 data train and 10,000 data test"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M2X2LYjB92r_"},"source":["---\n","### a. Import Data CIFAR-10"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rvv-wROB92r_","colab":{}},"source":["import tensorflow as tf\n","\n","(X_train_ori, y_train_ori), (X_test_ori, y_test_ori) = tf.keras.datasets.cifar10.load_data()\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","print('X_train.shape =',X_train_ori.shape)\n","print('y_train.shape =',y_train_ori.shape)\n","print('X_test.shape  =',X_test_ori.shape)\n","print('y_test.shape  =',y_test_ori.shape)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ta7ftQi-92sC"},"source":["---\n","### b. Visualizing Data\n","Show the first 20 images from `X_train`"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RgmpnQx092sD","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train_ori[i+j*10])\n","        ax[j,i].set_title(classes[y_train_ori[i+j*10,0]])\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JP_N9cAB92sG"},"source":["---\n","### c. Split Training Data\n","\n","Get the last 1000 data from Training Set as Validation Set"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fQsc4Wa192sI","colab":{}},"source":["X_val = X_train_ori[-1000:,:]\n","y_val = y_train_ori[-1000:]\n","\n","X_train = X_train_ori[:-1000, :]\n","y_train = y_train_ori[:-1000]\n","\n","X_test = X_test_ori\n","y_test = y_test_ori\n","\n","print('X_val.shape   =',X_val.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XLpqKlTJ92sL"},"source":["---\n","### d. Normalizing Data\n","Normalize `X_train`, `X_val` and `X_test` by *zero-centering* them:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YPJiHpgC92sM","colab":{}},"source":["X_train = X_train.astype('float32')\n","X_val = X_val.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","mean_image = np.mean(X_train, axis = 0)\n","X_train -= mean_image\n","X_val -= mean_image\n","X_test -= mean_image\n","\n","print('np.mean(X_train) =',np.mean(X_train))\n","print('np.mean(X_val)   =',np.mean(X_val))\n","print('np.mean(X_test)  =',np.mean(X_test))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OGEk3WMy92sX"},"source":["---\n","### e. Reshape Data\n","Reshape each data in `X_train`, `X_val` and `X_test` into 1-dimensional matrix\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WmW3BZn692sZ","colab":{}},"source":["X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]*X_train.shape[3]))\n","X_val = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]*X_val.shape[3]))\n","X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]*X_test.shape[3]))\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fMvowg3l92sd"},"source":[" Reshape `y_train`, `y_val` and `y_test` into a vector \n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9pE9Pgsz92sd","colab":{}},"source":["y_train = y_train.ravel()\n","y_val = y_val.ravel()\n","y_test = y_test.ravel()\n","\n","print('y_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BfTpR-9h92tI"},"source":["---\n","## 2 - Softmax Function \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S1Q0Yrag92tJ"},"source":["### a. Softmax Score\n","\n","The implementation is the same as previous Task, so in here we already provide you with the implementation\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p5n9QYlx92tK","colab":{}},"source":["def softmax(x):  \n","    \n","    # shift x by subtracting with its maximum value . Use np.max(...)\n","    x -= np.max(x)\n","    \n","    # Apply exp() element-wise to x. Use np.exp(...).    \n","    x_exp = np.exp(x)\n","    \n","    # Create a vector X_sum that sums each row of X_exp. Use np.sum(..., axis = 1, keepdims = True).\n","    x_sum = np.sum(x_exp, axis = 1, keepdims = True)  \n","    \n","    # Compute softmax(x) score by dividing X_exp by X_sum. It should automatically use numpy broadcasting.\n","    score = x_exp / x_sum\n","    \n","    return score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2Yxwqh0m92tM"},"source":["### b. Softmax Loss\n","\n","Implement a softmax loss function using numpy. The implementation is the same as previous Task, so in here we already provide you with the implementation\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"py4nnXFO92tM","colab":{}},"source":["def softmax_loss(score, y):\n","   \n","    num_examples = score.shape[0]\n","    \n","    #make a number list containing [1 2 3 ... n]\n","    number_list = range(num_examples)\n","    \n","    # calculate the correct log probability of score[number_list,y] by applycing -np.log(...)\n","    corect_logprobs = -np.log(score[number_list,y])\n","    \n","    # average the correct log probability, use np.sum then divide it by num_examples\n","    loss = np.sum(corect_logprobs)/num_examples\n","    \n","    \n","    # 3. COMPUTE THE GRADIENT ON SCORES\n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    \n","    return loss, dscores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rCisaNV8Yz30"},"source":["---\n","## 3 - Two-Layered Sigmoid with Softmax"]},{"cell_type":"markdown","metadata":{"id":"08nhrsctSIce","colab_type":"text"},"source":["---\n","\n","### a. Training Function\n","\n","The network architecture should be: \n","<pre><font color=\"blue\"><b>Input - FC layer - Sigmoid - FC Layer - Softmax</b></font></pre>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0A_RVWT692tO"},"source":["\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement Training Function\n","\n","    * call affine forward function\n","    * call sigmoid forward function\n","    * call affine forward function\n","    \n","    * call softmax function\n","    * call softmax_loss function\n","    \n","    * call affine backward function\n","    * call sigmoid backward function\n","    * call affine backward function\n","    \n","    * implement weight update"]},{"cell_type":"code","metadata":{"id":"dXa9ftd803Ki","colab_type":"code","colab":{}},"source":["def train_two_layer_sigmoid(X, y, hidden_size, W=None, b=None, \n","                            lr=1e-4, lr_decay=0.9, reg=0.25, \n","                            epochs=100, batch_size=200, verbose=True):\n","    num_train, dim = X.shape\n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","    \n","    num_classes = np.max(y) + 1  # assume y takes values 0...K-1 where K is number of classes\n","    \n","    # initialize weights if not provided\n","    if W is None:\n","        W0 = 1e-4 * np.random.randn(dim, hidden_size)\n","        W1 = 1e-4 * np.random.randn(hidden_size, num_classes)\n","        W = [W0, W1]\n","    if b is None:\n","        b0 = np.zeros((1,hidden_size))\n","        b1 = np.zeros((1,num_classes))\n","        b = [b0, b1]\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","                     \n","    for ep in range(epochs):\n","        # Shuffle data train index\n","        train_rows = np.arange(num_train)\n","        np.random.shuffle(train_rows)\n","        \n","        # split index into mini batches\n","        id_batch = np.split(train_rows, num_iter)\n","        \n","        for batch in id_batch:\n","          \n","            # get mini batch data and label\n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","\n","\n","            # calculate 1st layer score by calling affine forward function using X_batch, W[0], and b[0]\n","            layer1 = ??\n","\n","\n","            # calculate 1st activation score by calling sigmoid forward function using layer1 score\n","            act1 = ??\n","\n","\n","            # calculate 2nd layer score by calling affine forward function using act1, W[1], and b[1]\n","            layer2 = ??\n","\n","\n","            # calculate softmax score by calling softmax function using layer2 score\n","            softmax_score = ??\n","\n","\n","            # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n","            loss, dout = ??\n","\n","\n","            # add regularization to the loss\n","            loss+= reg * (np.sum(W[0] * W[0]) + np.sum(W[1] * W[1]))\n","\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","            # calculate layer 2 weights gradient by calling affine backward function using dout, act1, W[1], and b[1]\n","            dW1, db1, dact1 = ??\n","\n","\n","            # calculate sigmoid gradient by calling sigmoid backward function using dact1 and act1 score\n","            dlayer1 = ??\n","\n","\n","            # calculate layer 1 weights gradient by calling affine backward function using dlayer1, X_batch, W[0], and b[0]\n","            dW0, db0, _ = ??\n","\n","\n","            # perform regulatization gradient\n","            dW1 += 2 * reg * W[1]\n","            dW0 += 2 * reg * W[0]\n","\n","            # perform parameter update by subtracting W and b with a fraction of dW and db\n","            # according to the learning rate\n","            W[0] = ??         # W0 - lr * dW0\n","            b[0] = ??         # b0 - lr * db0\n","            W[1] = ??         # W1 - lr * dW1\n","            b[1] = ??         # b1 - lr * db1\n","        \n","            # iteration count\n","            it +=1\n","\n","            if verbose and it % 100 == 0:\n","                print ('iteration',it,'(epoch', ep,'/',epochs, '): loss =', loss)\n","                \n","                \n","        # At the end of one epoch:\n","        # Decay learning rate\n","        lr *= lr_decay\n","            \n","    print('Training Done')\n","    return W, b, loss_history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-_BCDToT92tQ"},"source":["---\n","### b. Train the Softmax Classifier\n","\n","Try the training Function using the initial parameter"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nACDxIkF92tQ","colab":{}},"source":["W_sigm, b_sigm, loss = train_two_layer_sigmoid(X_train, y_train, hidden_size=50, epochs=15)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ba2P8It0T-jb","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>The loss should starts around 2.3 and ends around 2.27</pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a2jFi4gF92tR"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"41z1XYZ_92tT","colab":{}},"source":["plt.rcParams['figure.figsize'] = [12, 5]\n","plt.plot(loss)\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SLY2ozMwSRoK","colab_type":"text"},"source":["---\n","### c. Predict Function\n","Implement Predict function\n","\n","The network architecture should be: \n","<pre><font color=\"blue\"><b>Input - FC layer - Sigmoid - FC Layer - argmax</b></font></pre>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HHZUjG9V92tU"},"source":["\n","#### <font color='red'>**EXERCISE:** </font> \n","\n","Implement Predict Function\n","\n","    * call affine forward function\n","    * call sigmoid forward function\n","    * call affine forward function\n","    * call argmax to get max score id\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"unreYTQZ92tW","colab":{}},"source":["def predict_two_layer_sigmoid(X, W, b):    \n","    y_pred = np.zeros(X.shape[1])\n","\n","    \n","    # calculate 1st layer score by calling affine forward function using X_batch, W[0], and b[0]\n","    layer1 = ??\n","\n","\n","    # calculate 1st activation score by calling sigmoid forward function using layer1 score\n","    act1 = ??\n","\n","\n","    # calculate 2nd layer score by calling affine forward function using act1, W[1], and b[1]\n","    layer2 = ??\n","\n","    \n","    # take the maximum prediction from layer 2 and use that column to get the class   \n","    # use np.argmax with axis=-1 \n","    y_pred = ??\n","\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ABy48A5e-GCE"},"source":["### d. Training Accuracy\n","Calculate the Training Accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n2g7l0Aa92tX","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_two_layer_sigmoid(X_train, W_sigm, b_sigm)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-Cdus5LE92ta"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should get about <b>~18%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qEotzZEw92ta","colab":{}},"source":["y_pred = predict_two_layer_sigmoid(X_val, W_sigm, b_sigm)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yRcDOPQf92tc"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should also get about <b>~19%</b> accuracy on validation set</pre>\n","\n","<br>\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wyRhOJwgVVtW","colab":{}},"source":["# W_sigm, b_sigm, loss = train_two_layer_sigmoid(X_train, y_train, W=W_sigm, b=b_sigm, hidden_size=50, epochs=3000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8W2SdTgvQTU","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SjuSV6z7vQ0W"},"source":["---\n","## 4 - Two-Layer ReLU with Softmax\n","\n","Now we implement Two-Layer Neural Network, but this time we're using ReLU activation function\n","\n","By the end of this part you should see that ReLU converge much faster compared to Sigmoid"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QMJhNLHjvQ0b"},"source":["\n","### a. Predict Function\n","\n","This time, we implement the predict function first, because we are going to use `predict` function inside the `training` function to track the `validation` accuracy \n","\n","The network architecture should be: \n","<pre><font color=\"blue\"><b>Input - FC layer - ReLU - FC Layer - argmax\n"]},{"cell_type":"markdown","metadata":{"id":"ojzMGLZkx47f","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE:**</font> \n","Implement Predict Function\n","\n","    * call affine forward function\n","    * call relu forward function\n","    * call affine forward function\n","    * call argmax to get max score id"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GeDqFBPqvQ0f","colab":{}},"source":["def predict_two_layer_relu(X, W, b):    \n","    y_pred = np.zeros(X.shape[1])\n","\n","    \n","    # calculate 1st layer score by calling affine forward function using X_batch, W[0], and b[0]\n","    layer1 = ??\n","\n","    # calculate 1st activation score by calling relu forward function using layer1 score\n","    act1 = ??\n","\n","    # calculate 2nd layer score by calling affine forward function using act1, W[1], and b[1]\n","    layer2 = ??\n","    \n","    # take the maximum prediction from layer 2 and use that column to get the class    \n","    # use np.argmax with axis=-1 \n","    y_pred = ?? \n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AF73Cw48yA3z","colab_type":"text"},"source":["### b. Training Function\n","\n","The network architecture should be: \n","<pre><font color=\"blue\"><b>Input - FC layer - ReLU - FC Layer - Softmax\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IB3tp7WGvQ0m"},"source":["\n","\n","#### <font color='red'>**EXERCISE :**</font> \n","\n","Implement Training Function\n","\n","    * implement mini batch gradient descent\n","    \n","    * call affine forward function\n","    * call relu forward function\n","    * call affine forward function\n","    \n","    * call softmax function\n","    * call softmax_loss function\n","    \n","    * call affine backward function\n","    * call relu backward function\n","    * call affine backward function\n","    \n","    * implement weight update\n","    * add weights regularization\n","    * calculate the training and validation accuracy\n","    * decay learning rate\n","    "]},{"cell_type":"code","metadata":{"id":"_L52hoRxXIqA","colab_type":"code","colab":{}},"source":["def train_two_layer_relu(X, y, X_val, y_val, hidden_size, \n","                         W=None, b=None, lr=1e-4, lr_decay=0.9, \n","                         reg=0.25, epochs=100, batch_size=200, verbose=True):\n","  \n","    num_train, dim = X.shape\n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","    \n","    num_classes = np.max(y) + 1  # assume y takes values 0...K-1 where K is number of classes\n","    \n","    # initialize weights if not provided\n","    if W is None:\n","        W0 = 1e-4 * np.random.randn(dim, hidden_size)\n","        W1 = 1e-4 * np.random.randn(hidden_size, num_classes)\n","        W = [W0, W1]\n","    if b is None:\n","        b0 = np.zeros((1,hidden_size))\n","        b1 = np.zeros((1,num_classes))\n","        b = [b0, b1]\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","                     \n","    for ep in range(epochs):\n","        # Shuffle data train index\n","        # ----->   see sigmoid train function   -----> \n","        train_rows = ??\n","        shuffle ??\n","        \n","        # split index into mini batches\n","        # ----->   see sigmoid train function   -----> \n","        id_batch = ??\n","        \n","        for batch in id_batch:\n","      \n","            # get mini batch data and label\n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","\n","            # calculate 1st layer score by calling affine forward function using X_batch, W[0], and b[0]\n","            layer1 = ??\n","\n","            # calculate 1st activation score by calling relu forward function using layer1 score\n","            act1 = ??\n","\n","            # calculate 2nd layer score by calling affine forward function using act1, W[1], and b[1]\n","            layer2 = ??\n","\n","            # calculate softmax score by calling softmax function using layer2 score\n","            softmax_score = ??\n","\n","            # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n","            loss, dout = ??\n","\n","            # add regularization to the loss:\n","            #    for each weights, calculate the sum square, multiply by regularization strength\n","            #    then add it to the loss      \n","            # ----->   see sigmoid train function   -----> \n","            loss = ?? \n","\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","            # calculate layer 2 weights gradient by calling affine backward function using dout, act1, W[1], and b[1]\n","            dW1, db1, dact1 = ??\n","\n","            # calculate sigmoid gradient by calling relu backward function using dact1 and act1 score\n","            dlayer1 = ??\n","\n","            # calculate layer 1 weights gradient by calling affine backward function using dlayer1, X_batch, W[0], and b[0]\n","            dW0, db0, _ = ??\n","\n","\n","            # perform regulatization gradient\n","            # for each dWi, add with twice of the weight multiplied by regularization strength\n","            # ----->   see sigmoid train function   -----> \n","            dW1 = ?? \n","            dW0 = ?? \n","\n","\n","            # perform parameter update by subtracting W and b with a fraction of dW and db\n","            # according to the learning rate \n","            W[0] = ?? \n","            b[0] = ?? \n","            W[1] = ?? \n","            b[1] = ?? \n","  \n","            # iteration count\n","            it +=1\n","\n","      \n","            if verbose and it % 100 == 0:\n","                print ('iteration',it,'(epoch', ep,'/',epochs, '): loss =', loss)\n","\n","            \n","        # At the end of one epoch:\n","        \n","        # 1. Check accuracy\n","        #    calculate the training accuracy by calling predict_two_layer_relu function on X_batch\n","        #    and compare it tu y_batch. Then calculate the mean correct (accuracy in range 0-1)\n","        train_acc = (predict_two_layer_relu(X_batch, W, b) == y_batch).mean()\n","        train_acc_history.append(train_acc)\n","        \n","\n","        # 2. Calculate the training accuracy by calling predict_two_layer_relu function on X_val\n","        #    and compare it tu y_val. Then calculate the mean correct (accuracy in range 0-1)\n","        val_acc = (predict_two_layer_relu(X_val, W, b) == y_val).mean()\n","        val_acc_history.append(val_acc)\n","        \n","\n","        # 3. Decay learning rate\n","        #    multiply learning rate with decay\n","        #    ----->   see sigmoid train function   -----> \n","        lr = ??\n","  \n","    # compile all history\n","    history = [loss_history, train_acc_history, val_acc_history]\n","    \n","    if verbose:\n","        print('Training Done')\n","      \n","    return W, b, history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4yoam4ZvvQ0t"},"source":["### c. Train the Softmax Classifier\n","\n","Try the training Function using the initial parameter"]},{"cell_type":"code","metadata":{"colab_type":"code","scrolled":true,"id":"jTunuQE4vQ0v","colab":{}},"source":["W_relu, b_relu, history = train_two_layer_relu(X_train, y_train, \n","                                               X_val, y_val, \n","                                               hidden_size=50, \n","                                               epochs=8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ngZzyJre1EVJ"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>The loss should starts around 2.3 and ends around 1.8 with only 8 epochs</pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xBBgZHM1vQ00"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"colab_type":"code","scrolled":true,"id":"dRdlLmlEvQ02","colab":{}},"source":["loss, train_acc, val_acc = history\n","\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aGBTu7o8vQ06"},"source":["Visualize the training and validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","scrolled":true,"id":"IgjLCW8HvQ08","colab":{}},"source":["plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.title('Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UkySJRvtQ-mx"},"source":["### d. Training Accuracy\n","Calculate the Training Accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CykD3x4_vQ1B","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_two_layer_relu(X_train, W_relu, b_relu)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vGdp3IzIvQ1G"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should be able to get about <b>~32%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d4raNUUhvQ1I","colab":{}},"source":["y_pred = predict_two_layer_relu(X_val, W_relu, b_relu)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8Ob5DVehvQ1N"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should also be able to get about <b>~33%</b> accuracy on validation set</pre>\n","\n","<br>\n","\n","You can retrain further the weights by adding the pre-trained W and b to the arguments when calling training function\n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"P8wEtCW-1d3I","colab_type":"code","colab":{}},"source":["# W_relu, b_relu, history = train_two_layer_relu(X_train, y_train, X_val, y_val, W=W_relu, b=b_relu, hidden_size=50, epochs=1000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1LsoAvV492tv"},"source":["## 5 - First Layer Visualization\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OiaTO7K192tv","colab":{}},"source":["## run this to turn off the scrolling effect in jupyter notebook\n","\n","from IPython.core.magics.display import Javascript\n","Javascript(\"\"\"\n","  IPython.OutputArea.prototype._should_scroll = function(lines) {\n","      return false;\n","  }\"\"\"\n",")\n","\n","## set return true to turn on the scrolling effect"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7T0XX9vRaqLN","colab":{}},"source":["!wget 'https://raw.githubusercontent.com/CNN-ADF/Task2019/master/resources/vis_utils.py'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"91xGTPaj92ty","scrolled":true,"colab":{}},"source":["from vis_utils import visualize_grid\n","\n","plt.rcParams['figure.figsize'] = [10, 10]\n","\n","plt.imshow(visualize_grid(W_relu[0].reshape(32, 32, 3, -1).transpose(3, 0, 1, 2), padding=3).astype('uint8'))\n","\n","plt.gca().axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MX7mAsBx92tz"},"source":["<pre>You should see that if you re-train the network, the weight result visualization will be different."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WH0hh11zdqcY"},"source":["---\n","# [Part 3] Hyperparameter Tuning\n","\n","Now, let's build a two-layered Neural Network and train it using CIFAR-10 dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wp7QWSwS92t0"},"source":["**What's wrong?**. \n","* Looking at the visualizations above, we see that the loss is decreasing **more or less linearly**, which seems to suggest that the learning rate may be too low. \n","* Moreover, there is **no gap between the training and validation** accuracy, suggesting that the model we used has low capacity, and that we should increase its size. \n","* On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n","\n","<br>\n","\n","**Tuning**. \n","* Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice.\n","* Below, you should experiment with different values of the various hyperparameters, including **hidden layer size**, **learning rate**, and **regularization strength**. \n","* You might also consider tuning the **learning rate decay**, but you should be able to get good performance using the default value.\n","\n","<br>\n","\n","**Approximate results**. \n","* You should be aim to achieve a classification accuracy of greater than **48% on the validation set**. \n","* Our best network gets over 52% on the validation set."]},{"cell_type":"markdown","metadata":{"id":"Gyd7mYV0SiAo","colab_type":"text"},"source":["---\n","## 1 - Fine Tune the Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RmCiEmWrROFw"},"source":["#### <font color=\"red\">**EXERCISE :**</font>\n","    * Use the validation set to tune hyperparameters (regularization strength and learning rate)\n","    * find the best learning rate and regularization strength using staged random search, (Coarse-to-Fine Search)\n","    * try to gradually decrease the random range to find the best learning rate and regularization strength\n","    * use only few epochs or iteration"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_CRrHw2a92t0","colab":{}},"source":["import warnings\n","import datetime\n","warnings.filterwarnings('ignore')\n","\n","results = {}\n","best_val = -1\n","best_reg = 0\n","best_lr = 0\n","\n","best_W = None\n","best_b = None\n","max_epoch = 3\n","max_trial = 30\n","\n","for trial in range(max_trial):\n","    \n","    reg = 10**np.random.uniform(-2, 1)    # <---------- you can try and change this <----------\n","    lr = 10**np.random.uniform(-2,-4)     # <---------- you can try and change this <----------    \n","    hidden_size = 200                     # <---------- you can try and change this <----------    \n","    \n","    W, b, H = train_two_layer_relu(X_train, y_train, X_val, y_val, hidden_size,\n","                                      epochs=max_epoch, batch_size=200, \n","                                      lr=lr, lr_decay=0.9, \n","                                      reg=reg, verbose=False) \n","    val_acc = (predict_two_layer_relu(X_val, W, b) == y_val).mean() \n","    if val_acc > best_val: \n","        best_W = W \n","        best_b = b \n","        best_val = val_acc \n","        best_lr  = lr \n","        best_reg = reg \n","    print(str(datetime.datetime.now()), ', val_acc:', val_acc, '\\tlr:', lr, \n","          '\\treg:', reg, '\\t', str(trial)+'/'+str(max_trial))\n","    \n","print (\"best regularizer  : \", best_reg)\n","print (\"best learning rate: \", best_lr)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dTMH_Wlc92t1"},"source":["#### <font color=\"red\">**EXERCISE :**</font>\n","    * Try different range of hyperparameter\n","    * Try different strategy to find the hyperparameter\n","    * Try to finetune the other hyperparameter such as number of hidden neuron and lr_decay\n","    * Try other architectures such as changing the activation function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DMDeUKJ192t2"},"source":["---\n","## 2 - Train the Network Fully\n","\n","When you are done experimenting,\n","\n","Train the network for longer epochs using the best **`learning rate`** and best **`regularization strength`**\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JcBCr4t-92t3","scrolled":true,"colab":{}},"source":["\n","print (\"regularizer  : \", best_reg)\n","print (\"learning rate: \", best_lr)\n","\n","best_W, best_b, history = train_two_layer_relu(X_train, y_train, X_val, y_val,\n","                                               W = best_W, b = best_b,\n","                                               hidden_size=50, epochs=15, \n","                                               lr = best_lr, reg = best_reg)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e7fucZM_92t8"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ud-yWUve92t-","scrolled":true,"colab":{}},"source":["loss, train_acc, val_acc = history\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1mKyrEWK92uB"},"source":["Visualize the training and validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_CPvhV6092uB","scrolled":false,"colab":{}},"source":["plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.title('Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FeNj2wz1REM0"},"source":["---\n","## 3 - Accuracy and Visualization\n","Calculate the Training Accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"li8UULY092uC","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_two_layer_relu(X_train, best_W, best_b)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uV6rk80a92uE"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>If you're careful, You should be able to get about <b>~60%</b> accuracy on training set "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NtgFFdM492uE","colab":{}},"source":["y_pred = predict_two_layer_relu(X_val, best_W, best_b)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8rEVaW_C92uF"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should also be able to get about <b>~50%</b> accuracy on validation set</pre>\n","\n","<br>\n","\n","You can retrain further the weights by adding the pre-trained W and b to the arguments when calling training function\n","\n","---\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kEGRYolZ92uF","scrolled":false,"colab":{}},"source":["plt.imshow(visualize_grid(best_W[0].reshape(32, 32, 3, -1).transpose(3, 0, 1, 2), padding=3).astype('uint8'))\n","\n","plt.gca().axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tRx-kvEc92uH"},"source":["---\n","## 4 - Test the Trained Weights\n","\n","Evaluate your final trained network on the test set; you should be able get **above 48%.**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hWA3_ob492uH","scrolled":true,"colab":{}},"source":["y_pred = predict_two_layer_relu(X_test, best_W, best_b)\n","\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label       =',y_test[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7qtpPpf9Oqcf"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>If you're careful, You should be able to get about <b>~48%</b> accuracy on test set "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p7GkxoGT92uK"},"source":["---\n","## 5 - Missclassified Images\n","An important way to gain intuition about how an algorithm works is to visualize the mistakes that it makes. \n"," \n"," In this visualization, we show examples of images that are misclassified by our current system. \n"," \n"," The first column  shows images that our system labeled as \"plane\" but whose true label is  something other than \"plane\"."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"J1h57y3h92uL","scrolled":true,"colab":{}},"source":["plt.rcParams['figure.figsize'] = [10, 10]\n","\n","print('\\n\\n    missclassified images\\n')\n","examples_per_class = 8\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for cls, cls_name in enumerate(classes):\n","    idxs = np.where((y_test != cls) & (y_pred == cls))[0]\n","    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n","    for i, idx in enumerate(idxs):\n","        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n","        plt.imshow(X_test_ori[idx].astype('uint8'))\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls_name)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lzG3ZWZq92uM"},"source":["---\n","---\n","# [Optional] Two-Layer NeuralNet on Feature Space\n","\n","We have seen that we can achieve reasonable performance on an image classification task by training a linear classifier on the pixels of the input image. In this exercise we will show that we can improve our classification performance by training linear classifiers not on raw pixels but on features that are computed from the raw pixels.\n","\n","All of your work for this exercise will be done in this notebook."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dN-REYCMEPOi","colab":{}},"source":["from __future__ import print_function\n","\n","from scipy.ndimage import uniform_filter"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"I97mZ8U_92uM"},"source":["## 1 - Feature Extraction Functions\n","* For each image we will compute a Histogram of Oriented Gradients (HOG) as well as a color histogram using the hue channel in HSV color space. We form our final feature vector for each image by concatenating the HOG and color histogram feature vectors.\n","\n","* Roughly speaking, HOG should capture the texture of the image while ignoring color information, and the color histogram represents the color of the input image while ignoring texture. \n","* As a result, we expect that using both together ought to work better than using either alone. Verifying this assumption would be a good thing to try for your interests.\n","\n","* The `hog_feature` and `color_histogram_hsv` functions both operate on a single\n","image and return a feature vector for that image.\n","* The `extract_features` function takes a set of images and a list of feature functions and evaluates each feature function on each image, storing the results in a matrix where\n","each column is the concatenation of all feature vectors for a single image."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D3p77eRO92uN","colab":{}},"source":["def extract_features(imgs, feature_fns, verbose=False):\n","    num_images = imgs.shape[0]\n","    if num_images == 0:\n","        return np.array([])\n","\n","    # Use the first image to determine feature dimensions\n","    feature_dims = []\n","    first_image_features = []\n","    for feature_fn in feature_fns:\n","        feats = feature_fn(imgs[0].squeeze())\n","        assert len(feats.shape) == 1, 'Feature functions must be one-dimensional'\n","        feature_dims.append(feats.size)\n","        first_image_features.append(feats)\n","\n","    # Now that we know the dimensions of the features, we can allocate a single\n","    # big array to store all features as columns.\n","    total_feature_dim = sum(feature_dims)\n","    imgs_features = np.zeros((num_images, total_feature_dim))\n","    imgs_features[0] = np.hstack(first_image_features).T\n","\n","    # Extract features for the rest of the images.\n","    for i in range(1, num_images):\n","        idx = 0\n","        for feature_fn, feature_dim in zip(feature_fns, feature_dims):\n","            next_idx = idx + feature_dim\n","            imgs_features[i, idx:next_idx] = feature_fn(imgs[i].squeeze())\n","            idx = next_idx\n","        if verbose and i % 1000 == 0:\n","            print('Done extracting features for %d / %d images' % (i, num_images))\n","\n","    return imgs_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TIJ4x5lkEVnF","colab":{}},"source":["def rgb2gray(rgb):\n","    return np.dot(rgb[..., :3], [0.299, 0.587, 0.144])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OWJfslUI92uO","colab":{}},"source":["def hog_feature(im):\n","\n","    # convert rgb to grayscale if needed\n","    if im.ndim == 3:\n","        image = rgb2gray(im)\n","    else:\n","        image = np.at_least_2d(im)\n","\n","    sx, sy = image.shape  # image size\n","    orientations = 9  # number of gradient bins\n","    cx, cy = (8, 8)  # pixels per cell\n","\n","    gx = np.zeros(image.shape)\n","    gy = np.zeros(image.shape)\n","    gx[:, :-1] = np.diff(image, n=1, axis=1)  # compute gradient on x-direction\n","    gy[:-1, :] = np.diff(image, n=1, axis=0)  # compute gradient on y-direction\n","    grad_mag = np.sqrt(gx ** 2 + gy ** 2)  # gradient magnitude\n","    grad_ori = np.arctan2(gy, (gx + 1e-15)) * (180 / np.pi) + 90  # gradient orientation\n","\n","    n_cellsx = int(np.floor(sx / cx))  # number of cells in x\n","    n_cellsy = int(np.floor(sy / cy))  # number of cells in y\n","    # compute orientations integral images\n","    orientation_histogram = np.zeros((n_cellsx, n_cellsy, orientations))\n","    for i in range(orientations):\n","        # create new integral image for this orientation\n","        # isolate orientations in this range\n","        temp_ori = np.where(grad_ori < 180 / orientations * (i + 1),\n","                            grad_ori, 0)\n","        temp_ori = np.where(grad_ori >= 180 / orientations * i,\n","                            temp_ori, 0)\n","        # select magnitudes for those orientations\n","        cond2 = temp_ori > 0\n","        temp_mag = np.where(cond2, grad_mag, 0)\n","        orientation_histogram[:, :, i] = uniform_filter(temp_mag, size=(cx, cy))[int(cx / 2)::cx, int(cy / 2)::cy].T\n","\n","    return orientation_histogram.ravel()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dHz0cZ85EXcy","colab":{}},"source":["def color_histogram_hsv(im, nbin=10, xmin=0, xmax=255, normalized=True):\n","    ndim = im.ndim\n","    bins = np.linspace(xmin, xmax, nbin + 1)\n","    hsv = matplotlib.colors.rgb_to_hsv(im / xmax) * xmax\n","    imhist, bin_edges = np.histogram(hsv[:, :, 0], bins=bins, density=normalized)\n","    imhist = imhist * np.diff(bin_edges)\n","\n","    # return histogram\n","    return imhist"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kUMpJIDQ92uQ"},"source":["## 2 - Reload the CIFAR-10 dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OnrJGipX92uQ","colab":{}},"source":["try:\n","    del X_train, y_train\n","    del X_test, y_test\n","    print('Clear previously loaded data.')\n","except:\n","    pass\n","\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","mask = list(range(49000, 50000))\n","X_val = X_train[mask]\n","y_val = y_train[mask]\n","mask = list(range(49000))\n","X_train = X_train[mask]\n","y_train = y_train[mask]\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'forse', 'ship', 'truck']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wlO2BiZ792uR"},"source":["## 3 - Extract Features"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5FNOS5pg92uR","colab":{}},"source":["num_color_bins = 20 # Number of bins in the color histogram\n","feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n","\n","X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n","X_val_feats = extract_features(X_val, feature_fns)\n","X_test_feats = extract_features(X_test, feature_fns)\n","\n","# Preprocessing: Subtract the mean feature\n","mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n","X_train_feats -= mean_feat\n","X_val_feats -= mean_feat\n","X_test_feats -= mean_feat\n","\n","# Preprocessing: Divide by standard deviation. This ensures that each feature\n","# has roughly the same scale.\n","std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n","X_train_feats /= std_feat\n","X_val_feats /= std_feat\n","X_test_feats /= std_feat\n","\n","# Preprocessing: Add a bias dimension\n","X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n","X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n","X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7rBfgwQ992uT","colab":{}},"source":["y_train = y_train.ravel()\n","y_val = y_val.ravel()\n","y_test = y_test.ravel()\n","\n","print('X_train_feats.shape =', X_train_feats.shape)\n","print('y_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NwUjiS2A92uV"},"source":["## 4 - Train a Two-Layer Neural Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aQ6n1u5C92uV"},"source":["Again, fine tune the network, and find the best hyperparameter (learning rate, regularizations, bins, hidden neuron, etc)\n","\n","Then train the network once again using feature space CIFAR10 dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bEt5bFZm92uV","scrolled":false,"colab":{}},"source":["W, b, H = train_two_layer_relu(X_train_feats, y_train, X_val_feats, y_val,\n","                               hidden_size=100, epochs=10,\n","                               lr = 0.9, reg = 0.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mwRICvC692uX"},"source":["This approach should outperform all previous approaches: you should easily be able to achieve over 55% classification accuracy on the test set; \n","\n","our best model achieves **about 60% classification accuracy**."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LnVQn2aS92uX","colab":{}},"source":["y_pred = predict_two_layer_relu(X_val_feats, W, b)\n","\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","\n","print('Validation Accuracy =', accuracy*100,'%')\n","print('Test label          =',y_val[:15])\n","print('Predicted label     =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R9aMIIpv92uY","scrolled":true,"colab":{}},"source":["y_pred = predict_two_layer_relu(X_test_feats, W, b)\n","\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label       =',y_test[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"axGOM0K9RevR"},"source":["---\n","## 5 - Missclassified Images\n","An important way to gain intuition about how an algorithm works is to visualize the mistakes that it makes. \n"," \n"," In this visualization, we show examples of images that are misclassified by our current system. \n"," \n"," The first column  shows images that our system labeled as \"plane\" but whose true label is  something other than \"plane\"."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SsrzrAtt92uZ","scrolled":true,"colab":{}},"source":["plt.rcParams['figure.figsize'] = [10, 10]\n","\n","print('\\n\\n    missclassified images\\n')\n","examples_per_class = 8\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for cls, cls_name in enumerate(classes):\n","    idxs = np.where((y_test != cls) & (y_pred == cls))[0]\n","    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n","    for i, idx in enumerate(idxs):\n","        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n","        plt.imshow(X_test_ori[idx].astype('uint8'))\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls_name)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 2\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bvBk-u7_92ub"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}