{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNN2019 - 03 - Deep Neural Network.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"hPwFEN34mNUP","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sjlcHwermNUS","colab_type":"text"},"source":["\n","# Task 3 - Deep Neural Network\n","\n","\n","In this assignment you will practice putting together a simple image classification pipeline, based on the Multi-layer Neural Network classifier. \n","\n","The goals of this assignment are as follows:\n","\n","    * understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)\n","    * implement various modern activation functions\n","    * implement more compact and efficient Deep Neural Network API\n","    * investigate the power of deep network\n","    * implement and apply Batch Normalization\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pMlC1Kwa9Dl0"},"source":["---\n","# [Part 0] Import libraries"]},{"cell_type":"code","metadata":{"id":"LUazLJ-dmNUW","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.set_printoptions(precision=7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cWDWPVtzmNUd","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"JUJ1GFimmNUe","colab_type":"code","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXX0qY6-mNUj","colab_type":"text"},"source":["---\n","# [Part 1] More Compact API\n","In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular.\n","\n","<p align=\"center\"><img src=\"https://stanford.edu/~shervine/images/neural-network.png\"></p>\n","\n","In this exercise we will implement a **forward** and a **backward** function for fully-connected networks using a **more compact modular** approach. \n","\n","The **forward** function will receive inputs, weights, and other parameters and will return both an output and a **cache** object storing data needed for the backward pass, like this:\n","\n","```python\n","def layer_forward(x, w):\n","  \"\"\" Receive inputs x and weights w \"\"\"\n","  # Do some computations ...\n","  z = # ... some intermediate value\n","  # Do some more computations ...\n","  out = # the output\n","   \n","  cache = (x, w, z, out) # Values we need to compute gradients\n","   \n","  return out, cache\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"iygf2DBIEI8y","colab_type":"text"},"source":["\n","---\n","The **backward** pass will receive upstream derivatives and the **cache** object, and will return **gradients** with respect to the inputs and weights, like this:\n","\n","```python\n","def layer_backward(dout, cache):\n","  \"\"\"\n","  Receive dout (derivative of loss with respect to outputs) and cache,\n","  and compute derivative with respect to inputs.\n","  \"\"\"\n","  # Unpack cache values\n","  x, w, z, out = cache\n","  \n","  # Use values in cache to compute derivatives\n","  dx = # Derivative of loss with respect to x\n","  dw = # Derivative of loss with respect to w\n","  \n","  return dx, dw\n","```\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cqPr5kaVuEd6"},"source":["---\n","## 1 - Basic Layer Functions\n","\n","\n","After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n","\n","For this part, we've laready provide you the implementation of some basic layers"]},{"cell_type":"markdown","metadata":{"id":"oLrwVZt2mNUk","colab_type":"text"},"source":["---\n","### a. Affine API\n"]},{"cell_type":"code","metadata":{"id":"BnVusViAmNUl","colab_type":"code","colab":{}},"source":["def affine_forward(x, W, b ):   \n","    \"\"\"\n","    Computes the forward pass for an affine (fully-connected) layer.\n","\n","    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n","    examples, where each example x[i] has shape (d_1, ..., d_k). \n","\n","    Inputs:\n","    - x    : A numpy array containing input data, of shape (N, d_1, ..., d_k)\n","    - W    : A numpy array of weights, of shape (D, M)\n","    - b    : A numpy array of biases, of shape (M,)\n","    \n","    Returns a tuple of:\n","    - v    : output, of shape (N, M)\n","    - cache: (x, w, b)\n","    \"\"\"\n","\n","    v = np.dot(x, W) + b    \n","    cache = (x, W, b)\n","    \n","    return v, cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"spWWmF1HmNUq","colab_type":"code","colab":{}},"source":["def affine_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for an affine layer.\n","\n","    Inputs:\n","    - dout : Upstream derivative, of shape (N, M)\n","    - cache: Tuple of:\n","      - x  : Input data, of shape (N, d_1, ... d_k)\n","      - W  : Weights, of shape (D, M)\n","      - b  : Biases, of shape (M,)\n","\n","    Returns a tuple of:\n","    - dx   : Gradient with respect to x, of shape (N, d1, ..., d_k)\n","    - dw   : Gradient with respect to w, of shape (D, M)\n","    - db   : Gradient with respect to b, of shape (M,)\n","    \"\"\"\n","    \n","    x, W, b = cache\n","    dW = np.dot(x.T,dout)\n","    db = np.sum(dout, axis=0, keepdims=True)\n","    dx = dout.dot(W.T)\n","    \n","    return dW, db, dx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0-3RefQmNUv","colab_type":"text"},"source":["### b. Tanh API\n","\n","<p align=\"center\"><img src=\"https://stanford.edu/~shervine/images/tanh.png\" width=\"200\"></p>"]},{"cell_type":"code","metadata":{"id":"oik7PQ_3mNUx","colab_type":"code","colab":{}},"source":["def tanh_forward(x):     \n","    \"\"\"\n","    Computes the forward pass for a layer of Hyperbolic tangent (tanh).\n","\n","    Input:\n","    - x    : Inputs, of any shape\n","\n","    Returns a tuple of:\n","    - out  : Output, of the same shape as x\n","    - cache: 1-tanh(x)^2\n","    \"\"\"\n","  \n","    out = np.tanh(x)\n","    cache = 1-out**2\n","    \n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2koHn3lmNU3","colab_type":"code","colab":{}},"source":["def tanh_backward(dout, cache):  \n","    \"\"\"\n","    Computes the backward pass for a layer of Hyperbolic tangent (tanh).\n","\n","    Input:\n","    - dout : Upstream derivatives, of any shape\n","    - cache: Input x, of same shape as dout\n","\n","    Returns:\n","    - dx   : Gradient with respect to x\n","    \"\"\" \n","  \n","    dx = dout*cache\n","    \n","    return dx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NJrRDsp8nIRH","colab_type":"text"},"source":["### c. ReLU API\n","<p align=\"center\"><img src=\"https://stanford.edu/~shervine/images/relu.png\" width=\"200\"></p>"]},{"cell_type":"code","metadata":{"id":"5zXo1d5YnIRK","colab_type":"code","colab":{}},"source":["def relu_forward(x):\n","    \"\"\"\n","    Computes the forward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - x    : Inputs, of any shape\n","\n","    Returns a tuple of:\n","    - out  : Output, of the same shape as x\n","    - cache: x\n","    \"\"\"\n","  \n","    out = x * (x > 0).astype(float)\n","    cache = x\n","    \n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYKK3-y2rVla","colab_type":"code","colab":{}},"source":["def relu_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - dout : Upstream derivatives, of any shape\n","    - cache: Input x, of same shape as dout\n","\n","    Returns:\n","    - dx   : Gradient with respect to x\n","    \"\"\"\n","  \n","    dx = dout * (cache >= 0)\n","    \n","    return dx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-k8-FNOmNU7","colab_type":"text"},"source":["### d. Softmax API"]},{"cell_type":"code","metadata":{"id":"x_9sC3jFmNU9","colab_type":"code","colab":{}},"source":["def softmax(x):  \n","    \"\"\"\n","    Computes the categorical score for softmax classification.\n","\n","    Inputs:\n","    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n","      class for the ith input.\n","\n","    Returns a tuple of:\n","    - score: Normalized log probability score of softmax\n","    \"\"\"\n","  \n","    x -= np.max(x)\n","    x_exp = np.exp(x)\n","    x_sum = np.sum(x_exp, axis = 1, keepdims = True)  \n","    score = x_exp / x_sum\n","    \n","    return score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWes0OWamNVD","colab_type":"code","colab":{}},"source":["def softmax_loss(score, y):\n","    \"\"\"\n","    Computes the loss and gradient for softmax classification.\n","\n","    Inputs:\n","    - score: Input data, of shape (N, C) where x[i, j] is the softmax score \n","      for the jth class for the ith input.\n","    - y    : Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n","      0 <= y[i] < C\n","\n","    Returns a tuple of:\n","    - loss   : Scalar giving the loss\n","    - dscores: Gradient of the loss with respect to x\n","    \"\"\"\n","   \n","    num_examples = score.shape[0]\n","    number_list = range(num_examples)\n","    corect_logprobs = -np.log(score[number_list,y])\n","    loss = np.sum(corect_logprobs)/num_examples\n","    \n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    \n","    return loss, dscores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"U3M4090292sg"},"source":["---\n","## 2 - Advanced Activation Functions\n","\n","For this part, you need to implement several advanced activation functions that became popular recently"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SovOoekP92sw"},"source":["---\n","### a. Parametric ReLU (Leaky ReLU)\n","\n","Leaky ReLUs allow a small non-zero gradient to propagate through the network when the unit is not active hence avoiding bottlenecks that can prevent learning in the Neural Network\n","\n","<p align=\"center\"><img src=\"https://stanford.edu/~shervine/images/leaky-relu.png\" width=\"200\"></p>\n","\n","Implement the forward and backward function of parametric Rectified Linear Unit (Leaky ReLU) activation function\n"]},{"cell_type":"markdown","metadata":{"id":"2Iz5fs9J1Ng6","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement Parametric ReLU forward function**\n","\n","\n","$$\n","\\begin{align}\n","f(x, \\alpha) = \n","\\begin{cases}\n","\\alpha x, & \\text{for } x<0\\\\\n","x, & \\text{for } x\\geq0\n","\\end{cases}\n","\\end{align}\n","$$\n","\n","<br>\n","\n","*<i>hint: you can use <b>if-then</b> or <b>np.where(condition, if true, if false)</b></i>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YPdGZzEP92sw","colab":{}},"source":["def prelu_forward(x, alpha):  \n","    \"\"\"\n","    Computes the forward pass for a layer of parametric rectified linear units (PReLUs).\n","\n","    Input:\n","    - x    : Inputs, of any shape\n","    - alpha: leaky parameter\n","\n","    Returns a tuple of:\n","    - out  : Output, of the same shape as x\n","    - cache: x\n","    \"\"\"\n","    \n","    # calculate prelu forward \n","    out = ??\n","\n","    cache = (x, alpha)\n","    \n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RrkvR9h4Il8e","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_QAzZPL-92sz","colab":{}},"source":["x     = np.array([-2, -1, 0, 1, 2, 3])\n","dp, _ = prelu_forward(x, alpha = 0.01) \n","\n","print(dp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FCsWPV1792s0"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [-0.02 -0.01  0.    1.    2.    3.  ]\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DN54FbAv92s1"},"source":["---\n","#### <font color='red'>**EXERCISE**: </font>\n","**Implement PReLU backward function**\n","$$\n","\\begin{align*}\n","f'(x, \\alpha) = \n","\\begin{cases}\n","\\alpha, & \\text{for } x<0\\\\\n","1, & \\text{for } x\\geq0\n","\\end{cases}\\\\\n","\\\\\n","\\partial out = \\partial out * f'(x)\n","\\end{align*}\n","$$\n","\n","*<i>hint: you can use <b>if-then</b> or <b>np.where(condition, if true, if false)</b></i>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6KLXiutj92s1","colab":{}},"source":["def prelu_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of parametric rectified linear units (PReLUs).\n","\n","    Input:\n","    - dout : Upstream derivatives, of any shape\n","    - cache: Input x, of same shape as dout\n","\n","    Returns:\n","    - dout : Gradient with respect to x\n","    \"\"\"\n","    \n","    \n","    x, alpha = cache\n","    \n","    # calculate local gradient of prelu \n","    dp_prime = ??\n","    \n","    # calculate the gradient propagation\n","    dout = ??\n","    \n","    \n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BdbN3yQKIq6V"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KHn_dvZq92s4","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","dp, cache = prelu_forward(x, alpha = 0.01) \n","\n","np.random.seed(10)\n","dout = np.random.random((6,)) \n","\n","dout = prelu_backward(dout, cache)\n","\n","\n","np.set_printoptions(precision=5)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aDAhyVY_92s7"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [7.71321e-03 2.07519e-04 6.33648e-03 7.48804e-01 4.98507e-01 2.24797e-01]\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uOEsPa5Y92s8"},"source":["---\n","### b. ELU Function\n","\n","ELUs are exponential functions which have negative values that allow them to push mean unit activations closer to zero like batch normalization but with lower computational complexity.\n","\n","<p align=\"center\"><img src=\"https://stanford.edu/~shervine/images/elu.png\" width=\"200\"></p>\n","\n","\n","Implement the forward and backward function of the new Exponential Linear Unit (ELU) activation function"]},{"cell_type":"markdown","metadata":{"id":"Tv-s0-hj4RMN","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement ELU forward function**\n","\n","$$\n","\\begin{align}\n","f(x, \\alpha) = \n","\\begin{cases}\n","\\alpha (e^x-1), & \\text{for } x<0\\\\\n","x, & \\text{for } x\\geq0\n","\\end{cases}\n","\\end{align}\n","$$\n","\n","*<i>hint: you can use <b>if-then</b> or <b>np.where(condition, if true, if false)</b></i>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dxoUn19E92s_","colab":{}},"source":["def elu_forward(x, alpha):  \n","    \"\"\"\n","    Computes the forward pass for a layer of Exponential Linear Unit (ELU).\n","\n","    Input:\n","    - x    : Inputs, of any shape\n","    - alpha: leaky parameter\n","\n","    Returns a tuple of:\n","    - out  : Output, of the same shape as x\n","    - cache: x, alpha\n","    \"\"\"\n","    \n","    \n","    # calculate elu forward \n","    out = ??\n","\n","    cache = (x, alpha)\n","    \n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iq5x121vIrTX"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iQEAKYcQ92tB","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","de, _ = elu_forward(x, alpha = 0.01) \n","\n","np.set_printoptions(precision=7)\n","print(de)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-EtJ07v-92tD"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [-0.8646647 -0.6321206  0.         1.         2.         3.       ]\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-8RnowdB92tD"},"source":["---\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","**Implement `ELU` backward function**\n","$$\n","\\begin{align*}\n","f'(x, \\alpha) = \n","\\begin{cases}\n","f(x,\\alpha)+\\alpha, & \\text{for } x<0\\\\\n","1, & \\text{for } x\\geq0\n","\\end{cases}\\\\\n","\\\\\n","\\partial out = \\partial out * f'(x)\n","\\end{align*}\n","$$\n","\n","<br>\n","\n","\n","---\n","\n","**Note that for $x<0$**\n","\n","$$\n","\\begin{align*}\n","f(x,\\alpha)+\\alpha, \\ \\ & = \\ \\alpha (e^x-1)+\\alpha\\\\\n"," & = \\ \\alpha.e^x-\\alpha+\\alpha\\\\\n"," & = \\ \\alpha.e^x\n","\\end{align*}\n","$$\n","\n","\n","*<i>hint: you can use <b>if-then</b> or <b>np.where(condition, if true, if false)</b></i>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RJUGzh5u92tE","colab":{}},"source":["def elu_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of Exponential Linear Unit (ELU).\n","\n","    Input:\n","    - dout : Upstream derivatives, of any shape\n","    - cache: \n","      - Input x, of same shape as dout\n","      - alpha, leaky parameter\n","\n","    Returns:\n","    - dout : Gradient with respect to x\n","    \"\"\"\n","    \n","    \n","    x, alpha = cache\n","    \n","    # calculate local gradient of elu \n","    de_prime = ??\n","    \n","    # calculate the gradient propagation\n","    dout = ??\n","    \n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KK2N8SlvIr4A"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l6Jmcj9992tG","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","dp, cache = elu_forward(x, alpha = 1.0) \n","\n","np.random.seed(10)\n","dout = np.random.random((6,)) \n","\n","dout = elu_backward(dout, cache)\n","\n","np.set_printoptions(precision=7)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MfSRcG9d92tI"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [0.1043869 0.0076342 0.6336482 0.7488039 0.498507  0.2247966]\n","</pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fl4S98zy_DsC"},"source":["---\n","### c. SELU Function\n","\n","**SELU**s are activations which induce self-normalizing properties and are used in **Self-Normalizing Neural Networks** (SNNs). SNNs enable high-level abstract representations that tend to automatically converge towards zero mean and unit variance.\n","\n","Implement the forward and backward function of the new **Scaled Exponential Linear Unit** (SELU) activation function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5LzJafPQ_DsJ"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement SELU forward function**\n","\n","$$\n","\\begin{align}\n","f(x, \\alpha) = \n","\\lambda*\\begin{cases}\n","\\alpha (e^x-1), & \\text{for } x<0\\\\\n","x, & \\text{for } x\\geq0\n","\\end{cases}\n","\\end{align}\n","$$\n","\n","*<i>hint: you can use <b>if-then</b> or <b>np.where(condition, if true, if false)</b></i>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4E88-Uu5_DsN","colab":{}},"source":["def selu_forward(x):  \n","    \"\"\"\n","    Computes the forward pass for a layer of Scaled Exponential Linear Unit (SELU).\n","\n","    Input:\n","    - x    : Inputs, of any shape\n","\n","    Returns a tuple of:\n","    - out  : Output, of the same shape as x\n","    - cache: x, alpha, scale\n","    \"\"\"\n","    \n","    alpha = 1.6732632423543772848170429916717\n","    scale = 1.0507009873554804934193349852946\n","    \n","    # calculate elu forward \n","    out = ??\n","\n","    cache = (x, alpha, scale)\n","    \n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OO5K2Wg0Is2o"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ucrL0LJi_DsU","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","de, _ = selu_forward(x) \n","\n","np.set_printoptions(precision=7)\n","print(de)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vw8y0R0l_Dsa"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [-1.5201665 -1.1113307  0.         1.050701   2.101402   3.152103 ]\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sOOczZUi_Dsc"},"source":["---\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","**Implement `SELU` backward function**\n","$$\n","\\begin{align*}\n","f'(x, \\alpha) = \n","\\lambda*\\begin{cases}\n","f(x,\\alpha)+\\alpha, & \\text{for } x<0\\\\\n","1, & \\text{for } x\\geq0\n","\\end{cases}\\\\\n","\\\\\n","\\partial out = \\partial out * f'(x)\n","\\end{align*}\n","$$\n","\n","<br>\n","\n","\n","---\n","\n","**Note that for $x<0$**\n","\n","$$\n","\\begin{align*}\n","f(x,\\alpha)+\\alpha, \\ \\ & = \\ \\alpha (e^x-1)+\\alpha\\\\\n"," & = \\ \\alpha.e^x-\\alpha+\\alpha\\\\\n"," & = \\ \\alpha.e^x\n","\\end{align*}\n","$$\n","\n","\n","*<i>hint: you can use <b>if-then</b> or <b>np.where(condition, if true, if false)</b></i>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3bklZk_n_Dse","colab":{}},"source":["def selu_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of Scaled Exponential Linear Unit (SELU).\n","\n","    Input:\n","    - dout : Upstream derivatives, of any shape\n","    - cache: \n","      - Input x, of same shape as dout\n","      - alpha, leaky parameter\n","      - scale parameter\n","\n","    Returns:\n","    - dout : Gradient with respect to x\n","    \"\"\"\n","    \n","    x, alpha, scale = cache\n","    \n","    # calculate local gradient of elu \n","    de_prime = ??\n","    \n","    # calculate the gradient propagation\n","    dout = ??\n","    \n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-1zLzY2OItSA"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ErbKGXvb_Dsi","colab":{}},"source":["x = np.array([-2, -1, 0, 1, 2, 3])\n","dp, cache = selu_forward(x) \n","\n","np.random.seed(10)\n","dout = np.random.random((6,)) \n","\n","dout = selu_backward(dout, cache)\n","\n","np.set_printoptions(precision=7)\n","print(dout)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OkR_aB34_Dsn"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre> [0.1835225 0.0134217 1.1140165 0.786769  0.5237818 0.2361941]\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"VX4n9KNXmNVY","colab_type":"text"},"source":["---\n","\n","---\n","# [Part 2] Deep Neural Net API\n","\n","For this exercise, we'll create a Deep Neural Net API which will automatically create deep layers for a given list of hidden layer\n","\n","With this, unlike the exercise before, we can create however many layers in our architecture\n","\n","<br>\n","\n","Not only that, we'll add **activation** argument so we can change the activation function easily between **relu** and **tanh**"]},{"cell_type":"markdown","metadata":{"id":"iCloT9NVmNVI","colab_type":"text"},"source":["---\n","## 1 - Weight Init\n","Below is a function to repeatedly initialize weights and bias for each layer"]},{"cell_type":"code","metadata":{"id":"yQe8zlZfmNVK","colab_type":"code","colab":{}},"source":["def init_weights(d_in, hidden, d_out, std=1e-2, seed=None):\n","    \"\"\"\n","    Inputs:\n","    - d_in  : int, number of input dimension\n","    - hidden: list of number hidden neuron in each hiidden layer\n","    - d_out : int, number of output dimension\n","    - std   : standar deviation for generating weights\n","    - seed  : random seed\n","    \n","    Outputs:\n","    - W: list of Weights\n","    - b: list of biases\n","    \"\"\"\n","    \n","    W = []\n","    b = []\n","    np.random.seed(seed)\n","    dims = [d_in] + hidden + [d_out] \n","    \n","    for i in range(len(dims)-1):\n","        W.append(std * np.random.randn(dims[i],dims[i+1]))\n","        b.append(np.zeros((1, dims[i+1])))\n","    return W, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8wk4SoVTmNVQ","colab_type":"text"},"source":["Let's try the `init_weights` function by creating weights and bias for a 4-layered Neural Network (3 hidden layer). With input of 5, otput 10 classes, and hidden neuron in each hidden layer are 10, 20, and 30"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"aurNSMrfmNVR","colab_type":"code","colab":{}},"source":["d_in  = 5\n","hidden=[10, 20, 30]\n","d_out =10\n","\n","W, b = init_weights(d_in, hidden, d_out)\n","print('Number of Layer =',len(W))\n","for i in range(len(W)):\n","    print('Shape of each layer-',i,'weight:',W[i].shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n20LInIKmNVX","colab_type":"text"},"source":["**Expected Output**: \n","<pre>\n","Number of Layer = 4\n","Shape of each layer- 0 weight: (5, 10)\n","Shape of each layer- 1 weight: (10, 20)\n","Shape of each layer- 2 weight: (20, 30)\n","Shape of each layer- 3 weight: (30, 10)"]},{"cell_type":"markdown","metadata":{"id":"JTzrXg8nmNVZ","colab_type":"text"},"source":["---\n","## 2 - Predict Function\n","\n","Implement the predict function first, because we are going to use **predict** function inside the **training** function to track the **validation** accuracy \n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input - <font color=\"blue\">N * [FC Layer - activation]</font> - FC Layer - argmax</b></pre>\n","\n","<br>\n","\n","The **N** is the number of hidden layer, which can be calculated from **len(W)-1**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XSugv9vVmNVb","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement Predict Function**\n","\n","    * loop call forward function for each hidden layer weights\n","    * check and use the requested activation funtion\n","    * call forward function for the last layer"]},{"cell_type":"code","metadata":{"id":"p3_ufLTHmNVd","colab_type":"code","colab":{}},"source":["def predict_multi_layer(X, W, b, act_f ='tanh'):    \n","    \"\"\"\n","    Inputs:\n","    - X    : Input data, of shape(N, D)\n","    - W    : list of Weight\n","    - b    : list of biases\n","    - act_f: activation function ('tanh' or 'relu')\n","    \n","    Output:\n","    - y_pred : list of class prediction\n","    \"\"\"\n","    \n","    \n","    y_pred = np.zeros(X.shape[1])\n","    n_layer = len(W)\n","    \n","    \n","    \n","    # first activation is X\n","    act = X\n","    \n","    ## ------------------------- start your code here -------------------------\n","    \n","    # loop i over n_layer-1\n","    for i in range(n_layer-1):\n","    \n","        # calculate layer score by calling affine forward function using act, W[i], and b[i]\n","        layer, _ = ??\n","  \n","        if ( act_f == 'tanh'):\n","            # calculate activation score by calling tanh forward function using layer score\n","            act, _ = ??\n","          \n","        else:\n","            # calculate activation score by calling relu forward function using layer score\n","            act, _ = ??\n","          \n","\n","    # calculate last layer score by calling affine forward function using act, W[-1], and b[-1]\n","    last_layer, _ = ??\n","    \n","    \n","    # take the maximum prediction from the last layer and use that column to get the class       \n","    # use np.argmax with axis=-1 \n","    y_pred = ??\n","\n","    ## ------------------------- end your code here -------------------------\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNthPdhwmNVi","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"cYkXz_agq4LX","colab_type":"code","colab":{}},"source":["np.random.seed(30)\n","\n","X = np.random.rand(10,20)\n","d_in =20\n","hidden=[15, 25, 35]\n","d_out=10\n","\n","W, b = init_weights(d_in, hidden, d_out, seed=30)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GiA0X7o6q7oG","colab_type":"text"},"source":["Check using **tanh** activation"]},{"cell_type":"code","metadata":{"id":"swAHAe9ymNVj","colab_type":"code","colab":{}},"source":["y = predict_multi_layer(X, W, b, act_f = 'tanh')\n","\n","print('y.shape =', y.shape)\n","print('y =',y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LS-kMWHJmNVn","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n"," y.shape = (10,)\n"," y = [4 7 9 4 4 4 7 4 7 4]\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O7YbAVM3rGgu"},"source":["Check using **relu** activation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G52aju0prGgx","colab":{}},"source":["y = predict_multi_layer(X, W, b, act_f='relu')\n","\n","print('y.shape =', y.shape)\n","print('y =',y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hOndXjB-rGg3"},"source":["**Expected Output**:\n","<pre>\n"," y.shape = (10,)\n"," y = [1 6 0 5 4 5 1 4 0 4]\n"]},{"cell_type":"markdown","metadata":{"id":"ZLv7H7WXmNVo","colab_type":"text"},"source":["---\n","## 3 - Training Function\n","\n","Now let's complete the training function\n","\n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input - <font color=\"blue\">N * [FC Layer - activation]</font> - FC Layer - Softmax</b></pre>\n","\n","<br>\n","\n","The **N** is the number of hidden layer, which can be calculated from **len(W)-1**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"56ExupMlmNVp","colab_type":"text"},"source":["<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","**Implement Training Function**\n","\n","there are **four steps** in this training function\n","\n","---\n","\n","**1. Forward Pass**\n","\n","    * loop over hidden layer [for len(W)-1]\n","        * call affine forward function\n","        * call activation forward function\n","    * call affine forward function for the last layer\n","    * call softmax score function\n","\n","**2. Calculate Loss**\n","\n","    * call softmax_loss function\n","    * loop over weights [for W]\n","        * calculate loss with regularization\n","\n","\n","**3. Backward Pass**\n","\n","    * call affine backward function for the last layer\n","    * loop over hidden layer [from len(W)-2 to 0]\n","        * call activation backward function\n","        * call affine backward function\n","\n","**4. Weight Update**\n","\n","    * loop over weights [for W]\n","        * implement weight update\n","    * calculate the training and validation accuracy"]},{"cell_type":"code","metadata":{"id":"XokPEJ3LmNVr","colab_type":"code","colab":{}},"source":["def train_multi_layer(X, y, X_val, y_val, hidden_size, act_f='tanh',\n","                      W=None, b=None, std=1e-4, seed=None,\n","                      lr=1e-4, lr_decay=0.95, reg=0.25, \n","                      epochs=100, batch_size=200, verbose=True):\n","    \"\"\"\n","    Inputs:\n","    - X          : array of train data, of shape (N, D)\n","    - y          : array of train labels, of shape (N,)\n","    - X_val      : array of validation data, of shape (Nv, D)\n","    - y_val      : array of validation labels, of shape (Nv,)\n","    - hidden_size: list of hidden neuron for each hidden layer\n","    - act_f      : activation function ('tanh' or 'relu')\n","    - W          : list of Weight, if W is None, it will be initialized\n","    - b          : list of biases, if W is None, bias will be initialized\n","    - std        : float, standar deviation for generating weights\n","    - seed       : int, initial random seed\n","    - lr         : float, initial learning rate\n","    - lr_decay   : float, 0-1, decay rate to reduce learning rate each epoch\n","    - reg        : float, regularization rate\n","    - epochs     : int, number of training epoch\n","    - batch_size : int, number of batch used each step\n","    - verbose    : boolean, verbosity\n","    \n","    Outputs:\n","    - W          : list of trained Weights\n","    - b          : list of trained biases\n","    - history    : list of training history [loss, train_acc, val_acc]\n","    \n","    \"\"\"\n","    \n","    num_train, dim = X.shape\n","    \n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","    \n","    # assume y takes values 0...K-1 where K is number of classes\n","    num_classes = np.max(y) + 1  \n","        \n","    # initialize Weights\n","    if W is None:\n","        W, b = init_weights(dim, hidden_size, num_classes, std, seed) \n","        \n","    # number of layer (including output layer)\n","    n_layer = len(W)\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","    \n","    \n","    ## ------------------------- start your code here --------------------------\n","\n","    print('start training using', act_f, 'activation function')\n","    for ep in range(epochs):\n","        # Shuffle data train index\n","        train_rows = np.arange(num_train)\n","        np.random.shuffle(train_rows)\n","        \n","        # split index into mini batches\n","        id_batch = np.split(train_rows, num_iter)\n","  \n","        for batch in id_batch:\n","      \n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","\n","            # store all cache in dictionary\n","            cache = {}\n","\n","            # first layer activation input is X_batch\n","            act = X_batch\n","\n","            # ------------------------------------------------------------------\n","            # 1. Forward Pass\n","            # ------------------------------------------------------------------\n","\n","            # loop i over hidden layer (n_layer-1)\n","            # see predict function implementation\n","            for i in ??:\n","\n","                # calculate layer score by calling affine forward function using activation act, W[i], and b[i]\n","                layer, cache_affine = ??\n","\n","\n","                if ( act_f == 'tanh'):\n","                    # calculate activation score by calling tanh forward function using layer score\n","                    act, _ = ??\n","                    \n","                else:\n","                    # calculate activation score by calling relu forward function using layer score\n","                    act, _ = ??\n","\n","                    \n","                # combine cache from affine and activation layer into cache for this layer\n","                cache[i] = (cache_affine, cache_act)\n","\n","            # calculate last layer score by calling affine forward function using activation act, W[i+1], and b[i+1]\n","            last_layer, cache[i+1] = ??\n","\n","            # calculate softmax score by calling softmax function using last_layer output score\n","            softmax_score = ??\n","\n","            # ------------------------------------------------------------------\n","            # 2. Calculate Loss\n","            # ------------------------------------------------------------------\n","\n","            # evaluate loss and gradient by calling softmax_loss function using input softmax_score and y_batch\n","            loss, dout = ??\n","\n","            # add regularization to the loss:\n","            #    for each weights, calculate the sum square, multiply regularization strength\n","            #    then add it to the loss\n","            # see the implementation in the previous Task\n","            for w in W:\n","                # loss = loss + reg * sum(w*w)\n","                loss += ??  \n","\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","\n","            # ------------------------------------------------------------------\n","            # 3. Backward Pass\n","            # ------------------------------------------------------------------    \n","\n","            # dictionary to contain all gradients\n","            dW = {}\n","            db = {}\n","\n","            # calculate last weights gradient by calling affine backward function using dout and cache[n_layer-1]\n","            dW[n_layer-1], db[n_layer-1], dact = ??\n","\n","            #loop i from n_layer-2 down to 0\n","            for i in range(n_layer-2,-1,-1):\n","\n","                # extract affine cache and activation cache from layer cache\n","                cache_affine, cache_act = cache[i]\n","\n","                if ( act_f == 'tanh'):\n","                    # calculate tanh gradient by calling tanh backward function using dact and cache_act\n","                    dlayer = ??\n","                    \n","                else:\n","                    # calculate relu gradient by calling relu backward function using dact and cache_act\n","                    dlayer = ??\n","\n","                    \n","                # calculate layer weights gradient by calling affine backward function using dlayer and cache_affine\n","                dW[i], db[i], dact = ??\n","\n","                # add regularization to gradient\n","                dW[i] += 2 * reg * W[i]\n","\n","            # ------------------------------------------------------------------\n","            # 4. Weight Update\n","            # ------------------------------------------------------------------    \n","\n","            # perform parameter update by subtracting W[i] and b[i] for each layer with a fraction of dW[i] and db[i]\n","            # according to the learning rate\n","            # loop over W\n","            for i in range(len(W)):    \n","                # w_i = w_i - lr * dw_i\n","                W[i] -= ??\n","    \n","                # b_i = b_i - lr * db_i\n","                b[i] -= ??\n","\n","\n","            # iteration count\n","            it +=1\n","\n","            if verbose and it % 100 == 0:\n","                print ('iteration',it,'(epoch', ep+1,'/',epochs, '): loss =', loss)\n","              \n","            \n","        # At the end of one epoch\n","        # 1. Check accuracy\n","        #    calculate the training accuracy by calling predict_multi_layer function on X_batch\n","        #    and compare it to y_batch. Then calculate the mean correct (accuracy in range 0-1)\n","        train_acc = (predict_multi_layer(X_batch, W, b, act_f) == y_batch).mean()\n","        train_acc_history.append(train_acc)\n","\n","        # 2. Calculate the training accuracy by calling predict_multi_layer function on X_val\n","        #    and compare it tu y_val. Then calculate the mean correct (accuracy in range 0-1)\n","        val_acc = (predict_multi_layer(X_val, W, b, act_f) == y_val).mean()\n","        val_acc_history.append(val_acc)\n","\n","        # 3. Decay learning rate\n","        #    multiply learning rate with decay\n","        #    see sigmoid train function\n","        lr *= lr_decay\n","            \n","            \n","    ## ------------------------- end your code here ----------------------------\n","    \n","    history = [loss_history, train_acc_history, val_acc_history]\n","    \n","    if verbose:\n","      print('Training done')\n","    \n","    return W, b, history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOXrvo7nmNVu","colab_type":"text"},"source":["---\n","***\n","# [Part 3] CIFAR-10 Dataset\n","\n","Again, we'll use the CIFAR-10 dataset\n","\n","for that, let's load and preprocess it first"]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv","colab_type":"text"},"source":["## 1 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz","colab_type":"text"},"source":["## 2 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","colab_type":"code","colab":{}},"source":["X_val_ori = X_train_ori[-1000:,:]\n","y_val     = y_train[-1000:]\n","\n","X_train_ori = X_train_ori[:-1000, :]\n","y_train     = y_train[:-1000]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4","colab_type":"text"},"source":["## 3 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","colab_type":"code","colab":{}},"source":["X_train = X_train_ori.astype('float32')\n","X_val = X_val_ori.astype('float32')\n","X_test = X_test_ori.astype('float32')\n","\n","mean_image = np.mean(X_train, axis = 0)\n","X_train -= mean_image\n","X_val -= mean_image\n","X_test -= mean_image\n","\n","X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]*X_train.shape[3]))\n","X_val = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]*X_val.shape[3]))\n","X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]*X_test.shape[3]))\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n","\n","y_train = y_train.ravel()\n","y_val = y_val.ravel()\n","y_test = y_test.ravel()\n","\n","print('\\ny_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvFT62Z2mNV8","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","X_train.shape = (49000, 3072)\n","X_val.shape   = (1000, 3072)\n","X_test.shape  = (10000, 3072)\n","\n","y_train.shape = (49000,)\n","y_val.shape   = (1000,)\n","y_test.shape  = (10000,)"]},{"cell_type":"markdown","metadata":{"id":"HihrFmP3mNV9","colab_type":"text"},"source":["---\n","***\n","# [Part 4] Shallow Neural Network\n","\n","This part, let's train a shallow network with only 1 hidden neuron. \n","\n","We'll try both **tanh** and **relu** activation function, then compare the results"]},{"cell_type":"markdown","metadata":{"id":"ZFls6cKUmNV-","colab_type":"text"},"source":["---\n","## 1 - Two Layer Tanh Network \n","\n","First one is two layer net using **tanh** activation function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LDNVR1Y3Il-7","colab_type":"text"},"source":["---\n","### a. Train Network"]},{"cell_type":"markdown","metadata":{"id":"50Pjp3ySIWJQ","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **one-hidden layer neural network** with **50 hidden neurons**, using **tanh** activation function\n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"PLYZtiiumNV_","colab_type":"code","colab":{}},"source":["np.random.seed(None)\n","W_tanh, b_tanh, history_tanh = train_multi_layer(\n","    X_train, y_train, X_val, y_val, \n","    hidden_size=[50], \n","    act_f = 'tanh',\n","    std=1e-4, lr=1e-4, \n","    lr_decay=0.95, reg=0.01, \n","    epochs=12)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIXSksJimNWF","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","loss should starts around 2.3 and ends around 2.1"]},{"cell_type":"markdown","metadata":{"id":"_kR8iupymNWG","colab_type":"text"},"source":["---\n","### b. Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"e4EcgAUVmNWH","colab_type":"code","colab":{}},"source":["loss, train_acc, val_acc = history_tanh\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"id":"LDsrivwamNWJ","colab_type":"text"},"source":["**Expected Results**:\n","\n","<pre>You should see that although the loss is gradually decreasing, the accuracy has not increased.\n","It might be caused by too high of learning rate, or the network needs to be trained longer, or it's just that the capacity of the two-layered network is not enough."]},{"cell_type":"markdown","metadata":{"id":"IQSVh0HOOgJT","colab_type":"text"},"source":["---\n","### c. Training Accuracy\n","Calculate the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"YfMbdyFmmNWK","colab_type":"code","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_multi_layer(X_train, W_tanh, b_tanh, act_f='tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Scnz-xdfmNWP","colab_type":"text"},"source":["**Expected Output**:\n","\n","<pre>You should be able to get about <b>~18%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"id":"y6MNlkuHmNWR","colab_type":"code","colab":{}},"source":["y_pred = predict_multi_layer(X_val, W_tanh, b_tanh, act_f = 'tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PwxpQeb3mNWT","colab_type":"text"},"source":["**Expected Output**:\n","\n","<pre>You should also be able to get about <b>~20%</b> accuracy on validation set</pre>\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vc2HtG17OJTX"},"source":["---\n","## 2 - Two Layer ReLU Network \n","\n","The next one is two layer net using relu activation function.\n","\n","Here you should see that the network using **relu** will converge much faster compared to using **tanh**\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eENFBxVMOJTe"},"source":["---\n","### a. Train Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O8KniRvAOJTj"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **one-hidden layer neural network** with **50 hidden neurons**, \n","\n","use the same parameter as before in tanh, but now using **relu** activation function\n"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"6_ormJOfOJTm","colab":{}},"source":["np.random.seed(None)\n","W_relu, b_relu, history_relu = train_multi_layer(\n","    X_train, y_train, X_val, y_val, \n","    hidden_size=[50], \n","    act_f = 'relu',\n","    std=1e-4, lr=1e-4, \n","    lr_decay=0.95, reg=0.01, \n","    epochs=12)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OEtJULj7OJTt"},"source":["**Expected Output**:\n","<pre>\n","loss should starts around 2.3 and ends around 1.7"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sK2JC43kOJTv"},"source":["---\n","### b. Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"h-AuJUb1OJTx","colab":{}},"source":["loss, train_acc, val_acc = history_relu\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"colab_type":"text","id":"jZ862A9UOJT1"},"source":["**Expected Results**:\n","\n","<pre>You should see that as the loss is decreasing, the training and validation accuracy keep increasing.\n","Using relu also shown that it prevents overfitting as the training and validation accuracy is really close.\n","It means that we can train it longer to achieve even better accuracy."]},{"cell_type":"markdown","metadata":{"id":"mOXxHS3eO2Si","colab_type":"text"},"source":["---\n","### c. Training Accuracy\n","Calculate the train-validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iFKv17p-OJT3","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_multi_layer(X_train, W_relu, b_relu, act_f='relu')\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dlA7UueLOJT7"},"source":["**Expected Output**:\n","\n","<pre>You should be able to get about <b>~39%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"c-oPut6JOJT8","colab":{}},"source":["y_pred = predict_multi_layer(X_val, W_relu, b_relu, act_f='relu')\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zs_mfBMQOJUC"},"source":["**Expected Output**:\n","\n","<pre>You should also be able to get about <b>~38%</b> accuracy on validation set</pre>\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"markdown","metadata":{"id":"NmAbJvfCPCmS","colab_type":"text"},"source":["---\n","## 3 - Comparison\n","You should already be able to see the comparison, but let's visualize it anyway"]},{"cell_type":"code","metadata":{"id":"Gq3END0gNyy6","colab_type":"code","colab":{}},"source":["loss_tanh, train_tanh, val_tanh = history_tanh\n","loss_relu, train_relu, val_relu = history_relu\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(train_tanh, label='tanh')\n","plt.plot(train_relu, label='relu')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Training Accuracy')\n","\n","plt.subplot(122)\n","plt.plot(val_tanh, label='tanh')\n","plt.plot(val_relu, label='relu')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Validation Accuracy')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"napBNC1tYulr","colab_type":"text"},"source":["**Expected Output**:\n","\n","You can see that the **relu** network can already learn even if it only uses one layer, while the **tanh** network got stuck and cannot improve"]},{"cell_type":"markdown","metadata":{"id":"k5t2TWycmNWU","colab_type":"text"},"source":["---\n","***\n","#[Part 5] Deep Neural Network\n","* In this part, we'll increase the network capacity by increasing the number of hidden layers\n"," \n","* Like before, we'll try both **tanh** and **relu** activation function, then compare the results\n"," \n","* And since we're using more layers, we'll train it a little bit longer"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iTYOR_oTUYMJ"},"source":["---\n","## 1 - Four Layer Tanh Network \n","\n","First one is four layer net using **tanh** activation function\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vc7LETzCUYMQ"},"source":["---\n","### a. Train Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xc2RyhhXUYMU"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **three-hidden layer neural network** with **50 hidden neurons** each using **tanh** activation function\n","\n"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"BBAJlDlAUYMY","colab":{}},"source":["hidden_size=[50, 50, 50]\n","\n","W_tanh_4, b_tanh_4, history_tanh_4 = train_multi_layer(\n","    X_train, y_train, X_val, y_val, \n","    hidden_size=hidden_size, \n","    act_f = 'tanh',\n","    std=1e-2, lr=1e-2,\n","    lr_decay=0.95, reg=0.01, \n","    epochs=20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7ZIL-ShMUYMg"},"source":["**Expected Output**:\n","<pre>\n","loss should starts around 2.45 and ends around 2.33"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NWrj-pbiUYMh"},"source":["---\n","### b. Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"9C6Su2WQUYMj","colab":{}},"source":["loss, train_acc, val_acc = history_tanh_4\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"colab_type":"text","id":"OkVfOvfcUYMp"},"source":["**Expected Results**:\n","\n","<pre>You should see now that the loss is gradually decreasing, and the accuracy is actually increasing compared to two layer tanh.\n","\n","And it's finally going down after ~4000 iterations\n","\n","But you can see that it's still not high enough. It means that the network need to be trained much longer."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"waoBKzJqUYMr"},"source":["---\n","### c. Training Accuracy\n","Calculate the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nSQ4AgzqUYMs","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_multi_layer(X_train, W_tanh_4, b_tanh_4, act_f='tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2CRdVyk5UYMw"},"source":["**Expected Output**:\n","\n","<pre>You should be able to get about <b>~17%</b> accuracy on training set using the initial run\n","\n","Notice that it's lower than one layer tanh"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XsmjABg9UYMz","colab":{}},"source":["y_pred = predict_multi_layer(X_val, W_tanh_4, b_tanh_4, act_f = 'tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YGKyv4msUYM4"},"source":["**Expected Output**:\n","\n","<pre>You should also be able to get about <b>~18%</b> accuracy on validation set</pre>\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"N_ntC3M8UYM5"},"source":["---\n","## 2 - Four Layer ReLU Network \n","\n","The next one is four layer net using relu activation function.\n","\n","You should see major improvement in accuracy when using more layer, aside from that it will also converge much faster compared to using tanh\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CaYkqoKkUYM6"},"source":["---\n","### a. Train Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HbHQDxmeUYM7"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **three-hidden layer neural network** with **50 hidden neurons** each\n","\n","use the same parameter as before in **tanh**, but now using **relu** activation function\n"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"IGlCm0ORUYM8","colab":{}},"source":["hidden_size=[50, 50, 50]\n","W_relu_4, b_relu_4, history_relu_4 = train_multi_layer(\n","    X_train, y_train, X_val, y_val, \n","    hidden_size=hidden_size, \n","    act_f = 'relu',\n","    std=1e-2, lr=1e-2,\n","    lr_decay=0.95, reg=0.01, \n","    epochs=20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XPRTFnUFUYNB"},"source":["**Expected Output**:\n","<pre>\n","loss should starts around 2.45 and ends around 1.39"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0JFHJFqnUYNC"},"source":["---\n","### b. Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"vWdJSQZaUYND","colab":{}},"source":["loss, train_acc, val_acc = history_relu_4\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"colab_type":"text","id":"GbgPK92jUYNJ"},"source":["**Expected Results**:\n","\n","<pre>You should see that as the loss is decreasing, the training and validation accuracy keep increasing.\n","Using relu also shown that it prevents overfitting as the training and validation accuracy is really close.\n","It means that we can train it longer to achieve even better accuracy."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bdhjHRycUYNK"},"source":["---\n","### c. Training Accuracy\n","Calculate the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rYv57xzuUYNL","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_multi_layer(X_train, W_relu_4, b_relu_4, act_f='relu')\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LWrNQPhjUYNQ"},"source":["**Expected Output**:\n","\n","<pre>You should be able to get about <b>~57%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A6qmS-40UYNR","colab":{}},"source":["y_pred = predict_multi_layer(X_val, W_relu_4, b_relu_4, act_f='relu')\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lNUsuUZIUYNU"},"source":["**Expected Output**:\n","\n","<pre>You should also be able to get about <b>~52%</b> accuracy on validation set</pre>\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K3X8gDPgUYNV"},"source":["---\n","## 3 - Comparison\n","You should already be able to see the comparison, but let's visualize it anyway"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ndfPo-ktUYNV","colab":{}},"source":["loss_tanh_4, train_tanh_4, val_tanh_4 = history_tanh_4\n","loss_relu_4, train_relu_4, val_relu_4 = history_relu_4\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(train_tanh_4, label='tanh')\n","plt.plot(train_relu_4, label='relu')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Training Accuracy')\n","\n","plt.subplot(122)\n","plt.plot(val_tanh_4, label='tanh')\n","plt.plot(val_relu_4, label='relu')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Validation Accuracy')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"54wEw2x2Zo9U"},"source":["**Expected Output**:\n","\n","You can see that the **relu** network converge faster compared to the **tanh** network "]},{"cell_type":"markdown","metadata":{"id":"WYR_Mk0xZsdz","colab_type":"text"},"source":["---\n","Now let's see if we compare all four networks"]},{"cell_type":"code","metadata":{"id":"DC4olBbFUNin","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [14, 8]\n","plt.subplots_adjust(wspace=0.25)\n","plt.subplots_adjust(hspace=0.3)\n","\n","plt.subplot(221)\n","plt.plot(train_tanh, label='tanh 1')\n","plt.plot(train_tanh_4, label='tanh 4')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc tanh')\n","plt.ylim(.05,.4)\n","plt.title('Training Accuracy')\n","\n","plt.subplot(222)\n","plt.plot(val_tanh, label='tanh 1')\n","plt.plot(val_tanh_4, label='tanh 4')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc tanh')\n","plt.ylim(.05,.4)\n","plt.title('Validation Accuracy')\n","\n","plt.subplot(223)\n","plt.plot(train_relu, label='relu 1')\n","plt.plot(train_relu_4, label='relu 4')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc relu')\n","\n","plt.subplot(224)\n","plt.plot(val_relu, label='relu 1')\n","plt.plot(val_relu_4, label='relu 4')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc relu')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lH84yhuuajFX","colab_type":"text"},"source":["**Expected Output**:\n","\n","You should see that in **relu** case, there's an improvement in accuracy when using more layer,"]},{"cell_type":"markdown","metadata":{"id":"CPUfCJKmmNWx","colab_type":"text"},"source":["---\n","***\n","# [Part 6] Batch Normalization\n","\n","* One way to make deep networks easier to train is to use more sophisticated and advanced optimization procedures such as SGD+momentum, RMSProp, or Adam. Now we will talk about them later in the class.\n","\n","* Another strategy is to change the architecture of the network to make it easier to train. \n","One idea along these lines is batch normalization which was proposed by [[1]](https://arxiv.org/abs/1502.03167) in 2015.\n","\n","<br>\n","\n","* The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. \n","\n","* However, even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.\n","\n","<br>\n","\n","* Now let's see the result of Deep Network without Batch Normalization\n","\n","[1] [Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n","Internal Covariate Shift\", ICML 2015.](https://arxiv.org/abs/1502.03167)"]},{"cell_type":"markdown","metadata":{"id":"LcX9WcZJmNWz","colab_type":"text"},"source":["---\n","## 1 - Weight Distribution without BatchNorm\n","\n","Let's build a `5-Layered Neural Network` with 500 hidden neuron each. Then we feed a random data. \n","\n","You'll see that the activation distribution for each layer will plummet as the activation in each layer are not zero-centered"]},{"cell_type":"code","metadata":{"id":"rS6hymhVmNW1","colab_type":"code","colab":{}},"source":["Hs = {}\n","X = np.random.randn(1000, 500)\n","\n","d_in =500\n","hidden=[500, 500, 500, 500]\n","d_out=500\n","W, b = init_weights(d_in, hidden, d_out)\n","\n","a = X\n","for i in range(len(W)):\n","    z, _ = affine_forward(a, W[i], b[i])\n","    a, _ = tanh_forward(z)\n","    Hs[i] = a"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdgNur1_PK7b","colab_type":"text"},"source":["Now let's see the activation distribution as we calculate the forward pass"]},{"cell_type":"code","metadata":{"id":"sSmsbmBlmNW3","colab_type":"code","colab":{}},"source":["print('input layer had meann %f and std %f' % (np.mean(X), np.std(X)))\n","layer_means = [np.mean(H) for i, H in Hs.items()]\n","layer_stds = [np.std(H) for i, H in Hs.items()]\n","for i, H in Hs.items():\n","    print ('hidden layer %d had mean %f and std %f' % (i + 1, layer_means[i], layer_stds[i]))\n","\n","print()\n","plt.figure(figsize=(14,3))\n","plt.subplot(121)\n","plt.plot(list(Hs.keys()), layer_means, 'ob-')\n","plt.title('layer mean')\n","plt.subplot(122)\n","plt.plot(list(Hs.keys()), layer_stds, 'or-')\n","plt.title('layer std')\n","plt.show()\n","\n","\n","print('\\n\\nActivation Distribution')\n","plt.figure(figsize=(14,3))\n","for i, H in Hs.items():\n","    plt.subplot(1, len(Hs), i + 1)\n","    plt.hist(H.ravel(), 30, range=(-1, 1))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZpejfXdmNW5","colab_type":"text"},"source":["---\n","##2 - BatchNorm Forward (Math)\n","\n","* The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, [1] proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. \n","* These estimated means and standard deviations are then used to center and normalize the features of the minibatch.\n","<br>\n","\n","* It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. \n","* To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.\n","\n","<br>\n","\n","You can see in [here](http://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html) to get the basic idea of how forward and backward pass in Batch Normalization is done"]},{"cell_type":"markdown","metadata":{"id":"1D_RuoIRmNW6","colab_type":"text"},"source":["![batchnorm](http://kratzert.github.io/images/bn_backpass/BNcircuit.png)"]},{"cell_type":"markdown","metadata":{"id":"xxt77LHcmNW6","colab_type":"text"},"source":["---\n","\n","### a. Forward Train in Steps\n","For the convenience in calculating Batch Norm Backward Pass, we split the forward implementation into **9 steps** to get all the necessary intermediate value"]},{"cell_type":"markdown","metadata":{"id":"e_e3sYvqmNW7","colab_type":"text"},"source":["See the table below to track the steps\n","\n","<table>\n","    <tr align='left'>\n","        <td>(1) Mini-batch Mean</td>\n","        <td>|<br>|</td>\n","        <td align='left'>\n","        $$\n","        \\begin{align}\n","        \\mu B = \\frac{1}{N} \\sum\\limits_{i=1}^{N} x_i \\\\\n","        \\end{align}\n","        $$</td>\n","        <td>|<br>|</td>\n","        <td>\n","        $$\n","        \\begin{align}\n","        mu = \\frac{1}{N} * sum( x ) \\\\\n","        \\end{align}\n","        $$</td>\n","    </tr>\n","    <tr align='left'>\n","        <td>(2-4) Mini-batch Variance</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align*}        \n","        \\sigma^2_B = \\frac{1}{N} \\sum\\limits_{i=1}^{N}(x_i-\\mu B)^2\\\\\n","        \\end{align*}\n","        $$</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">    \n","        $$\n","        \\begin{align}\n","        xmu & = x-mu\\\\\n","        xmq & = xmu^2 \\\\\n","        variance & = \\frac{1}{N} * sum( xmq )\\\\\n","        \\end{align}\n","        $$</td>\n","    </tr>\n","    <tr align=\"left\">\n","        <td>(5-7) Normalize</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align*}        \n","        \\widehat{x_i} & = \\frac{x_i-\\mu B}{\\sqrt{\\sigma^2_B+\\epsilon}}\n","        \\end{align*}\n","        $$</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">    \n","        $$\n","        \\begin{align}\n","        sqrt\\_var & = \\sqrt{variance + \\epsilon}\\\\\n","        inv\\_var & = \\frac{1}{sqrt\\_var}\\\\\n","        x\\_norm & = xmu * inv\\_var\\\\\n","        \\end{align}\n","        $$</td>\n","    </tr>\n","    <tr align=\"left\">\n","        <td>(8-9) Scale and Shift</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align*}        \n","        y_i & = \\gamma\\widehat{x_i}+\\beta\n","        \\end{align*}\n","        $$</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">           \n","        $$\n","        \\begin{align*}        \n","        x\\_gamma & = gamma*x\\_norm\\\\\n","        out & = x\\_gamma+beta\n","        \\end{align*}\n","        $$</td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"8y5gc7FBmNW8","colab_type":"text"},"source":["---\n","### b. Running Average and Variance\n","\n","BatchNorm estimated means and standard deviations of all training data based on current batch of data. But the minibatch may not represent all training data. \n","\n","To compensate this, a running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.\n","      \n","<table>\n","    <tr align=\"left\">\n","        <td>Running Mean</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align}\n","        M\\mu B =\\ & m * M\\mu B\\\\ & +(1-m)*\\mu B\\\\\n","        \\end{align}\n","        $$</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align}\n","        rn\\_mean =\\ & momentum * rn\\_mean \\\\& +(1-momentum)*mu\\\\\n","        \\end{align}\n","        $$</td>\n","    </tr>\n","    <tr align=\"left\">\n","        <td>Running Variance</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align}\n","        M\\sigma^2_B =\\ & m * M\\sigma^2_B\\\\& +(1-m)*\\sigma^2_B\\\\\n","        \\end{align}\n","        $$</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align}\n","        rn\\_var =\\ & momentum * rn\\_var\\\\& +(1-momentum)*var\\\\\n","        \\end{align}\n","        $$</td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"bY1MthatmNW8","colab_type":"text"},"source":["---\n","### c. Forward Testing\n","At test time, BatchNorm will scale and shift the batch of testing data using the final running mean and running average acieved during training\n","\n","      \n","<table>\n","    <tr align=left>\n","        <td>Calculate Normalized $x$<br> based on running mean and variance</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align}\n","        \\widehat{x_i} & = \\frac{x_i-M\\mu B}{\\sqrt{M\\sigma^2_B+\\epsilon}} \\\\\n","        \\end{align}\n","        $$</td>\n","    </tr>\n","    <tr align=left>\n","        <td>Scale and Shift</td>\n","        <td>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align}\n","        y_i & = \\gamma\\widehat{x_i}+\\beta\n","        \\end{align}\n","        $$</td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Evnn1LP6X03D","colab_type":"text"},"source":["---\n","##3 - BatchNorm Forward API"]},{"cell_type":"markdown","metadata":{"id":"7tVwwk3CYCsv","colab_type":"text"},"source":["---\n","### a. Forward Function\n","Now implement both the maths of training and testing batchnorm above in a complete forward function"]},{"cell_type":"markdown","metadata":{"id":"g_tvMUXDmNW8","colab_type":"text"},"source":["---\n","\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement Batch Norm Forward Pass**\n","\n","\n","      "]},{"cell_type":"code","metadata":{"id":"c3oDv0HUmNW9","colab_type":"code","colab":{}},"source":["def batchnorm_forward(x, gamma, beta, bn_arg):\n","    \"\"\"\n","    Proses Forward Pass untuk Batch Normalization.\n","    \n","    Inputs:\n","    - x             : input matrix data berukuran (N, D)\n","    - gamma         : vektor parameter skala (scale parameter) berukuran (D,)\n","    - beta          : vektor parameter pergeseran (shift paremeter) berukuran (D,)\n","    - bn_arg        : Dictionary berisi parameter-parameter perhitungan dengan indeks keys sbb:\n","      - mode        : 'train' atau 'test'; mode proses yang dapat dipilih; required\n","      - eps         : konstanta untuk menjaga stabilitas nilai perhitungan (numerical stability)\n","      - momentum    : konstanta untuk perhitungan running mean / variance.\n","      - running_mean: Array berukuran (D,) berisi nilai running mean dari fitur input\n","      - running_var : Array berukuran (D,) berisi nilai running variance dari fitur input\n","\n","    Return sebuah tuple berisi:\n","    - out   : matrix output berukuran (N, D)\n","    - cache : sebuah tuple berisi nilai-nilai yang dibutuhkan untuk backward pass\n","    \"\"\"\n","    \n","    mode     = bn_arg['mode']\n","    eps      = bn_arg.get('eps', 1e-5)\n","    momentum = bn_arg.get('momentum', 0.9)\n","\n","    N, D         = x.shape\n","    running_mean = bn_arg.get('running_mean', np.zeros(D, dtype=x.dtype))\n","    running_var  = bn_arg.get('running_var', np.zeros(D, dtype=x.dtype))\n","\n","    out, cache = None, None\n","    if mode == 'train':\n","        \n","        # Step 1: calculate mu, use np.sum with axis=0\n","        # resulting shape of mu (D,)\n","        mu = ??\n","\n","        # Step 2: mean center x\n","        # resulting shape of var (N,D)\n","        xmu = ??\n","\n","        # Step 3: calculate square of mean center\n","        # resulting shape of xmq (N,D)\n","        xmq = ??\n","\n","        # Step 4: calculate variance, use np.sum with axis=0\n","        # resulting shape of var (D,)\n","        var = ??\n","\n","        # Step 5: calculate square root of variance, \n","        # add epsilon to prevent division by zero\n","        # resulting shape sqrt_var (D,)\n","        sqrt_var = ??\n","\n","        # Step 6: calculate inverse sqrt_var\n","        # resulting shape inv_var (D,)\n","        inv_var = ??\n","        \n","        # Step 7: normalize x        \n","        # resulting shape x_norm (N,D)\n","        x_norm = ??\n","\n","        # Step 8: scale (multiply) x_norm by gamma\n","        # resulting shape out (N,D)\n","        x_gamma = ??\n","        \n","        # Step 9: shift (add) x_gamma by beta\n","        # resulting shape out (N,D)\n","        out = ??\n","        \n","\n","        # Step 10: calculate running mean and average\n","        running_mean = ??\n","        running_var  = ??\n","        \n","        # store the necessary intermediate\n","        cache = (mu, xmu, xmq, var, sqrt_var, inv_var, x_norm, x_gamma, gamma, beta, x,  bn_arg)\n","\n","    elif mode == 'test':\n","        running_mean = bn_arg['running_mean']\n","        running_var = bn_arg['running_var']\n","        \n","        # Step 1: normalize x   \n","        # subtract by running_mean, \n","        # and divide by square root of running_var (add epsilon to prevent division by zero)\n","        # resulting shape x_norm (N,D)\n","        x_norm = (x - running_mean) / np.sqrt(running_var + eps)\n","                \n","        # Step 2: Scaly by gamma and shift by beta\n","        # resulting shape out (N,D)\n","        out = gamma * x_norm + beta\n","        \n","    else:\n","        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n","\n","    # Store the updated running means back into bn_arg\n","    bn_arg['running_mean'] = running_mean\n","    bn_arg['running_var']  = running_var\n","\n","    return out, cache\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4nFyhkt7mNW_","colab_type":"text"},"source":["---\n","### b. Check BatchNorm Forward Train\n","Check your implementation"]},{"cell_type":"code","metadata":{"id":"E1mYOd6qmNXA","colab_type":"code","colab":{}},"source":["np.random.seed(35)\n","N = 200\n","d_in =50\n","hidden=[60]\n","d_out=3\n","\n","W, b = init_weights(d_in, hidden, d_out, std=1, seed=35)\n","x = np.random.randn(N, d_in)\n","a = np.maximum(0, x.dot(W[0])).dot(W[1])\n","\n","print('Before batch normalization:')\n","print('  means: ', a.mean(axis=0))\n","print('  stds : ', a.std(axis=0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ht7O5uHomNXC","colab_type":"text"},"source":["**Expected result**:\n","<pre>Before batch normalization:\n","  means:  [-32.913745   14.1273126  -7.7720162]\n","  stds :  [40.1508681 37.4447318 33.7706377]"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"R0lpoCO0mNXD","colab_type":"code","colab":{}},"source":["# Nilai rata-rata (means) seharusnya mendekati 0\n","# dan standar deviasi (stds) mendekati 1\n","\n","print('After batch normalization (gamma=1, beta=0)')\n","a_norm, _ = batchnorm_forward(a, np.ones(d_out), np.zeros(d_out), {'mode': 'train'})\n","print('  mean: ', a_norm.mean(axis=0))\n","print('  std : ', a_norm.std(axis=0))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqp_dLsHmNXE","colab_type":"text"},"source":["**Expected result**:\n","<pre>After batch normalization (gamma=1, beta=0)\n","  mean:  [ 4.7910894e-16 -1.4765966e-16  1.6459056e-16]\n","  std :  [1. 1. 1.]\n"]},{"cell_type":"markdown","metadata":{"id":"eRmE1sGqmNXF","colab_type":"text"},"source":["Check with different `gamma` and `beta` for each dimension"]},{"cell_type":"code","metadata":{"id":"tXKsC25jmNXF","colab_type":"code","colab":{}},"source":["# Kini nilai rata-rata (means) seharusnya mendekati nilai beta\n","# dan standar deviasi (stds) mendekati gamma\n","\n","gamma = np.asarray([1.0, 2.0, 3.0])\n","beta = np.asarray([11.0, 12.0, 13.0])\n","a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n","print ('After batch normalization (nontrivial gamma, beta)')\n","print ('  means: ', a_norm.mean(axis=0))\n","print ('  stds : ', a_norm.std(axis=0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"seFL2xBSmNXI","colab_type":"text"},"source":["**Expected result**:\n","<pre>After batch normalization (nontrivial gamma, beta)\n","  means:  [11. 12. 13.]\n","  stds :  [1. 2. 3.]"]},{"cell_type":"markdown","metadata":{"id":"tbqS-YqcmNXI","colab_type":"text"},"source":["---\n","### c. Check BatchNorm Forward Test\n","Check forward pass in batchnorm in testing-time. First run forward batchnorm training several times to warm up (accumulate) the running mean and variance, then check the testing activation"]},{"cell_type":"code","metadata":{"id":"Ma9XqCdWmNXJ","colab_type":"code","colab":{}},"source":["# Periksa proses forward pass pada saat testing-time dengan menjalankan\n","# proses forward pass training beberapa kali untuk mengakumulasi nilai\n","# running average, kemudian periksa hasil nilai aktivasi means dan varians\n","# setelah proses forward pass testing-time\n","\n","np.random.seed(45)\n","W, b = init_weights(d_in, hidden, d_out, std=1, seed=45)\n","\n","gamma = np.ones(d_out)\n","beta = np.zeros(d_out)\n","    \n","bn_arg = {'mode': 'train'}    \n","for t in range(50):\n","    x = np.random.randn(N, d_in)\n","    a = np.maximum(0, x.dot(W[0])).dot(W[1])\n","    batchnorm_forward(a, gamma, beta, bn_arg)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UdPsG0UomNXK","colab_type":"text"},"source":["switch batchnorm to testing"]},{"cell_type":"code","metadata":{"id":"ir3ofivdmNXK","colab_type":"code","colab":{}},"source":["bn_arg['mode'] = 'test'\n","\n","np.random.seed(45)\n","x = np.random.randn(N, d_in)\n","a = np.maximum(0, x.dot(W[0])).dot(W[1])\n","a_norm, _ = batchnorm_forward(a, gamma, beta, bn_arg)\n","\n","# Hasil nilai rata-rata (means) harusnya akan berkisar nol,\n","# dan nilai standar deviasi (std) harusnya berkisar satu\n","# namun akan terlihat bernilai lebih \"kasar\" (noisy)\n","# dibandingkan hasil mean dan std pada saat forward pass training-time\n","print ('After batch normalization (test-time):')\n","print ('  means: ', a_norm.mean(axis=0))\n","print ('  stds : ', a_norm.std(axis=0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmDSVfGVmNXM","colab_type":"text"},"source":["**Expected result**:\n","<pre>After batch normalization (test-time):\n","  means:  [0.0044162 0.0420161 0.1427807]\n","  stds :  [0.9847371 0.9425361 1.0175674]"]},{"cell_type":"markdown","metadata":{"id":"rioEGoBWmNXP","colab_type":"text"},"source":["---\n","## 4 - BatchNorm Backward (Math)\n","\n","Again, let's see the gradient flow of Batch Normalization\n","\n","![batchnorm](http://kratzert.github.io/images/bn_backpass/BNcircuit.png)"]},{"cell_type":"markdown","metadata":{"id":"J9DzvsqKmNXQ","colab_type":"text"},"source":["---\n","### a. BatchNorm Backward Pass\n","From the intermediate values, we backtrack using chain rule to calculate the gradient of `x`, `gamma`, and `beta`\n","\n","The full implementation of Batch Norm Gradient is as follow:\n","\n","\n","<table width=700>\n","  <tr>\n","    <td width=100>$$\\begin{align*} \\partial\\beta & = \\end{align*} $$</td>\n","    <td>$$\n","\\begin{align*}     \n","\\sum\\limits_{i=1}^N\\partial out \n","\\end{align*}$$</td>\n","  </tr>\n","  <tr>\n","    <td>$$\\begin{align*} \\partial\\gamma & = \\end{align*} $$ </td>\n","    <td>$$\n","\\begin{align*}     \n","\\sum\\limits_{i=1}^N\\partial out * \\widehat{x_i}\n","\\end{align*}$$</td>\n","  </tr>\n","  <tr>\n","    <td>$$\\begin{align*} \\partial x_i = \\end{align*} $$ </td>\n","    <td>$$\\begin{align*}  \n","\\frac{\\gamma}{N*\\sqrt{\\sigma^2_B+\\epsilon}} * \\Bigg( \\big(N*\\partial out\\big) - \\left(\\sum\\limits_{i=1}^{N}\\partial out\\right)-\\left(\\frac{x_i-\\mu B}{\\sigma^2_B+\\epsilon}*\\sum\\limits_{i=1}^{N}\\big(\\partial out*(x_i-\\mu B)\\big)\\right)\\Bigg)\n","\\end{align*}\n","$$</td>\n","  </tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"ujVJZmmOmNXQ","colab_type":"text"},"source":["---\n","\n","### b. BatchNorm Backward in Steps\n","But for convenience, we split the implementation, match to the forward pass, into `9 steps`"]},{"cell_type":"markdown","metadata":{"id":"yqIMdteFmNXR","colab_type":"text"},"source":["See the table below to track the steps\n","\n","<table>\n","    <tr align=\"left\">\n","        <td>(9) Scale and Shift</td>\n","        <td>|<br>|<br>|<br>|<br>|<br>|</td>\n","        <td align='center'>\n","        $$\n","        \\begin{align*}     \n","        \\partial\\beta & = \\sum\\limits_{i=1}^N\\partial out   \\\\\n","        \\partial\\gamma & = \\sum\\limits_{i=1}^N(\\partial out * \\widehat{x_i})\\\\\n","        \\end{align*}\n","        $$</td>\n","        <td>|<br>|<br>|<br>|<br>|<br>|</td>\n","        <td align=\"left\">           \n","        $$\n","        \\begin{align*}        \n","        & dbeta & = & \\ \\ sum(dout) \\\\\\\\\n","        & dgamma & = & \\ \\ sum(x\\_norm * dout)\n","        \\end{align*}\n","        $$</td>\n","    </tr>\n","    <tr align=\"left\">\n","        <td>(6-8) Normalize</td>\n","        <td>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align*} \n","        \\\\\n","\t\t\\partial\\widehat{x_i} & = \\gamma * \\partial out\\\\\n","\t\t\\\\\n","\t\t\\partial\\left(\\frac{1}{\\sqrt{\\sigma^2 B+\\epsilon}}\\right) & = \\sum\\limits_{i=1}^{N}( (x_i-\\mu B) * \\partial\\widehat{x_i} )\\\\\n","\t\t\\\\\n","\t\t\\partial(x_i-\\mu B) & = \\frac{1}{\\sqrt{\\sigma^2_B+\\epsilon}}*\\partial\\widehat{x_i}\\\\\n","\t\t\\partial\\left(\\sqrt{\\sigma^2 B+\\epsilon}\\right) & = -\\frac{1}{\\sigma^2_B+\\epsilon}* \\partial\\left(\\frac{1}{\\sqrt{\\sigma^2_B+\\epsilon}}\\right)\n","        \\end{align*}\n","        $$</td>\n","        <td>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|</td>\n","        <td align=\"left\">    \n","        $$\n","        \\begin{align*}\n","\t\t& dx\\_norm & = & \\ \\ gamma * dout\\\\\\\\\n","\t\t& dinv\\_var & = & \\ \\ sum( xmu * dx\\_norm )\\\\\\\\\n","\t\t& dxmu & = & \\ \\ inv\\_var * dx\\_norm\\\\\\\\\n","\t\t& dsqrt\\_var & = & \\ \\ \\frac{-1}{ sqrt\\_var^2} * dinv\\_var\n","        \\end{align*}\n","        $$</td>\n","    </tr>\n","    <tr align=\"left\">\n","        <td>(3-5) Mini-batch Variance</td>\n","        <td>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align*} \n","        \\\\\n","\t\t\\partial\\left(\\sigma^2_B\\right) =\\ \\ & \\frac{1}{2\\sqrt{\\sigma^2_B + \\epsilon}} *  \\partial\\left(\\sqrt{\\sigma^2 B+\\epsilon}\\right)\\\\\n","\t\t\\\\\n","\t\t\\partial \\big((x_i-\\mu B)^2\\big) =\\ \\ & \\frac{1}{N} * \\partial\\left(\\sigma^2_B\\right)\\\\\n","\t\t\\\\\n","\t\t\\partial(x_i-\\mu B) =\\ \\ & \\partial (x_i-\\mu B) +  2 * (x_i-\\mu B)\\\\\\\\& * \\partial \\big((x_i-\\mu B)^2\\big)\\\\\\\\\n","        \\end{align*}\n","        $$</td>\n","        <td>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|</td>\n","        <td align=\"left\">    \n","        $$\n","        \\begin{align*}\n","\t\t& dvar & = &\\ \\ \\frac{1}{2 \\sqrt{var + \\epsilon}}  * dsqrt\\_var\\\\\\\\\n","\t\t& dxmq & = &\\ \\ \\frac{1}{N} * temp_{xmq} * dvar\\\\\\\\\n","\t\t& dxmu & = &\\ \\ dxmu + 2 * xmu * dxmq\n","        \\end{align*}\n","        $$</td>\n","    </tr>\n","    <tr align=left>\n","        <td>(1-2) Mini-batch Mean</td>\n","        <td>|<br>|<br>|<br>|<br>|<br>|</td>\n","        <td align=\"left\">\n","        $$\n","        \\begin{align*}\n","\t\t\\partial\\mu B & = -\\sum\\limits_{i=1}^{N}\\left(\\partial(x_i-\\mu B)\\right)\\\\\\\\\n","\t\t\\partial x_i & = \\partial(x_i-\\mu B) + \\frac{1}{N} *  \\partial\\mu B\n","        \\end{align*}\n","        $$</td>\n","        <td>|<br>|<br>|<br>|<br>|<br>|</td>\n","        <td>\n","        $$\n","        \\begin{align*}\n","\t\t& dmu & = &\\ \\ -sum(dxmu)\\\\\\\\\n","\t\t& dx & = &\\ \\ dxmu + \\frac{1}{N} * temp_{dx} * dmu\n","        \\end{align*}\n","        $$</td>\n","    </tr>\n","</table>\n","      "]},{"cell_type":"markdown","metadata":{"id":"ldGmMoufYcol","colab_type":"text"},"source":["---\n","## 5 - BatchNorm Backward API"]},{"cell_type":"markdown","metadata":{"id":"0vfHLFaMYmmT","colab_type":"text"},"source":["---\n","### a. Backward Function\n","Now implement the maths above in a complete backward function"]},{"cell_type":"markdown","metadata":{"id":"soMNtuSLmNXS","colab_type":"text"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement Batch Norm Backward pass**"]},{"cell_type":"code","metadata":{"id":"Q1KRABN_mNXS","colab_type":"code","colab":{}},"source":["def batchnorm_backward(dout, cache):\n","    \"\"\"\n","    Proses Backward Pass untuk Batch Normalization\n","\n","    Input:\n","    - dout: Turunan loss dari layer setelahnya, matrix berukuran (N, D)\n","    - cache: variabel berisi nilai intermediate dari fungsi batchnorm_forward.\n","\n","    Returns sebuah tuple berisi:\n","    - dx: matrix gradient terhadap input x, berukuran (N, D)\n","    - dgamma: vektor gradient terhadap parameter skala gamma, berukuran (D,)\n","    - dbeta: vektor gradient terhadap parmeter shift beta, berukuran(D,)\n","    \"\"\"\n","    dx, dgamma, dbeta = None, None, None\n","    \n","       \n","    mu, xmu, xmq, var, sqrt_var, inv_var, x_norm, x_gamma, gamma, beta, x, bn_arg = cache\n","    N, D = dout.shape\n","    eps = bn_arg.get('eps', 1e-5)\n","    \n","\n","    # Backprop Step 9\n","    # use np.sum with axis=0\n","    dbeta = ??\n","    dgamma = ??\n","\n","    # Backprop step 8\n","    dx_norm = ??\n","\n","    # Backprop step 7\n","    # use np.sum with axis=0\n","    dxmu = ??\n","    dinv_var = ??\n","\n","    # Backprop step 6\n","    dsqrt_var = ??\n","\n","    # Backprop step 5\n","    dvar = ??\n","\n","    # Backprop step 4\n","    temp_xmq = np.ones((xmq.shape))\n","    dxmq = ??\n","\n","    # Backprop step 3\n","    dxmu = ??\n","\n","    # Backprop step 2\n","    # use np.sum with axis=0\n","    dmu = ??\n","\n","    # Basckprop step 1\n","    temp_dx = np.ones((dxmu.shape))\n","    dx = ??\n","\n","\n","    return dx, dgamma, dbeta"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mc89OzfrmNXU","colab_type":"text"},"source":["---\n","### b. Check BatchNorm Backward\n","Check your implementation"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"vp3rUBvxmNXU","colab_type":"code","colab":{}},"source":["N, D = 4, 5\n","\n","np.random.seed(20)\n","x = 5 * np.random.randn(N, D) + 12\n","gamma = np.random.randn(D)\n","beta = np.random.randn(D)\n","dout = np.random.randn(N, D)\n","\n","\n","bn_arg = {'mode': 'train'}\n","_, cache = batchnorm_forward(x, gamma, beta, bn_arg)\n","dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n","\n","\n","print('dx\\n',dx)\n","print('\\ndgamma\\n',dgamma)\n","print('\\ndbeta\\n',dbeta)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2fTWqF7smNXW","colab_type":"text"},"source":["**Expected Output**: \n","<pre>dx\n"," [[-0.0649239  1.4176637  0.122448   0.0091537  0.0518538]\n"," [ 0.1267    -0.5254938 -0.1093556 -0.1241527  0.072954 ]\n"," [ 0.0212493 -0.3930962  0.0552021  0.019523  -0.0458642]\n"," [-0.0830254 -0.4990737 -0.0682945  0.095476  -0.0789436]]\n","<br>dgamma\n"," [-2.094374   0.1425473  4.3629382 -1.9807797  0.308084 ]\n","<br>dbeta\n"," [-2.2437568  0.1814855  0.1372466  5.138691  -0.0726342]"]},{"cell_type":"markdown","metadata":{"id":"ub-V0MZCmNXX","colab_type":"text"},"source":["---\n","## 6 - Weight Distribution with BatchNorm\n","\n","Let's try again feeding `5-Layered Neural Network` with 500 hidden neuron each 1000 random data. But now, we'll use Batch Normalization in each after affine layer.\n"]},{"cell_type":"code","metadata":{"id":"OWu9PCDgmNXX","colab_type":"code","colab":{}},"source":["Hs = {}\n","X = np.random.randn(1000, 500)\n","\n","d_in =500\n","hidden=[500, 500, 500, 500]\n","d_out=500\n","\n","W, b = init_weights(d_in, hidden, d_out)\n","gamma = np.ones(d_in)\n","beta = np.zeros(d_in)\n","bn_arg = {'mode': 'train'}\n","\n","a = X\n","for i in range(len(W)):\n","    z, _  = affine_forward(a, W[i], b[i])\n","    zn, _ = batchnorm_forward(z, gamma, beta,  bn_arg)\n","    a, _  = tanh_forward(zn)\n","    Hs[i] = a"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YYPpfuKlmNXY","colab_type":"text"},"source":["You'll see that the activation distribution for each layer now much more stable and centered compared to before"]},{"cell_type":"code","metadata":{"id":"jP46gcgJmNXZ","colab_type":"code","colab":{}},"source":["print('input layer had meann %f and std %f' % (np.mean(X), np.std(X)))\n","layer_means = [np.mean(H) for i, H in Hs.items()]\n","layer_stds = [np.std(H) for i, H in Hs.items()]\n","for i, H in Hs.items():\n","    print ('hidden layer %d had mean %f and std %f' % (i + 1, layer_means[i], layer_stds[i]))\n","\n","print()\n","plt.figure(figsize=(14,3))\n","plt.subplot(121)\n","plt.plot(list(Hs.keys()), layer_means, 'ob-')\n","plt.title('layer mean')\n","plt.subplot(122)\n","plt.plot(list(Hs.keys()), layer_stds, 'or-')\n","plt.title('layer std')\n","plt.show()\n","\n","\n","print('\\n\\nActivation Distribution')\n","plt.figure(figsize=(14,3))\n","for i, H in Hs.items():\n","    plt.subplot(1, len(Hs), i + 1)\n","    plt.hist(H.ravel(), 30, range=(-1, 1))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YvfQLx-mNXa","colab_type":"text"},"source":["---\n","## 7 - Batch Normalization Fast Backward\n","When calculating gradient in backward pass, there are two ways to implement it. One strategy is to write out a computation graph composed of simple operations and backprop through all intermediate values. Another strategy is to work out the derivatives on paper. For example, you can derive a very simple formula for the sigmoid function's backward pass by simplifying gradients on paper.\n","\n","\n","As shown before, if you're careful when deriving the formula, you'll get the full implementation of Batch Norm Gradient as follow:\n","\n","<br>\n","\n","<table width=700>\n","  <tr>\n","    <td width=100>$$\\begin{align*} \\partial\\beta & = \\end{align*} $$</td>\n","    <td>$$\n","\\begin{align*}     \n","\\sum\\limits_{i=1}^N\\partial out \n","\\end{align*}$$</td>\n","  </tr>\n","  <tr>\n","    <td>$$\\begin{align*} \\partial\\gamma & = \\end{align*} $$ </td>\n","    <td>$$\n","\\begin{align*}     \n","\\sum\\limits_{i=1}^N\\partial out * \\widehat{x_i}\n","\\end{align*}$$</td>\n","  </tr>\n","  <tr>\n","    <td>$$\\begin{align*} \\partial x_i = \\end{align*} $$ </td>\n","    <td>$$\\begin{align*}  \n","\\frac{\\gamma}{N*\\sqrt{\\sigma^2_B+\\epsilon}} * \\Bigg( \\big(N*\\partial out\\big) - \\left(\\sum\\limits_{i=1}^{N}\\partial out\\right)-\\left(\\frac{x_i-\\mu B}{\\sigma^2_B+\\epsilon}*\\sum\\limits_{i=1}^{N}\\big(\\partial out*(x_i-\\mu B)\\big)\\right)\\Bigg)\n","\\end{align*}\n","$$</td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","Implementing the simplified batch normalization backward pass should compute nearly identical results, but the alternative implementation should be a bit faster.\n","\n","Below is the implementation of faster batch normalization backward pass"]},{"cell_type":"code","metadata":{"id":"emLv5zeNmNXb","colab_type":"code","colab":{}},"source":["def batchnorm_backward_fast(dout, cache):\n","    \"\"\"\n","    Proses Backward Pass untuk Batch Normalization\n","\n","    Input:\n","    - dout: Turunan loss dari layer setelahnya, matrix berukuran (N, D)\n","    - cache: variabel berisi nilai intermediate dari fungsi batchnorm_forward.\n","\n","    Returns sebuah tuple berisi:\n","    - dx: matrix gradient terhadap input x, berukuran (N, D)\n","    - dgamma: vektor gradient terhadap parameter skala gamma, berukuran (D,)\n","    - dbeta: vektor gradient terhadap parmeter shift beta, berukuran(D,)\n","    \"\"\"\n","    dx, dgamma, dbeta = None, None, None\n","    \n","       \n","    mu, xmu, xmq, var, sqrt_var, inv_var, x_norm, x_gamma, gamma, beta, x, bn_arg = cache\n","    N, D = dout.shape\n","    eps = bn_arg.get('eps', 1e-5)\n","    \n","    dbeta = np.sum(dout, axis=0) \n","    dgamma = np.sum(dout * x_norm, axis=0)\n","    N = x.shape[0]\n","    dx = 1/N * gamma * (var + eps)**-0.5 * (N * dout - np.sum(dout, axis=0) - (x - mu) * (var + eps)**-1 * np.sum(dout * (x - mu), axis=0))\n","\n","    return dx, dgamma, dbeta"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kxl61fFZmNXc","colab_type":"text"},"source":["---\n","### a. Check BatchNorm Fast Backward"]},{"cell_type":"code","metadata":{"id":"VjCVVSKymNXd","colab_type":"code","colab":{}},"source":["N, D = 4, 5\n","\n","np.random.seed(20)\n","x = 5 * np.random.randn(N, D) + 12\n","gamma = np.random.randn(D)\n","beta = np.random.randn(D)\n","dout = np.random.randn(N, D)\n","\n","bn_arg = {'mode': 'train'}\n","_, cache = batchnorm_forward(x, gamma, beta, bn_arg)\n","dx, dgamma, dbeta = batchnorm_backward_fast(dout, cache)\n","\n","\n","print('dx\\n',dx)\n","print('\\ndgamma\\n',dgamma)\n","print('\\ndbeta\\n',dbeta)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJGZZs8mmNXe","colab_type":"text"},"source":["**Expected Output**: \n","<pre>dx\n"," [[-0.0649239  1.4176637  0.122448   0.0091537  0.0518538]\n"," [ 0.1267    -0.5254938 -0.1093556 -0.1241527  0.072954 ]\n"," [ 0.0212493 -0.3930962  0.0552021  0.019523  -0.0458642]\n"," [-0.0830254 -0.4990737 -0.0682945  0.095476  -0.0789436]]\n","<br>dgamma\n"," [-2.094374   0.1425473  4.3629382 -1.9807797  0.308084 ]\n","<br>dbeta\n"," [-2.2437568  0.1814855  0.1372466  5.138691  -0.0726342]"]},{"cell_type":"markdown","metadata":{"id":"1Xvqviw5mNXf","colab_type":"text"},"source":["---\n","### b. Compare BatchNorm Backward Implementations"]},{"cell_type":"code","metadata":{"id":"kW-aaKDYmNXf","colab_type":"code","colab":{}},"source":["def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qBZg2nomNXh","colab_type":"code","colab":{}},"source":["import time\n","\n","np.random.seed(231)\n","N, D = 10000, 500\n","x = 5 * np.random.randn(N, D) + 12\n","gamma = np.random.randn(D)\n","beta = np.random.randn(D)\n","dout = np.random.randn(N, D)\n","\n","bn_arg = {'mode': 'train'}\n","out, cache = batchnorm_forward(x, gamma, beta, bn_arg)\n","\n","t1 = time.time()\n","dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n","t2 = time.time()\n","dx2, dgamma2, dbeta2 = batchnorm_backward_fast(dout, cache)\n","t3 = time.time()\n","\n","print('dx difference        :', rel_error(dx1, dx2))\n","print('dgamma difference    :', rel_error(dgamma1, dgamma2))\n","print('dbeta difference     :', rel_error(dbeta1, dbeta2))\n","print('speed backward normal:',t2-t1,'seconds')\n","print('speed backward fast  :',t3-t2,'seconds')\n","print('speedup              : %.2fx' % ((t2 - t1) / (t3 - t2)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oEPEaq9NmNXi","colab_type":"text"},"source":["---\n","***\n","# [Part 7] Deep Neural Net with BatchNorm\n","Now let's build a deeper network using Batch Normalization to classify CIFAR-10 data"]},{"cell_type":"markdown","metadata":{"id":"_KXORpCMmNXk","colab_type":"text"},"source":["---\n","## 1 - Predict Function\n","\n","Add the Batch Norm to the predict function \n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input - <font color=\"blue\">N * [FC - BN - activation]</font> - FC Layer - argmax</b></pre>\n","\n","<br>\n","\n","The **N** is the number of hidden layer, which can be calculated from **len(W)-1**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hSKZEBp8mNXk","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement Predict Function\n","    * loop call forward function for each hidden layer weights\n","    * call forward function for the last layer"]},{"cell_type":"code","metadata":{"id":"dBei97aGQMO5","colab_type":"code","colab":{}},"source":["def predict_multi_layer_bn(X, W, b, bn_params, act_f ='tanh'):    \n","    \"\"\"\n","    Inputs:\n","    - X        : Input data, of shape(N, D)\n","    - W        : list of Weight\n","    - b        : list of biases\n","    - bn_params: list of batchnorm args\n","    - act_f    : activation function ('tanh' or 'relu')\n","    \n","    Output:\n","    - y_pred : list of class prediction\n","    \"\"\"\n","    \n","    \n","    gamma, beta, bn_args = bn_params\n","    y_pred = np.zeros(X.shape[1])\n","    n_layer = len(W)\n","        \n","    # first activation is X\n","    act = X\n","    \n","    ## ------------------------- start your code here -------------------------\n","    \n","    # loop i over n_layer-1\n","    for i in range(n_layer-1):\n","    \n","        # calculate layer score by calling affine forward function using act, W[i], and b[i]\n","        layer, _ = ??  \n","  \n","        # perform batch normalization by calling batchnorm forward function using \n","        # layer output score, gamma[i], beta[i], and bn_args[i]        \n","        bn_args[i]['mode'] = 'test'\n","        norm_layer, _ = ??\n","        bn_args[i]['mode'] = 'train'\n","  \n","        if ( act_f == 'tanh'):\n","            # calculate activation score by calling tanh forward function using norm_layer output\n","            act, _ = ??\n","          \n","        else:\n","            # calculate activation score by calling relu forward function using norm_layer output\n","            act, _ = ??\n","          \n","\n","    # calculate last layer score by calling affine forward function using act, W[-1], and b[-1]\n","    last_layer, _ = ??\n","    \n","    # use np.argmax with axis=-1 \n","    y_pred = ??\n","\n","    ## ------------------------- end your code here -------------------------\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hQVXD0TGmNXn","colab_type":"text"},"source":["\n","---\n","## 2 - Training Function\n","\n","Now let's add the Batch Norm to the training function\n","\n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input - <font color=\"blue\">N * [FC - BN - activation]</font> - FC Layer - Softmax</b></pre>\n","\n","<br>\n","\n","The **N** is the number of hidden layer, which can be calculated from **len(W)-1**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RgEb79YQmNXo","colab_type":"text"},"source":["<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","**Implement Training Function**\n","\n","there are **four steps** in this training function\n","\n","---\n","\n","**1. Forward Pass**\n","\n","    * loop over hidden layer [for len(W)-1]\n","        * call affine forward function\n","        * call batchnorm forward function\n","        * call activation forward function\n","    * call affine forward function for the last layer\n","    * call softmax score function\n","\n","**2. Calculate Loss**\n","\n","    * call softmax_loss function\n","    * loop over weights [for W]\n","        * calculate loss with regularization\n","\n","\n","**3. Backward Pass**\n","\n","    * call affine backward function for the last layer\n","    * loop over hidden layer [from len(W)-2 to 0]\n","        * call activation backward function\n","        * call batchnorm backward function\n","        * call affine backward function\n","\n","**4. Weight Update**\n","\n","    * loop over weights [for W]\n","        * implement weight update\n","    * calculate the training and validation accuracy\n"]},{"cell_type":"code","metadata":{"id":"bB42-A-omNXp","colab_type":"code","colab":{}},"source":["def train_multi_layer_bn(X, y, X_val, y_val, hidden_size, act_f='tanh',\n","                         W=None, b=None, std=1e-4, seed=None,\n","                         lr=1e-4, lr_decay=0.95, reg=0.25, \n","                         epochs=100, batch_size=200, verbose=True):\n","    \"\"\"\n","    Inputs:\n","    - X          : array of train data, of shape (N, D)\n","    - y          : array of train labels, of shape (N,)\n","    - X_val      : array of validation data, of shape (Nv, D)\n","    - y_val      : array of validation labels, of shape (Nv,)\n","    - hidden_size: list of hidden neuron for each hidden layer\n","    - act_f      : activation function ('tanh' or 'relu')\n","    - W          : list of Weight, if W is None, it will be initialized\n","    - b          : list of biases, if W is None, bias will be initialized\n","    - std        : float, standar deviation for generating weights\n","    - seed       : int, initial random seed\n","    - lr         : float, initial learning rate\n","    - lr_decay   : float, 0-1, decay rate to reduce learning rate each epoch\n","    - reg        : float, regularization rate\n","    - epochs     : int, number of training epoch\n","    - batch_size : int, number of batch used each step\n","    - verbose    : boolean, verbosity\n","    \n","    Outputs:\n","    - W          : list of trained Weights\n","    - b          : list of trained biases\n","    - history    : list of training history [loss, train_acc, val_acc]\n","    \n","    \"\"\"\n","    \n","    num_train, dim = X.shape\n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","    \n","    # assume y takes values 0...K-1 where K is number of classes\n","    num_classes = np.max(y) + 1  \n","    \n","    # initialize Weights\n","    if W is None:\n","        W, b = init_weights(dim, hidden_size, num_classes, std, seed) \n","        \n","    # number of layer (including output layer)\n","    n_layer = len(W)\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","    \n","    #initialize batch parameters\n","    bn_args = {}\n","    gamma = {}\n","    beta = {}\n","    for i in range(n_layer-1):\n","        bn_args[i] = {'mode': 'train'} \n","        gamma[i] = np.ones(hidden_size[i], )\n","        beta[i] = np.zeros(hidden_size[i], )\n","    \n","            \n","    ## ------------------------- start your code here --------------------------\n","    \n","    \n","    print('start training using', act_f, 'activation function')\n","    for ep in range(epochs):\n","        # Shuffle data train index\n","        train_rows = np.arange(num_train)\n","        np.random.shuffle(train_rows)\n","        \n","        # split index into mini batches\n","        id_batch = np.split(train_rows, num_iter)\n","  \n","        for batch in id_batch:\n","      \n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","        \n","            # store all cache in dictionary\n","            cache = {}\n","\n","            # first layer activation input is X_batch\n","            act = X_batch\n","\n","            # -----------------------------------------------------------------\n","            # 1. Forward Pass\n","            # -----------------------------------------------------------------\n","\n","            # loop i over hidden layer (n_layer-1)\n","            # see predict function implementation\n","            for i in ??:\n","\n","                # calculate layer score by calling affine forward function using act, W[i], and b[i]\n","                layer, cache_affine = ??\n","\n","                # perform batch normalization by calling batchnorm forward function using \n","                # layer score, gamma[i], beta[i], and bn_args[i]\n","                norm_layer, cache_bn = ??\n","\n","                if ( act_f == 'tanh'):\n","                    # calculate activation score by calling tanh forward function using norm_layer score\n","                    act, _ = ??\n","                else:\n","                    # calculate activation score by calling relu forward function using norm_layer score\n","                    act, _ = ??\n","\n","                # combine cache from affine, batchnorm, and activation layer into cache for this layer\n","                cache[i] = (cache_affine, cache_bn, cache_act)\n","\n","            # calculate last layer score by calling affine forward function using activation act, W[i+1], and b[i+1]\n","            last_layer, cache[i+1] = ??\n","\n","            # calculate softmax score by calling softmax function using last_layer score\n","            softmax_score = ??\n","\n","            # -----------------------------------------------------------------\n","            # 2. Calculate Loss\n","            # -----------------------------------------------------------------\n","\n","            # evaluate loss and gradient by calling softmax_loss function using input softmax_score and y_batch\n","            loss, dout = ??\n","\n","            # add regularization to the loss:\n","            #    for each weights, calculate the sum square, multiply regularization strength\n","            #    then add it to the loss\n","            # see the implementation in the previous Task\n","            for w in W:\n","                # loss = loss + reg * sum(w*w)\n","                loss += ??  \n","\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","\n","            # -----------------------------------------------------------------\n","            # 3. Backward Pass\n","            # -----------------------------------------------------------------    \n","\n","            # dictionary to contain all gradients\n","            dW = {}\n","            db = {}\n","            dgamma = {}\n","            dbeta = {}\n","\n","            # calculate last weights gradient by calling affine backward function using dout and cache[n_layer-1]\n","            dW[n_layer-1], db[n_layer-1], dact = ??\n","\n","            #loop i from n_layer-2 down to 0\n","            for i in range(n_layer-2,-1,-1):\n","\n","                # extract affine cache and activation cache from layer cache\n","                cache_affine, cache_bn, cache_act = cache[i]\n","\n","                if ( act_f == 'tanh'):\n","                    # calculate tanh gradient by calling tanh backward function using dact and cache_act\n","                    dlayer = ??\n","                else:\n","                    # calculate relu gradient by calling relu backward function using dact and cache_act\n","                    dlayer = ??\n","\n","                # calculate batchnorm gradient by calling batchnorm backward fast function using dlayer and cache_bn\n","                dlayer, dgamma[i], dbeta[i] = ??\n","\n","                # calculate layer weights gradient by calling affine backward function using dlayer and cache_affine\n","                dW[i], db[i], dact = ??\n","\n","                # add regularization to gradient\n","                # add with twice of the weight multiplied by regularization strength\n","                # see previous implementation\n","                dW[i] += ??\n","    \n","    \n","\n","            # -----------------------------------------------------------------\n","            # 4. Weight Update\n","            # -----------------------------------------------------------------    \n","\n","            # perform parameter update by subtracting W[i] and b[i] for each layer \n","            # with a fraction of dW[i] and db[i] according to the learning rate\n","            # loop over W\n","            # see previous implementation\n","            for i in range(len(W)):    \n","                W[i] = ??\n","                b[i] = ??\n","\n","                \n","            # perform the same thing for gamma[i] and beta[i] for each layer\n","            # update by a fraction of dgamma[i] and dbeta[i] according to the learning rate\n","            for i in range(len(gamma)):\n","                # gamma_i = gamma_i - lr * dgamma_i\n","                gamma[i] = ??\n","                # beta_i = beta_i - lr * dbeta_i\n","                beta[i]  = ??\n","        \n","        \n","            # iteration count\n","            it +=1\n","\n","            if verbose and it % 100 == 0:\n","                print ('iteration',it,'(epoch', ep+1,'/',epochs, '): loss =', loss)\n","\n","                \n","                \n","        # At the end of one epoch\n","        # 1. Check accuracy\n","        #    calculate the training accuracy by calling predict_multi_layer_bn function on X_batch\n","        #    and compare it to y_batch. Then calculate the mean correct (accuracy in range 0-1)\n","        train_acc = (predict_multi_layer_bn(X_batch, W, b, [gamma, beta, bn_args], act_f) == y_batch).mean()\n","        train_acc_history.append(train_acc)\n","\n","        # 2. Calculate the training accuracy by calling predict_multi_layer_bn function on X_val\n","        #    and compare it tu y_val. Then calculate the mean correct (accuracy in range 0-1)\n","        val_acc = (predict_multi_layer_bn(X_val, W, b, [gamma, beta, bn_args], act_f) == y_val).mean()\n","        val_acc_history.append(val_acc)\n","\n","        # 3. Decay learning rate\n","        #    multiply learning rate with decay\n","        # see previous implementation\n","        lr = ??\n","\n","            \n","    \n","    ## -------------------------- end your code here ---------------------------\n","        \n","    history = [loss_history, train_acc_history, val_acc_history]\n","    bn_params = [gamma, beta, bn_args]\n","    print('Training done')\n","    \n","    return W, b, history, bn_params"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rqs85ArdmNXq","colab_type":"text"},"source":["---\n","## 3 - Train Tanh with BatchNorm\n","\n","Try the training Function using 3 hidden layer, 50 neurons each\n","\n","You should see major improvement in accuracy when using more layer\n"]},{"cell_type":"markdown","metadata":{"id":"r3CfxkdiQc0g","colab_type":"text"},"source":["---\n","### a. Train Network\n","First let's try the four layered tanh using batchnorm"]},{"cell_type":"code","metadata":{"id":"oqZSvxLQmNXq","colab_type":"code","colab":{}},"source":["hidden_size=[50, 50, 50]\n","lr=1e-2\n","np.random.seed(None)\n","\n","W_tanh_4bn, b_tanh_4bn, history_tanh_4bn, bn_params_tanh = train_multi_layer_bn(    \n","    X_train, y_train, X_val, y_val, \n","    hidden_size=hidden_size, \n","    act_f='tanh',\n","    std=1e-2, lr=lr, \n","    lr_decay=0.9, reg=0.01, \n","    epochs=20)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FErX_kN8agzO"},"source":["**Expected Output**:\n","<pre>\n","loss should starts around 2.2 and ends around 1.5"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SOOHxkCsagzh"},"source":["---\n","### b. Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"TWiqYVdlagzq","colab":{}},"source":["loss, train_acc, val_acc = history_tanh_4bn\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"colab_type":"text","id":"7cQELxPYagzz"},"source":["**Expected Results**:\n","\n","<pre>You should find that using batch normalization helps the network to converge faster.\n","\n","e.g. Higher Accuracy with the same epoch"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cTB19TsCagz2"},"source":["---\n","### c. Training Accuracy\n","Calculate the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WjkNZyrAagz4","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_multi_layer_bn(X_train, W_tanh_4bn, b_tanh_4bn, bn_params_tanh, act_f='tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z6W6OXRmagz9"},"source":["**Expected Output**:\n","\n","<pre>You should be able to get about <b>~54%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GiGDI20eagz-","colab":{}},"source":["y_pred = predict_multi_layer_bn(X_val, W_tanh_4bn, b_tanh_4bn, bn_params_tanh, act_f='tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HiUtNqwGag0B"},"source":["**Expected Output**:\n","\n","<pre>You should also be able to get about <b>~48%</b> accuracy on validation set</pre>\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RMbpGHNtcw4a"},"source":["---\n","### d. Comparison\n","let's compare between 4 layer tanh with and without batchnorm"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9CAulaL7cw4g","colab":{}},"source":["loss_tanh_4, train_tanh_4, val_tanh_4 = history_tanh_4\n","loss_tanh_4bn, train_tanh_4bn, val_tanh_4bn = history_tanh_4bn\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(train_tanh_4, label='tanh')\n","plt.plot(train_tanh_4bn, label='tanh+bn')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Training Accuracy')\n","\n","plt.subplot(122)\n","plt.plot(val_tanh_4, label='tanh')\n","plt.plot(val_tanh_4bn, label='tanh+bn')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Validation Accuracy')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a6DB_9HUcw4m"},"source":["**Expected Output**:\n","<pre>You should find that using batch normalization helps the network to converge much faster.\n","\n","e.g. Higher Accuracy with the same epoch"]},{"cell_type":"markdown","metadata":{"id":"l10MqwfskIpg","colab_type":"text"},"source":["And what's more, let's compare the Accuracy between **tanh network trained with batchnorm** and **relu network trained without batchnorm**\n"]},{"cell_type":"code","metadata":{"id":"7zd9LcITkWpx","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(train_relu_4, label='relu')\n","plt.plot(train_tanh_4bn, label='tanh+bn')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Training Accuracy')\n","\n","plt.subplot(122)\n","plt.plot(val_relu_4, label='relu')\n","plt.plot(val_tanh_4bn, label='tanh+bn')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Validation Accuracy')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X2RRZGAakdkN"},"source":["**Expected Output**:\n","<pre>You should find that the tanh network trained using batch normalization has its accuracy increases to almost equal to relu network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2A8r2ltZcBaU"},"source":["---\n","## 4 - Train ReLU with BatchNorm\n","\n","Try the training Function using 3 hidden layer, 50 neurons each\n","\n","You should see major improvement in accuracy when using more layer\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"L7GRfGcWcBaX"},"source":["---\n","### a. Train Network\n","First let's try the four layered tanh using batchnorm"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X3r6YugocBaZ","colab":{}},"source":["hidden_size=[50, 50, 50]\n","lr=1e-2\n","np.random.seed(None)\n","\n","W_relu_4bn, b_relu_4bn, history_relu_4bn, bn_params_relu = train_multi_layer_bn(    \n","    X_train, y_train, X_val, y_val, \n","    hidden_size=hidden_size, \n","    act_f='relu',\n","    std=1e-2, lr=lr, \n","    lr_decay=0.9, reg=0.01, \n","    epochs=20)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4a365fOmcBaf"},"source":["**Expected Output**:\n","<pre>\n","loss should starts around 2.2 and ends around 1.2"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rm9rNnkUcBah"},"source":["---\n","### b. Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"3fbFpEV6cBaj","colab":{}},"source":["loss, train_acc, val_acc = history_relu_4bn\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"colab_type":"text","id":"NCWv0OPlcBan"},"source":["**Expected Results**:\n","\n","<pre>You should find that using batch normalization helps the network to converge much faster.\n","\n","e.g. Higher Accuracy with the same epoch"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x-C3YVc5cBao"},"source":["---\n","### c. Training Accuracy\n","Calculate the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PS0ebT3FcBaq","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_multi_layer_bn(X_train, W_relu_4bn, b_relu_4bn, bn_params_relu, act_f='relu')\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xumIO9-BcBat"},"source":["**Expected Output**:\n","\n","<pre>You should be able to get about <b>~64%</b> accuracy on training set using the initial run"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7pREpJvZcBav","colab":{}},"source":["y_pred = predict_multi_layer_bn(X_val, W_relu_4bn, b_relu_4bn, bn_params_relu, act_f='relu')\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OOyyYPTGcBaz"},"source":["**Expected Output**:\n","\n","<pre>You should also be able to get about <b>~53%</b> accuracy on validation set</pre>\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LaJcGTfobaIK"},"source":["---\n","### d. Comparison\n","let's compare between 4 layer relu with and without batchnorm"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y1CsPrlObaIT","colab":{}},"source":["loss_relu_4, train_relu_4, val_relu_4 = history_relu_4\n","loss_relu_4bn, train_relu_4bn, val_relu_4bn = history_relu_4bn\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(train_relu_4, label='relu')\n","plt.plot(train_relu_4bn, label='relu+bn')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Training Accuracy')\n","\n","plt.subplot(122)\n","plt.plot(val_relu_4, label='relu')\n","plt.plot(val_relu_4bn, label='relu+bn')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc')\n","plt.title('Validation Accuracy')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RuRMdxFcbaId"},"source":["**Expected Output**:\n","<pre>You should find that using batch normalization helps the network to converge faster, but in this case, not by much.\n","\n","This happen because relu does not squash the activation distribution like tanh.\n","\n","It means that relu already perform well without natchnorm, and the addition of batchnorm helps a bit"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"60Tp15oWbaIf"},"source":["---\n","## 5 - Comparison\n","Now let's see if we compare all four networks"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"05g1hViKbaIh","colab":{}},"source":["plt.rcParams['figure.figsize'] = [14, 5]\n","plt.subplots_adjust(wspace=0.25)\n","plt.subplots_adjust(hspace=0.3)\n","\n","plt.subplot(121)\n","plt.plot(train_tanh_4, label='tanh')\n","plt.plot(train_tanh_4bn, label='tanh+bn')\n","plt.plot(train_relu_4, label='relu')\n","plt.plot(train_relu_4bn, label='relu+bn')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc tanh')\n","plt.title('Training Accuracy')\n","\n","plt.subplot(122)\n","plt.plot(val_tanh_4, label='tanh')\n","plt.plot(val_tanh_4bn, label='tanh+bn')\n","plt.plot(val_relu_4, label='relu')\n","plt.plot(val_relu_4bn, label='relu+bn')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('acc tanh')\n","plt.title('Validation Accuracy')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dCCfLvrdbaIn"},"source":["**Expected Output**:\n","\n","<pre>You should see that Batch Norm increase the accuracy"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 3\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bvBk-u7_92ub"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}