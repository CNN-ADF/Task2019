{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwZB6XzTCkOF"
   },
   "source": [
    "![title](https://image.ibb.co/erDntK/logo2018.png)\n",
    "\n",
    "---\n",
    "# [Class Exercise] Gradient Descent and Neural Network \n",
    "\n",
    "In this exercise you will practice a simple Neural Network, including:\n",
    "\n",
    "    * simple neuron API\n",
    "    * gradient descent introduction\n",
    "    * memory consumption in backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6ZFmhlI8CkOR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwIMoQeVCkPD"
   },
   "source": [
    "---\n",
    "# 1 - Neural Network\n",
    "\n",
    "Neural Network is almost always ilustrated as having the same computational work as in Human Brain, especially considering its main component: ***Neuron***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwIMoQeVCkPD"
   },
   "source": [
    "## Neuron\n",
    "Neuron Human  |  Neural Network Neuron\n",
    "-- | --\n",
    "![neuron](http://cs231n.github.io/assets/nn1/neuron.png) | ![neuron](http://cs231n.github.io/assets/nn1/neuron_model.jpeg)\n",
    "\n",
    "Based on that concept and analogy, we can implement **forward pass function** of a neuron in `Python` as follow:\n",
    "```python\n",
    "def forward(self, inputs):\n",
    "    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n",
    "    cell_body_sum = np.sum(inputs * self.weights) + self.bias   # affine function\n",
    "    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum))        # sigmoid activation function\n",
    "    return firing_rate\n",
    "```\n",
    "\n",
    "The basic computation inside a neuron is a weighted sum of its input. Neural Network then learn by modifying the weights in each neuron to minimize the output error.\n",
    "\n",
    "To simplify the computation, all neurons are grouped in stacks called **layers**. Thus, all weights of neurons in a layer can be formed as a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer Perceptron\n",
    "As we've seen in previous exercises, Single Layer Perceptron is essentially a Linear Classifier. With only one layer in the network, the architecture illustration is as described below\n",
    "\n",
    "\n",
    "![onelayer](https://image.ibb.co/fjR3oz/onelayer.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron\n",
    "We can further stacks the layers of neuron into a deeper architecture called Multi-Layer Perceptron (MLP). Layers located between input and output layer are called hidden layers.\n",
    "\n",
    "MLP with 2 neuron layers called *2-Layer Neural Network* or *1-Hidden Neural Network*. The same apply for MLP with 3 layers called *3-Layer Neural Net* or *2-Hidden Neural Net*. Below are the illustration of 2-layer and 3-layer net \n",
    "\n",
    "*2-layer NN* | *3-layer NN*\n",
    "- | -\n",
    "![2layerNN](https://image.ibb.co/dHnnFe/2layerNN.png) | ![3layerNN](https://image.ibb.co/iH18MK/3layerNN.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Backpropagation\n",
    "You've also seen this in previous exercises, that in learning a Neural Network using Gradient Descent, there are several steps to be made:\n",
    "    * forward pass to multipy weights and input\n",
    "    * calculate error\n",
    "    * backward pass to get the input gradients and weights gradients\n",
    "    \n",
    "If we implement it in a simple python, the code for Single Layer Perceptron will need just several lines of code as follow:\n",
    "```python\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    layer = np.dot(fitur, bobot0)+bias0\n",
    "    aktivasi = 1 / (1 + np.exp(-layer))    \n",
    "\n",
    "    error = target - aktivasi\n",
    "\n",
    "    g_aktivasi = (err) * (aktivasi * (1 - aktivasi))\n",
    "    g_bobot0 = fitur.T.dot(layer)\n",
    "    \n",
    "    bobot0 = bobot0 + lr*g_bobot\n",
    "```\n",
    "\n",
    "You'll notice that to train Multi Layered Perceptron is essentially repeating the forward pass for each layer, continued by repeating reversely backward pass through each layer. \n",
    "\n",
    "We can implement each prward/backward pass for every specific architecture, but that will be too wastefull. Instead, we can build several API functions so that we can easily add or remove layers in an architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Neural Network API\n",
    "\n",
    "Implementing functional API to build and train Deep Neural Network is what have been done by popular Deep Learning Library and frameworks such as Keras, Tensorflow, and Torch\n",
    "\n",
    "If we look closely, there are actualy two functions inside a neuron: **Affine** function and **Activation** function.\n",
    "\n",
    "We can illustrated **Affine** function as gathering the impulse (firing rate) from input or previus neuron activation then combine (multiply) it with its stored knowledge (weights). The illustration of **Activation** function is to give nonlinearity as well normalize the output back to impulse form (0-1)\n",
    "\n",
    "There are several classic activation functions that are widely used.\n",
    "*Sigmoid function* | *Tanh function*\n",
    "-- | --\n",
    "![sigmoid](http://cs231n.github.io/assets/nn1/sigmoid.jpeg) | ![tanh](http://cs231n.github.io/assets/nn1/tanh.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine Function\n",
    "The first is the affine or dense ayer as you've implemented before. The forward affine function is as follow:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x, W, b) = x.W + b\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, W, b):   \n",
    "  \n",
    "    v = x.dot(W)+b#?? # x dot w + b\n",
    "    \n",
    "    cache = (x, W, b)\n",
    "    \n",
    "    return v, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then continued by the backward pass:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\partial W & = x^T.\\partial out \\\\\n",
    "\\partial b & = \\sum \\partial out \\\\\n",
    "\\partial x & = \\partial out.W^T \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \n",
    "    x, W, b = cache\n",
    "    \n",
    "    dW = x.T.dot(dout)#?? # x' dot dout\n",
    "    \n",
    "    db = np.sum(dout, axis=0, keepdims=True)\n",
    "    \n",
    "    dx = dout.dot(W.T)#?? # dout dot W'\n",
    "    \n",
    "    return dW, db, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "Next is the implementation of Sigmoid activation function\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) = \\sigma(x) = \\frac{1}{1+e^{-v}}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(x):  \n",
    "  \n",
    "    out = 1./(1.+np.exp(-x))#?? # 1 / ( 1 + exp(-x) ) \n",
    "    \n",
    "    return out  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the backward function\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sigma'(x) = \\sigma(x) \\ (1 - \\sigma(x))\\\\\\\\\n",
    "\\partial out = \\partial out . \\sigma'(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dout, ds):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "        ds: sigmoid forward result\n",
    "        dout: gradient error\n",
    "    \"\"\"\n",
    "    ds_ = ds*(1.-ds)#?? # ds * ( 1 - ds )\n",
    "    \n",
    "    dout = dout * ds_\n",
    "    \n",
    "    return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Using Neural Network API\n",
    "\n",
    "With the API implementation is sorted, we can now easily build the Neural Network. \n",
    "To implement a one training epoch for Single Layer Perceptron, we just need to stack these functions\n",
    "<pre><font color='green'>affine_fwd</font> -> \n",
    "    <font color='blue'>sigmoid_fwd</font> ->\n",
    "        <font color='red'>calculate_error</font> ->\n",
    "    <font color='blue'>sigmoid_bwd</font> -> \n",
    "<font color='green'>affine_bwd</font> -> \n",
    "weights_update</pre>\n",
    "\n",
    "---\n",
    "Then, to build a 2 Layer Neural Net (1 hidden layer), we only need to add several functions \n",
    "<pre><font color='green'>affine_fwd</font> -> \n",
    "    <font color='blue'>sigmoid_fwd</font> ->\n",
    "        <font color='green'>affine_fwd</font> -> \n",
    "            <font color='blue'>sigmoid_fwd</font> ->\n",
    "                <font color='red'>calculate_error</font> ->\n",
    "            <font color='blue'>sigmoid_bwd</font> -> \n",
    "        <font color='green'>affine_bwd</font> -> \n",
    "    <font color='blue'>sigmoid_bwd</font> -> \n",
    "<font color='green'>affine_bwd</font> -> \n",
    "weights_update</pre>\n",
    "\n",
    "Let's try it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Sanity Check: Gradient Example\n",
    "\n",
    "First let's check if the implementation is correct\n",
    "\n",
    "![sgdxample](https://image.ibb.co/kV2NYU/04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = np.array([[3, 2]])\n",
    "w_s = np.array([[-3], [4]])\n",
    "b_s = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, c1 = affine_forward(x_s, w_s, b_s)\n",
    "output = sigmoid_forward(v)\n",
    "print('output =', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0.3\n",
    "dout = sigmoid_backward(error, output)\n",
    "print('dout =', dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_s, db_s, dx_s = affine_backward(dout, c1)\n",
    "print('dw_s =\\n', dw_s)\n",
    "print('\\ndb_s =', db_s)\n",
    "print('\\ndx_s =', dx_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer Perceptron\n",
    "Let's create a simple data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "\n",
    "nfitur = x.shape[1]   # 3 fitur\n",
    "nlabel = y.shape[1]   # 1 bit label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we need the weights, so craete it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi bobot\n",
    "w0 = 2*np.random.random((nfitur, nlabel)) -1\n",
    "b0 = np.zeros((1, nlabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now train it for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proses maju\n",
    "layer1, cache1 = affine_forward(x, w0, b0)\n",
    "aktivasi1 = sigmoid_forward(layer1)\n",
    "\n",
    "# hitung error\n",
    "error = y - aktivasi1\n",
    "print(\"mse = %0.7f\" % (np.mean(error ** 2)))\n",
    "\n",
    "#proses mundur\n",
    "g_layer1 = sigmoid_backward(error, aktivasi1)\n",
    "dw0, db0, dx = affine_backward(g_layer1, cache1)\n",
    "\n",
    "#update bobot\n",
    "lr = 0.2\n",
    "w0 += lr * dw0\n",
    "b0 += lr * db0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train it further by repeatedly running the cell above\n",
    "\n",
    "You'll see that over time, the loss should decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi Layer Perceptron using API\n",
    "For 2 Layered Neural Network, we can implement as follow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi bobot\n",
    "nhidden = 10\n",
    "w0 = 2*np.random.random((nfitur, nhidden)) -1\n",
    "w1 = 2*np.random.random((nhidden, nlabel)) -1\n",
    "b0 = np.zeros((1, nhidden))\n",
    "b1 = np.zeros((1, nlabel))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then the implementation for one epoch is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proses maju\n",
    "layer1, cache1 = affine_forward(x, w0, b0)\n",
    "aktivasi1 = sigmoid_forward(layer1)\n",
    "\n",
    "layer2, cache2 = affine_forward(aktivasi1, w1, b1)\n",
    "aktivasi2 = sigmoid_forward(layer2)\n",
    "\n",
    "\n",
    "# hitung error\n",
    "error = y - aktivasi2\n",
    "print(\"mse = %0.7f\" % (np.mean(error ** 2)))\n",
    "\n",
    "\n",
    "#proses mundur\n",
    "g_layer2 = sigmoid_backward(error, aktivasi2)\n",
    "dw1, db1, g_aktivasi1 = affine_backward(g_layer2, cache2)\n",
    "\n",
    "g_layer1 = sigmoid_backward(g_aktivasi1, aktivasi1)\n",
    "dw0, db0, dx = affine_backward(g_layer1, cache1)\n",
    "\n",
    "\n",
    "\n",
    "#update bobot\n",
    "lr = 0.2\n",
    "w1 += lr * dw1\n",
    "w0 += lr * dw0\n",
    "b1 += lr * db1\n",
    "b0 += lr * db0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train it further by repeatedly running the cell above\n",
    "\n",
    "You'll see that over time, the loss should decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Training Function\n",
    "Now let's implement a function to train the MLP several epoch, and track its loss over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xN3KkSsKCkXp"
   },
   "outputs": [],
   "source": [
    "def train_two_layer_nn(x, y, nhidden, lr, max_epoch=500, verbose=0):\n",
    "    \n",
    "    np.random.seed(int(8))\n",
    "    \n",
    "    \n",
    "    if verbose==1:\n",
    "        print('pembelajaran dimulai')\n",
    "        print('ukuran x =',x.shape)\n",
    "        print('ukuran y =',y.shape)\n",
    "\n",
    "    nfitur = x.shape[1]\n",
    "    nlabel = y.shape[1]\n",
    "    \n",
    "    w0 = 2*np.random.random((nfitur, nhidden)) -1\n",
    "    w1 = 2*np.random.random((nhidden, nlabel)) -1\n",
    "\n",
    "    b0 = np.zeros((1, nhidden))\n",
    "    b1 = np.zeros((1, nlabel))\n",
    "\n",
    "\n",
    "    mse = []\n",
    "    for ep in range(max_epoch):\n",
    "        layer1, cache1 = affine_forward(x, w0, b0)\n",
    "        aktivasi1 = sigmoid_forward(layer1)\n",
    "\n",
    "        layer2, cache2 = affine_forward(aktivasi1, w1, b1)\n",
    "        aktivasi2 = sigmoid_forward(layer2)\n",
    "\n",
    "        err = y - aktivasi2\n",
    "        cur_mse = np.mean(err ** 2)\n",
    "        mse.append(cur_mse)\n",
    "        \n",
    "        if verbose==1:\n",
    "            print('epoch=', ep, 'mse=', cur_mse)\n",
    "            \n",
    "        g_layer2 = sigmoid_backward(err, aktivasi2)\n",
    "        dw1, db1, g_aktivasi1 = affine_backward(g_layer2, cache2)\n",
    "\n",
    "        g_layer1 = sigmoid_backward(g_aktivasi1, aktivasi1)\n",
    "        dw0, db0, dx = affine_backward(g_layer1, cache1)\n",
    "        \n",
    "        w1 += lr * dw1\n",
    "        w0 += lr * dw0\n",
    "        b1 += lr * db1\n",
    "        b0 += lr * db0\n",
    "        \n",
    "    if verbose==1:\n",
    "        print('pembelajaran berakhir.')\n",
    "    print('mse=', cur_mse)\n",
    "    return w1, w0, b1, b0, mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWJWleSPCkYJ"
   },
   "source": [
    "---\n",
    "## Sanity Check: Loss Graph\n",
    "\n",
    "Let's check the implementation by training a 2-Layer Neural net with 4 hidden neuraon and learning rate=0.5\n",
    "\n",
    "Those two ***Hyperparameter*** are the most crucial variable that we have to understand its importance\n",
    "dalah nilai yang harus kita pahami kepentingannya.\n",
    "\n",
    "Max Epoch is actually also a value to consider. But in theory, the network should learn better the longer it's trained. Therefore, as long as you have the time, the maximum epoch number is less important. The limit is how long you want to train it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# important hyperparameter\n",
    "nhidden = 4\n",
    "lr = 0.5\n",
    "\n",
    "# not too important, yet still a hyperparameter\n",
    "max_epoch = 500\n",
    "\n",
    "w1, w0, b1, b0, mse = train_two_layer_nn(x, y, nhidden, lr, max_epoch=max_epoch, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezLqQhbVCkYm"
   },
   "source": [
    "plot the MSE after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5paEiUz-CkYr"
   },
   "outputs": [],
   "source": [
    "plt.plot(mse)\n",
    "plt.axis([0, max_epoch, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "twYqASHECkZD"
   },
   "source": [
    "you'll see that the MSE is gradually decreased indicated taht the network IS training.\n",
    "\n",
    "It means we passed the *Sanity Check*.\n",
    "\n",
    "Next we see how the network performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6kFtgGyRCkZG"
   },
   "outputs": [],
   "source": [
    "aktivasi1 = sigmoid_forward(affine_forward(x, w0, b0)[0])\n",
    "aktivasi2 = sigmoid_forward(affine_forward(aktivasi1, w1, b1)[0])\n",
    "print('output =\\n',np.round(aktivasi2))\n",
    "print('\\ntarget =\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYu20BiYCkZO"
   },
   "source": [
    "The network is trained, but turns out that it's still not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5 - Hyperparameter\n",
    "Hyperparameters are values that greatly impact the performance of Neural Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYu20BiYCkZO"
   },
   "source": [
    "## Learning Rate\n",
    "\n",
    "Now let's play with learning rate to improve the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRglLMM-OCDZ"
   },
   "source": [
    "\n",
    "### Learning Rate = 0.9\n",
    "Before we use learning rate=0.5, now let's try to increase it to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OH4NVbzNCkZQ"
   },
   "outputs": [],
   "source": [
    "lr = 0.9\n",
    "nhidden = 4\n",
    "max_epoch = 500\n",
    "\n",
    "w1, w0, b1, b0, mse = train_two_layer_nn(x, y, nhidden, lr, max_epoch=max_epoch)\n",
    "plt.plot(mse)\n",
    "plt.axis([0, max_epoch, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "aktivasi1 = sigmoid_forward(affine_forward(x, w0, b0)[0])\n",
    "aktivasi2 = sigmoid_forward(affine_forward(aktivasi1, w1, b1)[0])\n",
    "print(np.round(aktivasi2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAFwxfWvCkZb"
   },
   "source": [
    "Learning rate indicates how big the update performed to weights to decrease the error. High learning rate can make the error decrease faster. \n",
    "\n",
    "But is it always the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAFwxfWvCkZb"
   },
   "source": [
    "### Learning Rate = 0.1\n",
    "Next let's try learning rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dDiTqg5wCkZo"
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "nhidden = 4\n",
    "max_epoch = 500\n",
    "\n",
    "w1, w0, b1, b0, mse = train_two_layer_nn(x, y, nhidden, lr, max_epoch=max_epoch)\n",
    "plt.plot(mse)\n",
    "plt.axis([0, max_epoch, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "aktivasi1 = sigmoid_forward(affine_forward(x, w0, b0)[0])\n",
    "aktivasi2 = sigmoid_forward(affine_forward(aktivasi1, w1, b1)[0])\n",
    "print(np.round(aktivasi2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the learning rate is so small, the network cannot converge just in 500 epoch. But you'll see that if you increase the epoch to 5000, the network can achieve equal MSE as before. It just need longer time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "nhidden = 4\n",
    "max_epoch = 5000\n",
    "\n",
    "w1, w0, b1, b0, mse = train_two_layer_nn(x, y, nhidden, lr, max_epoch=max_epoch)\n",
    "plt.plot(mse)\n",
    "plt.axis([0, max_epoch, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "aktivasi1 = sigmoid_forward(affine_forward(x, w0, b0)[0])\n",
    "aktivasi2 = sigmoid_forward(affine_forward(aktivasi1, w1, b1)[0])\n",
    "print(np.round(aktivasi2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFHIb9jDCkae"
   },
   "source": [
    "\n",
    "\n",
    "So if bigger learning rate is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFHIb9jDCkae"
   },
   "source": [
    "## Complex Data\n",
    "We'll investigate further the effect of learning rate with a slightly more complex dataset\n",
    "\n",
    "Here we generate 600 data, with 10 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e4KDgWvcCkak"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "COLORS = ['red', 'blue']\n",
    "DIM = 10\n",
    "INFO = 8\n",
    "CLASS = 2\n",
    "NDATA = 600\n",
    "\n",
    "xb, yb1 = make_classification(n_samples=NDATA, n_classes=CLASS, n_features=DIM, n_informative=INFO, \n",
    "                                 n_clusters_per_class=4, flip_y=0.2, random_state=33)\n",
    "yb = yb1.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ah8H0dTCka_"
   },
   "source": [
    "Let's visualize the first 3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CoMYUW0dCkbF"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#  fitur yang ditampilkan\n",
    "ft = [0, 1, 2]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(xb[yb1==0,ft[0]],xb[yb1==0,ft[1]],xb[yb1==0,ft[2]], c=COLORS[0], marker='s')\n",
    "ax.scatter(xb[yb1==1,ft[0]],xb[yb1==1,ft[1]],xb[yb1==1,ft[2]], c=COLORS[1], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "To6G0BO8CkbX"
   },
   "source": [
    "Now to see if higher learning rate is better, let's try and train two layer with learning rate=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Difk9itkCkbb"
   },
   "outputs": [],
   "source": [
    "lr = 0.9\n",
    "nhidden = 4\n",
    "max_epoch = 250\n",
    "\n",
    "w1, w0, b1, b0, mse = train_two_layer_nn(xb, yb, nhidden, lr, max_epoch=max_epoch)\n",
    "plt.plot(mse)\n",
    "plt.axis([0, max_epoch, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "aktivasi1 = sigmoid_forward(affine_forward(xb, w0, b0)[0])\n",
    "aktivasi2 = sigmoid_forward(affine_forward(aktivasi1, w1, b1)[0])\n",
    "output = np.round(aktivasi2)\n",
    "\n",
    "akurasi = np.sum(output==yb)/xb.shape[0]*100\n",
    "print('akurasi =', akurasi , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvhsTx1hCkbv"
   },
   "source": [
    "You can see that the MSE plot is messed up. It was stagnant in early layer, then fluctuate and not was decreasing. This happened because the learning rate is too high.\n",
    "\n",
    "Now, let's try again with learning rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wpfEl5CnCkby",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "nhidden = 4\n",
    "max_epoch = 500\n",
    "\n",
    "w1, w0, b1, b0, mse = train_two_layer_nn(xb, yb, nhidden, lr, max_epoch=max_epoch)\n",
    "plt.plot(mse)\n",
    "plt.axis([0, max_epoch, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "aktivasi1 = sigmoid_forward(affine_forward(xb, w0, b0)[0])\n",
    "aktivasi2 = sigmoid_forward(affine_forward(aktivasi1, w1, b1)[0])\n",
    "output = np.round(aktivasi2)\n",
    "\n",
    "akurasi = np.sum(output==yb)/xb.shape[0]*100\n",
    "print('akurasi =', akurasi , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o3sxKx94CkcB"
   },
   "source": [
    "You can see that the with smaller learning rate, the training looks better, with the consequence that we need to train it much longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o3sxKx94CkcB"
   },
   "source": [
    "## Hidden Neuron\n",
    "Now let's investigate the hidden neuron hyperparameter\n",
    "\n",
    "In theory, more hidden neuron is better, more layer is also better.\n",
    "But too much layer and neuron is wasteful on your resource since its heavier and much longer to train\n",
    "\n",
    "Let's see if we train 20 hidden neuron using learning rate=0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ft_n3uN2CkcD"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "nhidden = 20\n",
    "max_epoch = 500\n",
    "\n",
    "w1, w0, b1, b0, mse = train_two_layer_nn(xb, yb, nhidden, lr, max_epoch=max_epoch)\n",
    "plt.plot(mse)\n",
    "plt.axis([0, max_epoch, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "aktivasi1 = sigmoid_forward(affine_forward(xb, w0, b0)[0])\n",
    "aktivasi2 = sigmoid_forward(affine_forward(aktivasi1, w1, b1)[0])\n",
    "output = np.round(aktivasi2)\n",
    "\n",
    "akurasi = np.sum(output==yb)/xb.shape[0]*100\n",
    "print('akurasi =', akurasi , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIGH2uioCkcY"
   },
   "source": [
    "The main problem with too many hidden neuron or hidden layer is overfitting\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/layer_sizes.jpeg\" style=\"height:300px;\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIGH2uioCkcY"
   },
   "source": [
    "# 6 - Gradient Descent\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6L_a1brONlN"
   },
   "source": [
    "## Full-Batch Gradient Descent\n",
    " \n",
    "What we've seen above is training scheme also called as **Full-Batch Gradient Descent**. Called Full Batch as we use the entire training data every step. One step is a process consists of once forward pass, calculate loss, backward pass, and weight update.\n",
    "\n",
    "The problem with this type of learning is as the data and network grow larger, the memory used will exponentially increased\n",
    "\n",
    "<img src=\"https://image.ibb.co/jSc4ve/gradien.jpg\" alt=\"gradien\"/>\n",
    " \n",
    "Let's see how heavy the example we've used before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ecHFeSepCkca"
   },
   "outputs": [],
   "source": [
    "print('size data train     =', xb.shape,',', xb.nbytes, 'byte')\n",
    "print('size bobot w0       =', w0.shape,' ,', w0.nbytes, 'byte')\n",
    "print('size bobot w1       =', w1.shape,'  ,', w1.nbytes, 'byte')\n",
    "print('size bobot b0       =', b0.shape,'  ,', b0.nbytes, 'byte')\n",
    "print('size bobot b1       =', b1.shape,'   ,', b1.nbytes, 'byte')\n",
    "print()\n",
    "\n",
    "\n",
    "layer1, cache1 = affine_forward(xb, w0, b0)\n",
    "aktivasi1 = sigmoid_forward(layer1)\n",
    "\n",
    "layer2, cache2 = affine_forward(aktivasi1, w1, b1)\n",
    "aktivasi2 = sigmoid_forward(layer2)\n",
    "\n",
    "print('size layer1         =', layer1.shape,',', layer1.nbytes, 'byte')\n",
    "print('size layer2         =', layer2.shape,' ,', layer2.nbytes, 'byte')\n",
    "print()\n",
    "\n",
    "err = yb - aktivasi2\n",
    "g_layer2 = sigmoid_backward(err, aktivasi2)\n",
    "dw1, db1, g_aktivasi1 = affine_backward(g_layer2, cache2)\n",
    "\n",
    "g_layer1 = sigmoid_backward(g_aktivasi1, aktivasi1)\n",
    "dw0, db0, dx = affine_backward(g_layer1, cache1)\n",
    "        \n",
    "print('size err            =', err.shape,' ,', err.nbytes, 'byte')\n",
    "print('size gradien layer1 =', g_aktivasi1.shape,',', g_aktivasi1.nbytes, 'byte')\n",
    "print('size gradien dw0    =', dw0.shape,' ,', dw0.nbytes, 'byte')\n",
    "print('size gradien dw1    =', dw1.shape,'  ,', dw1.nbytes, 'byte')\n",
    "print('size gradien db0    =', db0.shape,'  ,', db0.nbytes, 'byte')\n",
    "print('size gradien db1    =', db1.shape,'   ,', db1.nbytes, 'byte')\n",
    "\n",
    "total = xb.nbytes+w0.nbytes+w1.nbytes+b0.nbytes+b1.nbytes+layer1.nbytes+layer2.nbytes+err.nbytes+g_aktivasi1.nbytes+dw0.nbytes+dw1.nbytes+db0.nbytes+db1.nbytes\n",
    "print('\\n\\ntotal size =',total/1000,'KB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6FT-jXdCkck"
   },
   "source": [
    "As you can see, with data of shape $[600 \\ \\times \\ 10]$, we'll have a matrix sized `48KB` data, and around `1.9KB` weight and bias matrices for two layer (10 hidden neuron, 1 output neuron)\n",
    "\n",
    "Then as we forward propagate the data, we'll create another `layer1` and `layer2` matrix each sized `96KB` and `4.8KB`. Lastly, when we back-propagate the gradient, we created yet another matrik `error`, `g_layer1`, `dw0`, `dw1`, `db0`, and `db1`. \n",
    "\n",
    "Each of those matrices must be created and stored each epoch. Therefore, we'll have about `253.5KB` matrices used and stored in the computer memory (RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xMNAWWqKCkcn"
   },
   "outputs": [],
   "source": [
    "total = xb.nbytes+w0.nbytes+w1.nbytes+b0.nbytes+b1.nbytes+layer1.nbytes+layer2.nbytes+err.nbytes+g_aktivasi1.nbytes+dw0.nbytes+dw1.nbytes+db0.nbytes+db1.nbytes\n",
    "print('total memory 1 epoch =', total/1000, 'KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gt7f8vjHCkc6"
   },
   "source": [
    "Now imagine if we have a training set sized `100 Mega Bytes`, with thousands of features, then we use it to train a 10-layered neural network, each with a thousand hidden neuraon.\n",
    "\n",
    "That means we'll have a total of `80MB` Network, and `100MB` intermediate matrices for each layer (thus times 10 layer) which will be created during forward pass alone. That `1.08GB` matrix will be doubled when we process backward pass. So it will be more than `2 Giga Bytes` RAM used each epoch.\n",
    "\n",
    "For reference, IMAGENET dataset consist of more than 1.4 million images for 1000 classes. Even the small-sized dataset of $[64 \\ \\times \\ 64 \\ \\times \\ 3]$ is `12.7 GB`. The raw dataset is around `1.3TB`\n",
    "\n",
    "![imagenet](https://patrykchrabaszcz.github.io/assets/img/Imagenet32/8x8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWfbUhdoCkc9"
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "The problem with memory space is what lead us to **Stochastic Gradient Descent** or **SGD**. In SGD, each training step only use a single data instead of all of it. Therefore, the memory consumption is greatly minimized.\n",
    "\n",
    "---\n",
    "- **(Batch) Gradient Descent**:\n",
    "\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Forward propagation\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    # Compute cost.\n",
    "    cost = compute_cost(a, Y)\n",
    "    # Backward propagation.\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    # Update parameters.\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "- **Stochastic Gradient Descent**:\n",
    "\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Shuffle data\n",
    "    np.random.shuffle(data)\n",
    "    for j in range(0, m):\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Compute cost\n",
    "        cost = compute_cost(a, Y[:,j])\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "```\n",
    "---\n",
    "Best practice of using SGD is to shuffle the order of data each epoch. This is done so that the Network learn the data, and not the order\n",
    "\n",
    "<img src=\"https://image.ibb.co/msxyMK/shuffle.png\" style=\"height:300px;\">\n",
    "\n",
    "Let's see the implementation of SGD below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jyN_V_cyCkc_"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def train_sgd(x, y, nhidden, lr, max_epoch=500, verbose=0):\n",
    "    \n",
    "    np.random.seed(int(8))\n",
    "    \n",
    "    \n",
    "    if verbose==1:\n",
    "        print('learning starts')\n",
    "        print('x shape =',x.shape)\n",
    "        print('y shape =',y.shape)\n",
    "\n",
    "    n_data = x.shape[0]\n",
    "    nfitur = x.shape[1]\n",
    "    nlabel = y.shape[1]\n",
    "    \n",
    "    w0 = 2*np.random.random((nfitur, nhidden)) -1\n",
    "    w1 = 2*np.random.random((nhidden, nlabel)) -1\n",
    "\n",
    "    b0 = np.zeros((1, nhidden))\n",
    "    b1 = np.zeros((1, nlabel))\n",
    "\n",
    "    mse = []\n",
    "    for ep in range(max_epoch):\n",
    "        \n",
    "        x, y = shuffle(x, y)\n",
    "        \n",
    "        for i in range(n_data):\n",
    "            xs = x[i].reshape(1,-1)\n",
    "            \n",
    "            layer1, cache1 = affine_forward(xs, w0, b0)\n",
    "            aktivasi1 = sigmoid_forward(layer1)\n",
    "\n",
    "            layer2, cache2 = affine_forward(aktivasi1, w1, b1)\n",
    "            aktivasi2 = sigmoid_forward(layer2)\n",
    "\n",
    "            err = y[i] - aktivasi2\n",
    "            cur_mse = np.mean(err ** 2)\n",
    "\n",
    "            g_layer2 = sigmoid_backward(err, aktivasi2)\n",
    "            dw1, db1, g_aktivasi1 = affine_backward(g_layer2, cache2)\n",
    "\n",
    "            g_layer1 = sigmoid_backward(g_aktivasi1, aktivasi1)\n",
    "            dw0, db0, dx = affine_backward(g_layer1, cache1)\n",
    "            \n",
    "            w1 += lr * dw1\n",
    "            w0 += lr * dw0\n",
    "            b1 += lr * db1\n",
    "            b0 += lr * db0\n",
    "        \n",
    "        mse.append(cur_mse)\n",
    "        if verbose==1:\n",
    "            if ep%100==0:\n",
    "                print('epoch=', ep, 'mse=', cur_mse)\n",
    "        \n",
    "    if verbose==1:\n",
    "        print('learning ends.')\n",
    "    print('mse=', cur_mse)\n",
    "    return w1, w0, b1, b0, mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SJtHz-QDCkdQ"
   },
   "source": [
    "Let's try and train the same network using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ySOP6meaCkdh"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "nhidden = 50\n",
    "max_epoch = 500\n",
    "\n",
    "w1, w0, b1, b0, mse = train_sgd(xb, yb, nhidden, lr, max_epoch=max_epoch, verbose=1)\n",
    "plt.plot(mse)\n",
    "plt.axis([0, max_epoch, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "aktivasi1 = sigmoid_forward(affine_forward(xb, w0, b0)[0])\n",
    "aktivasi2 = sigmoid_forward(affine_forward(aktivasi1, w1, b1)[0])\n",
    "output = np.round(aktivasi2)\n",
    "\n",
    "akurasi = np.sum(output==yb)/xb.shape[0]*100\n",
    "print('accuracy =', akurasi , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lpBUazMXCkd0"
   },
   "source": [
    "You can see that the error is noisy as the weights are updated for each data. \n",
    "\n",
    "<img src=\"https://image.ibb.co/em1Q1K/sgd.png\" style=\"height:200px;\">\n",
    "\n",
    "Even so, learning using SGD will eventually decrease and reach equal accuracy but with less memory consumption, altough with longer training time each epoch since it's doing more iterations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lpBUazMXCkd0"
   },
   "source": [
    "## Mini-batch Gradient Descent\n",
    "\n",
    "**Mini-batch Gradient Descent**  is the middle ground between Vanilla GD and SGD. Using Mini batch we don't have to use all data, but also not use just one. Instead we use however many our mamory is capable of storing matrix for one epoch.\n",
    "\n",
    "- **Minibatch Gradient Descent**:\n",
    "\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Shuffle data\n",
    "    np.random.shuffle(data)\n",
    "    for batch in get_batches(X, batch_size=256): # sample 256 examples![image.png](attachment:image.png)\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(batch, parameters)\n",
    "        # Compute cost\n",
    "        cost = compute_cost(a, Y[:,j])\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "```\n",
    "---\n",
    "\n",
    "**Mini-batch SGD** will make the training converge much faster with less memory compared to *Vanilla GD*,yet less noisy compared to *SGD*.\n",
    "\n",
    "<img src=\"https://image.ibb.co/kD468z/minibatch_sgd.png\" style=\"height:200px;\">\n",
    "\n",
    "Similar to SGD, best practice in using Mini batch SGD is to shuffle the order of data each epoch\n",
    "\n",
    "<img src=\"https://image.ibb.co/eTTioz/partition.png\" style=\"height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UCYQ6WJ7Ckd3"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def train_batch_gd(x, y, nhidden, lr, batch=10, max_epoch=500, verbose=0):\n",
    "    \n",
    "    np.random.seed(int(8))\n",
    "    \n",
    "    \n",
    "    if verbose==1:\n",
    "        print('pembelajaran dimulai')\n",
    "        print('ukuran x =',x.shape)\n",
    "        print('ukuran y =',y.shape)\n",
    "\n",
    "    ndata = x.shape[0]\n",
    "    nfitur = x.shape[1]\n",
    "    nlabel = y.shape[1]\n",
    "    nbatch = ndata//batch\n",
    "    \n",
    "    w0 = 2*np.random.random((nfitur, nhidden)) -1\n",
    "    w1 = 2*np.random.random((nhidden, nlabel)) -1\n",
    "\n",
    "    b0 = np.zeros((1, nhidden))\n",
    "    b1 = np.zeros((1, nlabel))\n",
    "\n",
    "\n",
    "    mse = []\n",
    "    for ep in range(max_epoch):\n",
    "        \n",
    "        x, y = shuffle(x, y)\n",
    "        xb = x.reshape((batch,nbatch,nfitur))\n",
    "        yb = y.reshape((batch,-1))\n",
    "        \n",
    "        for i in range(xb.shape[0]):\n",
    "            xs = xb[i]\n",
    "            ys = yb[i].reshape(1,-1).T\n",
    "            \n",
    "            \n",
    "            layer1, cache1 = affine_forward(xs, w0, b0)\n",
    "            aktivasi1 = sigmoid_forward(layer1)\n",
    "\n",
    "            layer2, cache2 = affine_forward(aktivasi1, w1, b1)\n",
    "            aktivasi2 = sigmoid_forward(layer2)\n",
    "\n",
    "            err = ys - aktivasi2\n",
    "            cur_mse = np.mean(err ** 2)\n",
    "\n",
    "            g_layer2 = sigmoid_backward(err, aktivasi2)\n",
    "            dw1, db1, g_aktivasi1 = affine_backward(g_layer2, cache2)\n",
    "\n",
    "            g_layer1 = sigmoid_backward(g_aktivasi1, aktivasi1)\n",
    "            dw0, db0, dx = affine_backward(g_layer1, cache1)\n",
    "            \n",
    "            w1 += lr * dw1\n",
    "            w0 += lr * dw0\n",
    "            b1 += lr * db1\n",
    "            b0 += lr * db0\n",
    "        \n",
    "        mse.append(cur_mse)\n",
    "        if verbose==1:\n",
    "            if ep%100==0:\n",
    "                print('epoch=', ep, 'mse=', cur_mse)\n",
    "        \n",
    "    if verbose==1:\n",
    "        print('pembelajaran berakhir.')\n",
    "    print('mse=', cur_mse)\n",
    "    return w1, w0, b1, b0, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PPMI1fO_Ckd9"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "nhidden = 50\n",
    "max_epoch = 500\n",
    "\n",
    "w1, w0, b1, b0, mse = train_batch_gd(xb, yb, nhidden, lr, batch=10, max_epoch=max_epoch, verbose=1)\n",
    "plt.plot(mse)\n",
    "plt.axis([0, max_epoch, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "aktivasi1 = sigmoid_forward(affine_forward(xb, w0, b0)[0])\n",
    "aktivasi2 = sigmoid_forward(affine_forward(aktivasi1, w1, b1)[0])\n",
    "output = np.round(aktivasi2)\n",
    "\n",
    "akurasi = np.sum(output==yb)/xb.shape[0]*100\n",
    "print('akurasi =', akurasi , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2NfiNaMCkeH"
   },
   "source": [
    "You can see that using **Mini Batch Gradient Descent**, the loss is less noisy compared to using SGD\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q__eGSJ8CkeK"
   },
   "source": [
    "![footer](https://image.ibb.co/hAHDYK/footer2018.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Day 3-1, Hands-on Deep Learning (1).ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
