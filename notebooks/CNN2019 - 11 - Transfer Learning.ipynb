{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNN2019 - 11 - Transfer Learning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"hPwFEN34mNUP","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZnlKAepF9JBf"},"source":["\n","\n","# Task 11 - Transfer Learning\n","\n","\n","In this assignment you will practice in using the available pretrained model in Keras and use it as a Transfer Learning to fine tune your model\n","\n","The goals of this assignment are as follows:\n","\n","    * train and fine tune both full vgg and mini vgg on CIFAR-10\n","      * train from scratch\n","      * fine tune all layers\n","      * train classifier head\n","      * fine tune selected layers\n"]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1","colab_type":"text"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"id":"3iIKvXgjjCn-","colab_type":"text"},"source":["---\n","## 0 - Install TensorFlow 2\n","\n","If Tensorflow 2 is not already installed, install it first"]},{"cell_type":"code","metadata":{"id":"VitP-OLAjCGz","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu -q"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOPPPUZXi5EX","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","tf.__version__"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aIpLk-Iej1RC"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," '2.0.0'"]},{"cell_type":"markdown","metadata":{"id":"DvPSXMEIaFm1","colab_type":"text"},"source":["---\n","## 1 - Import Libraries\n","Import required libraries"]},{"cell_type":"code","metadata":{"id":"4KOPbytzogWG","colab_type":"code","colab":{}},"source":["import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import Model\n","from tensorflow.keras.models import Sequential\n","\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.applications.vgg16 import VGG16\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.utils import plot_model\n","\n","%matplotlib inline\n","np.set_printoptions(precision=7)\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk","colab_type":"code","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv","colab_type":"text"},"source":["---\n","## 2 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw","colab_type":"code","colab":{}},"source":["(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz","colab_type":"text"},"source":["---\n","## 3 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","colab_type":"code","cellView":"both","colab":{}},"source":["X_val_ori = X_train_ori[-1000:,:]\n","y_val     = y_train[-1000:]\n","\n","X_train_ori = X_train_ori[:-1000, :]\n","y_train     = y_train[:-1000]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4","colab_type":"text"},"source":["---\n","## 4 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","colab_type":"code","cellView":"both","colab":{}},"source":["X_train = X_train_ori.astype('float32')\n","X_val   = X_val_ori.astype('float32')\n","X_test  = X_test_ori.astype('float32')\n","\n","mean_image = X_train.mean(axis=(0, 1, 2), keepdims=True)\n","std_image = X_train.std(axis=(0, 1, 2), keepdims=True)\n","\n","X_train = (X_train - mean_image) /std_image\n","X_val = (X_val - mean_image) /std_image\n","X_test = (X_test - mean_image) /std_image\n","\n","X_train = X_train.astype('float32')\n","X_val = X_val.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n","\n","y_train = y_train.ravel()\n","y_val   = y_val.ravel()\n","y_test  = y_test.ravel()\n","\n","print('\\ny_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpPgnChbl8jj","colab_type":"text"},"source":["one hot the label"]},{"cell_type":"code","metadata":{"id":"neQU8KqDIn1_","colab_type":"code","colab":{}},"source":["y_train_hot = to_categorical(y_train, 10)\n","y_val_hot   = to_categorical(y_val, 10)\n","y_test_hot  = to_categorical(y_test, 10)\n","\n","print('y_train_hot.shape =',y_train_hot.shape)\n","print('y_val_hot.shape   =',y_val_hot.shape)\n","print('y_test_hot.shape  =',y_test_hot.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ieyETHY0uUDo","colab_type":"text"},"source":["---\n","## 5 - Define Hyperparameter"]},{"cell_type":"code","metadata":{"id":"LbXrm4NLNRvb","colab_type":"code","colab":{}},"source":["batch_size = 100\n","epochs = 10\n","history = {}\n","scores = {}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pKghE730YdkF"},"source":["---\n","---\n","# [Part 1] Helper Function"]},{"cell_type":"markdown","metadata":{"id":"syCbC0-GkGRi","colab_type":"text"},"source":["---\n","## 1 - Print Model Parameters\n","\n","Function to display list of layer and trainable status. <br> It also shows the total parameters\n","\n","Kind of simplified &nbsp;`model.summary()`"]},{"cell_type":"code","metadata":{"id":"RqncximoQCEK","colab_type":"code","colab":{}},"source":["def print_params(model):\n","  \n","  def count_params(weights):\n","      \"\"\"Count the total number of scalars composing the weights.\n","      # Arguments\n","          weights: An iterable containing the weights on which to compute params\n","      # Returns\n","          The total number of scalars composing the weights\n","      \"\"\"\n","      weight_ids = set()\n","      total = 0\n","      for w in weights:\n","          if id(w) not in weight_ids:\n","              weight_ids.add(id(w))\n","              total += int(K.count_params(w))\n","      return total\n","  \n","  trainable_count = count_params(model.trainable_weights)\n","  non_trainable_count = count_params(model.non_trainable_weights)\n","  \n","  print('id\\ttrainable : layer name')\n","  print('-------------------------------')\n","  for i, layer in enumerate(model.layers):\n","      print(i,'\\t',layer.trainable,'\\t  :',layer.name)\n","  print('-------------------------------')\n","\n","  print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n","  print('Trainable params: {:,}'.format(trainable_count))\n","  print('Non-trainable params: {:,}'.format(non_trainable_count))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TVNtMAr6kLQp","colab_type":"text"},"source":["---\n","## 2 - Model Builder\n","\n","Function to build the vgg model to train in cifar-10\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JPFI8K1vYdkD","colab":{}},"source":["def build_model(name, weights='imagenet', cut_at=-1, unfreeze_from=0, opt='adam'):\n","  \n","  # load model\n","  model = VGG16(weights=weights, include_top=False, input_shape=(32,32,3))\n","  \n","  # freeze all layer\n","  for layer in model.layers:\n","    layer.trainable = False\n","  \n","  # select layer output\n","  if cut_at==-1:\n","    x = model.output\n","  else:\n","    x = model.layers[cut_at].output\n","    \n","  # add new classifier head\n","  x = GlobalAveragePooling2D()(x)\n","  x = Dense(512, activation='relu')(x)\n","  x = Dense(512, activation='relu')(x)\n","  predictions = Dense(10, activation='softmax')(x)\n","\n","  # instantiate new model\n","  myModel = Model(inputs=model.input, outputs=predictions, name=name)\n","    \n","  # unfreeze selected layer\n","  for layer in myModel.layers[unfreeze_from:]:\n","    layer.trainable = True\n","      \n","  # compile model\n","  myModel.compile(\n","      loss='categorical_crossentropy',\n","      optimizer=opt, \n","      metrics=['accuracy']\n","  )\n","      \n","  # print parameters\n","  print_params(myModel)\n","  \n","  return myModel"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKAWDMHpkSjy","colab_type":"text"},"source":["---\n","## 3 - Plot Training History"]},{"cell_type":"code","metadata":{"id":"6qS-rDCAQCjL","colab_type":"code","colab":{}},"source":["def plot_history(history):\n","  plt.rcParams['figure.figsize'] = [12, 4]\n","  plt.subplots_adjust(wspace=0.2)\n","\n","  plt.subplot(121)\n","  # Plot training & validation accuracy values\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('Model accuracy')\n","  plt.ylabel('Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Val'])\n","\n","  plt.subplot(122)\n","  # Plot training & validation loss values\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('Model loss')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Val'])\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yaSBRIjFkWUs","colab_type":"text"},"source":["---\n","## 4 - Plot All Training Histories"]},{"cell_type":"code","metadata":{"id":"NOxwWIBbik99","colab_type":"code","colab":{}},"source":["def plot_all(history, labels=None):\n","  \n","  if labels==None:\n","    hist = history\n","  else:\n","    hist = {k: history[k] for k in labels}\n","  \n","  plt.rcParams['figure.figsize'] = [14, 5]\n","  plt.subplots_adjust(wspace=0.2)\n","  \n","  plt.subplot(121)\n","  for key, h in hist.items():\n","    plt.plot(h.history['accuracy'], label=key)\n","  plt.title('Train accuracy')\n","  plt.ylabel('Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend()\n","\n","  plt.subplot(122)\n","  for key, h in hist.items():\n","    plt.plot(h.history['val_accuracy'], label=key)\n","  plt.title('Validation Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend()\n","  \n","  plt.tight_layout()\n","  plt.show()  \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jf88JCXJARda","colab_type":"text"},"source":["---\n","## 5 - Plot Accuracy Bar"]},{"cell_type":"code","metadata":{"id":"Clp9m96GZ51o","colab_type":"code","colab":{}},"source":["def plot_bar(scores):\n","  \n","  scores_1={k: scores[k] for k in ('full_scratch', 'full_all', 'full_classifier', 'full_10')}\n","  scores_2={k: scores[k] for k in ('mini_scratch', 'mini_all', 'mini_classifier', 'mini_10')}\n","  \n","  plt.rcParams['figure.figsize'] = [8, 6]\n","  acc_1, acc_2 = [], []\n","  labels = ['scratch', 'all', 'classifier', '10']\n","  x = np.arange(len(labels))\n","  width = 0.35\n","  \n","  for key, s in scores_1.items():\n","    acc_1.append(s[1])    \n","    \n","  for key, s in scores_2.items():\n","    acc_2.append(s[1])\n","    \n","  fig, ax = plt.subplots()\n","  plt.bar(x - width/2, acc_1, width, label='full')\n","  plt.bar(x + width/2, acc_2, width, label='mini')\n","  \n","  plt.xticks(x,labels)\n","  plt.title('Test Accuracy')\n","  plt.ylabel('Accuracy')\n","  plt.xlabel('Model')\n","  plt.ylim(0,.9)\n","  plt.legend()\n","  plt.tight_layout()\n","  plt.show()  \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sue0_FVzjW1J","colab_type":"text"},"source":["---\n","---\n","# [Part 2] Train Full VGG\n","\n","\n","As you've tried in previous exercise, now we're going to train Full VGG on CIFAR-10 dataset\n","\n","Full VGG architecture takes all 13 Conv layers then we add a Global Pooling followed by three FC Layer\n","\n","we'll compare the performance if we train\n","\n","    a. the architecture from scratch (random initialization),\n","    b. fine tune the weight from ImageNet pretrain, \n","    c. train the classifier head only, and\n","    d. fine tune from layer 10 to the classifier head"]},{"cell_type":"markdown","metadata":{"id":"YjSX6ry3lOeb","colab_type":"text"},"source":["---\n","## 1 - Train From Scratch\n","\n","Let's define our first model"]},{"cell_type":"code","metadata":{"id":"yKUWSN4ANRyA","colab_type":"code","colab":{}},"source":["model_1S = build_model('full_scratch', weights=None)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsJJq85cxZRh","colab_type":"text"},"source":["Now train the model for 10 epochs with batch size=100"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ImilOOWzZnd1"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train the model"]},{"cell_type":"code","metadata":{"id":"0UlD64_CNRs7","colab_type":"code","colab":{}},"source":["history[model_1S.name] = model_1S.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot), \n","    batch_size=batch_size, \n","    epochs=epochs, \n","    verbose=2\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4sDZ6RCK86h6"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training loss should plateau around 2.3 \n","with training accuracy plateau around 10% \n","in about 1 minute per epoch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7Q1QJmwRVyX2","colab":{}},"source":["plot_history(history[model_1S.name])\n","\n","scores[model_1S.name] = model_1S.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[model_1S.name][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3DAMDKTnxjdA","colab_type":"text"},"source":["### NOTE\n","* You should see that the training got stuck at $10\\%$ accuracy\n","\n","* Possible cause is because the model is too deep, too many parameters to train. \n","\n","* Another possible cause is as is has been explained before, using 5 blocks of VGG yields to $[1\\times1]$ output feature,<br> which is absolutely too little to classify<br> *see &nbsp;`model.summary()`*"]},{"cell_type":"markdown","metadata":{"id":"r6rkURD6koes","colab_type":"text"},"source":["---\n","## 2 - Fine Tune ImageNet Pretrain\n","\n","Now let's try to fine tune the network using weights taken from ImageNet Pretraining"]},{"cell_type":"code","metadata":{"id":"fMXjSnVfZ2Hu","colab_type":"code","colab":{}},"source":["model_1A = build_model('full_all', weights='imagenet')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kGZsFwrZjsSG"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train the model"]},{"cell_type":"code","metadata":{"id":"jW7NyJKFNRqk","colab_type":"code","colab":{}},"source":["history[model_1A.name] = model_1A.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot), \n","    batch_size=batch_size, \n","    epochs=epochs, \n","    verbose=2\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YHvrj7rlFTks"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training loss should start around 1.89 and end around 0.35 \n","with training accuracy start around 25% and end around 88%\n","in about 1 minute per epoch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qmPbuTCSV8Pw","colab":{}},"source":["plot_history(history[model_1A.name])\n","\n","scores[model_1A.name] = model_1A.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[model_1A.name][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HehSUlNxy9pg"},"source":["### NOTE\n","* You should see that the the pretrained weights greatly help the training as the training accuracy jump straight fron $25\\%$ to $80\\%$\n","\n","* This happened because the weights already formed the feature extraction from being trained on ImageNet\n","\n","* And since CIFAR-10 dataset is essentially a subset of ImageNet dataset, the model don't need much to learn the Conv Layers, and just straight to train the classifier head\n","\n","* As we set all layers trainable, during training, the weights in all Conv layers are also updated to fit the current dataset"]},{"cell_type":"markdown","metadata":{"id":"obZ5SIjaks6V","colab_type":"text"},"source":["---\n","## 3 - Train Classifier Head\n","\n","Based on previous explanation, we conclude that the Conv Layers are already trained, so we can just train the classifier head\n","\n","For that, this time we freeze all layers except the three last FC Layers"]},{"cell_type":"code","metadata":{"id":"j-v9FPWPZ2FD","colab_type":"code","colab":{}},"source":["model_1C = build_model('full_classifier', weights='imagenet', unfreeze_from=19)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iU4xnDbsZed8"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train the model"]},{"cell_type":"code","metadata":{"id":"TPlt99cdNRng","colab_type":"code","colab":{}},"source":["history[model_1C.name] = model_1C.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot), \n","    batch_size=batch_size, \n","    epochs=epochs, \n","    verbose=2\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"edquBsWeFfN8"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training loss should start around 1.2 and end around 0.24 \n","with training accuracy start around 60% and end around 91%\n","in about 20 seconds per epoch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1gW6Qa29V8fQ","colab":{}},"source":["plot_history(history[model_1C.name])\n","\n","scores[model_1C.name] = model_1C.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[model_1C.name][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S0lTWjJk1kpX"},"source":["### NOTE\n","* *Oh No, the model overfit*\n","\n","* This may happened because, again, the model is too deep and the output are just $[1\\times1]$\n","\n","* The feature created from the latter conv layers are too detailed for CIFAR-1 dataset, thus the classifier head overfit to the training set"]},{"cell_type":"markdown","metadata":{"id":"TJ95U3pUliye","colab_type":"text"},"source":["---\n","## 4 - Train From Layer 10\n","\n","So now, let's try to fine tune more layers so that the model can fit better to CIFAR-10 dataset\n","\n","In here we set to train layers from $4^{th}$ block up to the classifier head"]},{"cell_type":"code","metadata":{"id":"eckcLMRAZ2CS","colab_type":"code","colab":{}},"source":["model_1N = build_model('full_10', weights='imagenet', unfreeze_from=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M5-c1Hj5ZfHl"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train the model"]},{"cell_type":"code","metadata":{"id":"Ax0EmwuGNRka","colab_type":"code","colab":{}},"source":["history[model_1N.name] = model_1N.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot), \n","    batch_size=batch_size, \n","    epochs=epochs, \n","    verbose=2\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YNMlF8jBaOtj"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training loss should start around 1.4 and end around 0.3 \n","with training accuracy start around 40% and end around 90%\n","in about 35 seconds per epoch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"V8KG38OKV8yQ","colab":{}},"source":["plot_history(history[model_1N.name])\n","\n","scores[model_1N.name] = model_1N.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[model_1N.name][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"loDmXG9k3Tnw"},"source":["### NOTE\n","* *Much better*\n","\n","* By training from much shallower layer, the accuracy has increased\n","\n","* This happen because the earlier conv layers build a lower feature extraction such as dots, colors, and lines \n","\n","* which doesn't need to be changed much to be used in CIFAR-10 dataset"]},{"cell_type":"markdown","metadata":{"id":"Q6c9xzdE4_9S","colab_type":"text"},"source":["---\n","## 5 - Comparison"]},{"cell_type":"code","metadata":{"id":"h4YC67I9v07a","colab_type":"code","colab":{}},"source":["plot_all(history, ['full_scratch', 'full_all', 'full_classifier', 'full_10'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sorkevUb5Jay","colab_type":"text"},"source":["You should see that the validation accuracy is slightly equal between fine tuned model using all layers and just from layer 10"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BkwXEYtIAter"},"source":["---\n","---\n","# [Part 3] Train Mini VGG\n","\n","Now let's try all that again, but using Mini VGG\n","\n","As you've tried to implement in previous exercise, we cut the VGG up to the fourth block, so the output feature activation is not too small, \n","\n","Then add three FC layers for classification head\n","\n","<br>\n","\n","Again, we'll compare the performance if we train\n","\n","    a. the architecture from scratch (random initialization),\n","    b. fine tune the weight from ImageNet pretrain, \n","    c. train the classifier head only, and\n","    d. fine tune from layer 10 to the classifier head"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cxWK3dmJAwz7"},"source":["---\n","## 1 - Train From Scratch"]},{"cell_type":"code","metadata":{"id":"_7abHi8rTXyl","colab_type":"code","colab":{}},"source":["model_2S = build_model('mini_scratch', cut_at=13, weights=None)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fRYnv0D0ZglS"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train the model"]},{"cell_type":"code","metadata":{"id":"Qy8SNkA9NRh_","colab_type":"code","colab":{}},"source":["history[model_2S.name] = model_2S.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot), \n","    batch_size=batch_size, \n","    epochs=epochs, \n","    verbose=2\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kzPXl5xnaeBJ"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training loss should start around 1.9 and end around 0.3 \n","with training accuracy start around 25% and end around 90%\n","in about 40 seconds per epoch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bPAqWHeZV9FC","colab":{}},"source":["plot_history(history[model_2S.name])\n","\n","scores[model_2S.name] = model_2S.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[model_2S.name][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gTCqg1UB6O5m"},"source":["### NOTE\n","* Training mini VGG from scratch performs much better than Full VGG as the output feature is bigger\n","\n","* With more input, the classifier head can classify better\n","\n","* nb: You might get stuck from bad weight initialization, try to re-initialize the model and train it again"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wNw5ra_iluKk"},"source":["---\n","## 2 - Fine Tune ImageNet Pretrain\n","\n","Now let's fine tune using ImageNet Pretraining"]},{"cell_type":"code","metadata":{"id":"4E4RIz1TZ6BS","colab_type":"code","colab":{}},"source":["model_2A = build_model('mini_all', cut_at=13, weights='imagenet')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pS0YQZcSZhvB"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train the model"]},{"cell_type":"code","metadata":{"id":"gx8W0Pk-NRfW","colab_type":"code","colab":{}},"source":["history[model_2A.name] = model_2A.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot), \n","    batch_size=batch_size, \n","    epochs=epochs, \n","    verbose=2\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Z954jq7CaklP"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training loss should start around 1.7 and end around 0.23 \n","with accuracy start around 30% and end around 92%\n","in about 40 seconds per epoch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8oObZ7wjV9Sw","colab":{}},"source":["plot_history(history[model_2A.name])\n","\n","scores[model_2A.name] = model_2A.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[model_2A.name][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SGZZiA2N8aQx"},"source":["### NOTE\n","* Training pretrained mini VGG does not decrease the performence\n","\n","* This validates that the network does not have to be that deep for this small dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QmEsdUo1lvfM"},"source":["---\n","## 3 - Train Classifier Head\n","\n","So let's try to train the classifier head only"]},{"cell_type":"code","metadata":{"id":"AqE7SSv6Z5-B","colab_type":"code","colab":{}},"source":["# fine tune classifier head\n","model_2C = build_model('mini_classifier', cut_at=13, weights='imagenet', unfreeze_from=14)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e8ptmsQuZjNB"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train the model"]},{"cell_type":"code","metadata":{"id":"7DzxGytKVwFQ","colab_type":"code","colab":{}},"source":["history[model_2C.name] = model_2C.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot), \n","    batch_size=batch_size, \n","    epochs=epochs, \n","    verbose=2\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sRU4uGrJat_1"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training loss should start around 0.9 and end around 0.2 \n","with accuracy start around 67% and end around 92%\n","in about 15 seconds per epoch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TG2yMrLJV9oJ","colab":{}},"source":["plot_history(history[model_2C.name])\n","\n","scores[model_2C.name] = model_2C.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[model_2C.name][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EGN2J7Nn_l_i"},"source":["### NOTE\n","* Seems like training the classifier head only still yields to everfitting\n","\n","* Though with shallower model, we get slightly better performance"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yR4iTLcIlwq1"},"source":["---\n","## 4 - Train From Layer 10\n","\n","Now to fine tune the architecture from $4^{th}$ block up to the classifier head "]},{"cell_type":"code","metadata":{"id":"4Ao1fqJfZ57i","colab_type":"code","colab":{}},"source":["model_2N = build_model('mini_10', cut_at=13, weights='imagenet', unfreeze_from=10 )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GcoLu4HAZkSL"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train the model"]},{"cell_type":"code","metadata":{"id":"ZPgjarbGVwCa","colab_type":"code","colab":{}},"source":["history[model_2N.name] = model_2N.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot), \n","    batch_size=batch_size, \n","    epochs=epochs, \n","    verbose=2\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7sOegmEza1Mj"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training loss should start around 1.2 and end around 0.3 \n","with accuracy start around 55% and end around 90%\n","in about 25 seconds per epoch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WD2VO-ZGV90F","colab":{}},"source":["plot_history(history[model_2N.name])\n","\n","scores[model_2N.name] = model_2N.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[model_2N.name][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Dkdk7C1tBGPf"},"source":["### NOTE\n","* Again, training from much shallower layer increase the accuracy and prevent overfitting\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cvRrsjoT-8tu"},"source":["---\n","## 5 - Comparison"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bh1J8Hvs-8uG","colab":{}},"source":["plot_all(history, ['mini_scratch', 'mini_all', 'mini_classifier', 'mini_10'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EMb-yt1X-8uV"},"source":["You should see that the validation accuracy is slightly equal between fine tuned model using all layers and just from layer 10"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ly4LCUQTYdhE"},"source":["---\n","---\n","\n","# [Part 4] Overall Scores\n","\n","Let's show the training, validation, and testing accuracy comprison"]},{"cell_type":"markdown","metadata":{"id":"hEY71JBUALU0","colab_type":"text"},"source":["---\n","## 1 - Train-Val Accuracy"]},{"cell_type":"code","metadata":{"id":"aTfXO8axVh9c","colab_type":"code","colab":{}},"source":["plot_all(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FVGR87--AO6X","colab_type":"text"},"source":["---\n","## 2 - Test Accuracy"]},{"cell_type":"code","metadata":{"id":"6tMFdxMjSQIk","colab_type":"code","colab":{}},"source":["plot_bar(scores)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5JECsUGsN94_","colab_type":"text"},"source":["---\n","---\n","# [Part 5] CIFAR-10 Open-ended Challenge\n","\n","In this section you can experiment with whatever ConvNet architecture you'd like on CIFAR-10.\n","\n","You should experiment with **architectures**, **hyperparameters**, **loss functions**, **regularization**, or anything else you can think of to train a model \n","\n","You should achieve <font color='blue' size='5'><b>at least 90% accuracy</b></font> on the **validation** set <font color='red' size='4'><b>within 10-20 epochs</b></font>. \n"]},{"cell_type":"markdown","metadata":{"id":"qbYFGiXWOPX4","colab_type":"text"},"source":["---\n","## Available Models you can try:\n","- [VGG19](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19)\n","- [InceptionV3](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3)\n","- [ResNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet)\n","- [ResNet v2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2)\n","- [Inception ResNet v2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_resnet_v2)\n","- [Xception](https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception)\n","- [DenseNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/densenet)\n","- [NasNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/nasnet)\n","- [MobileNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet)\n","- [MobileNet v2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2)"]},{"cell_type":"markdown","metadata":{"id":"0eYZkoAQO-S2","colab_type":"text"},"source":["---\n","## Tips for training\n","For each network architecture that you try, you should tune the learning rate and other hyperparameters. \n","\n","When doing this there are a couple important things to keep in mind:\n","\n","- If the parameters are working well, you should see improvement within a few hundred iterations\n","\n","- Use small learning rate to fine tune pretrained model\n","\n","- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n","\n","- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set."]},{"cell_type":"markdown","metadata":{"id":"CM0a9PvRPhqa","colab_type":"text"},"source":["<center>\n","<h2><font color='blue'>--- Go Wild, Have Fun, and Happy Training!  --- </font></h2>"]},{"cell_type":"markdown","metadata":{"id":"AW4Ees_zVLBJ","colab_type":"text"},"source":["---\n","## 1 - Define Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rq_8xfRAQIKO"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Design your Convolutional Neural Network Architecture\n","\n","    "]},{"cell_type":"code","metadata":{"id":"euoKYCFcQNOf","colab_type":"code","colab":{}},"source":["myModel = ??\n","\n","\n","myModel.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jJ9XUD5iVP26"},"source":["---\n","## 2 - Train Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OwRct_FtVP3A"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Compile the model\n","    Train the model\n","    "]},{"cell_type":"code","metadata":{"id":"lQCmkzyXSAeg","colab_type":"code","colab":{}},"source":["# Compile model\n","myModel.compile(??)\n","\n","num_epochs = ??\n","batch_size = ??\n","\n","history = myModel.fit(??)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y8bwtzimVUfs"},"source":["---\n","## 3 - Evaluate Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SOzUKWj5Q_La"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    evaluate your model on test set\n","    "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QZMNJP7hQ-G5","colab":{}},"source":["myModel = load_model(??)\n","\n","train_scores = myModel.evaluate(X_train, y_train_hot, verbose=1)\n","val_scores   = myModel.evaluate(X_val, y_val_hot, verbose=1)\n","test_scores  = myModel.evaluate(X_test, y_test_hot, verbose=1)\n","\n","print(\"\\nTraining Accuracy: %.2f%%\" % (train_scores[1]*100))\n","print(\"Validation Accuracy: %.2f%%\" % (val_scores[1]*100))\n","print(\"Testing Accuracy:    %.2f%%\" % (test_scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Q2Lgw7gSwqD"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","you should get above 90% of accuracy for train, val, and test set"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AzjAfyGhVkPK"},"source":["---\n","## 4 - Test Model on New Image\n","\n","For this part, you have to test your model on new image\n","\n","First of all, search for five images on the Internet, then list the URLs to the code below.\n","\n","The five images must belong to the 10 CIFAR-10 classes that the model recognizes."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pnU03-WEVkPW"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    define five image urls\n","    one image has been given for an example, you can change it"]},{"cell_type":"code","metadata":{"id":"CkFlqQoIF0YD","colab_type":"code","colab":{}},"source":["!wget -q -O 'data_test_0.jpg' 'https://ichef.bbci.co.uk/news/912/cpsprodpb/160B4/production/_103229209_horsea.png'\n","!wget -q -O 'data_test_1.jpg' '??'\n","!wget -q -O 'data_test_2.jpg' '??'\n","!wget -q -O 'data_test_3.jpg' '??'\n","!wget -q -O 'data_test_4.jpg' '??'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j5wH83YDXj0a","colab_type":"text"},"source":["Run and Recognize the images"]},{"cell_type":"code","metadata":{"id":"OBaJnd2gF6X_","colab_type":"code","colab":{}},"source":["import cv2 as cv\n","from PIL import Image\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","\n","for i in range(5):\n","  new_img = Image.open('data_test_'+str(i)+'.jpg')\n","  new_img = np.array(new_img)\n","  new_img2 = cv.resize(new_img, (32,32), interpolation=cv.INTER_AREA)\n","  plt.imshow(new_img2)\n","  plt.axis('off')\n","  plt.show()\n","\n","  new_img2 = (new_img2 - mean_pixel) / std_pixel\n","  pred = myModel.predict(new_img2)\n","  class_id = np.argmax(pred)\n","  print('predicted id   :',class_id)\n","  print('predicted class:', class_names[class_id])\n","  print('--------------------------------\\n\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 11\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bvBk-u7_92ub"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}