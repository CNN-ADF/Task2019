{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN2019 - 15 - Variational Autoencoder.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"d5pbL_VghhTr","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"bqXqlfdqCzgf","colab_type":"text"},"source":["\n","# Task 14 - Variational  AutoEncoder\n","\n","\n","An **Autoencoder** is a neural network which is an unsupervised learning algorithm which uses back propagation to generate output value which is almost close to the input value. It takes input such as image or vector anything with a very high dimensionality and run through the neural network and tries to compress the data into a smaller representation.\n","\n","<p align='center'>\n","<img src='https://www.researchgate.net/profile/Xifeng_Guo/publication/320658590/figure/fig1/AS:614154637418504@1523437284408/The-structure-of-proposed-Convolutional-AutoEncoders-CAE-for-MNIST-In-the-middle-there.png' width=80% />\n","\n","\n","While the basic idea behind a **Variational Autoencoder** is that instead of mapping an input to fixed vector, input is mapped to a distribution. The only difference between the autoencoder and variational autoencoder is that bottleneck vector is replaced with two different vectors one representing the mean of the distribution and the other representing the standard deviation of the distribution.\n","\n","<p align='center'>\n","<img src=\"https://i.ibb.co/tHyP83k/vae.png\" width=80% />\n","\n","In this notebook we will examine the difference between Vanilla (Convolutional) AutoEncoder and Variational AutoEncoder in generating MNIST images"]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk","colab_type":"code","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1","colab_type":"text"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"id":"3iIKvXgjjCn-","colab_type":"text"},"source":["---\n","## 1 - Install TensorFlow 2\n","\n","If Tensorflow 2 is not already installed, install it first"]},{"cell_type":"code","metadata":{"id":"VitP-OLAjCGz","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu -q"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOPPPUZXi5EX","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","tf.__version__"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aIpLk-Iej1RC"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," '2.0.0'"]},{"cell_type":"markdown","metadata":{"id":"WvTLgxWsfaJm","colab_type":"text"},"source":["---\n","## 2 - Import Modul"]},{"cell_type":"code","metadata":{"id":"cP16VLL2cb6o","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Model\n","\n","from tensorflow.keras.layers import Lambda, Input, Dense\n","from tensorflow.keras.layers import Conv2D, Flatten\n","from tensorflow.keras.layers import Reshape, Conv2DTranspose\n","\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.losses import mse, binary_crossentropy\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras import backend as K\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import argparse\n","import imageio\n","import glob\n","\n","np.set_printoptions(precision=4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oh_ZsU26gPiF","colab_type":"text"},"source":["---\n","## 3 - Load MNIST Dataset"]},{"cell_type":"code","metadata":{"id":"obPzmoTKcdbP","colab_type":"code","colab":{}},"source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","image_size = x_train.shape[1]\n","\n","x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n","x_test  = np.reshape(x_test, [-1, image_size, image_size, 1])\n","\n","x_train = x_train.astype('float32') / 255\n","x_test  = x_test.astype('float32') / 255"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J1wtm6NLfdy4","colab_type":"text"},"source":["---\n","## 4 - Helper Functions\n","\n","Below are several helper functions to visualize the generated image"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lsoDxyCxUEKH"},"source":["---\n","### a. Plot Latent Space\n","\n","This function visualize the latent space distribution extracted from `encoder` model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2M-Ut2-ico2r","colab":{}},"source":["def plot_latent_space(encoder_model, data, batch_size=128, vae=False):\n","\n","    x, y = data\n","    if vae:\n","        z, _, _ = encoder_model.predict(x, batch_size=batch_size)\n","    else:\n","        z       = encoder_model.predict(x, batch_size=batch_size)\n","\n","    print('z range: ('+str(np.min(z))+','+str(np.max(z))+')')\n","    \n","    plt.figure(figsize=(12, 10))\n","    plt.scatter(z[:, 0], z[:, 1], c=y)\n","    plt.colorbar()\n","    plt.xlabel(\"z[0]\")\n","    plt.ylabel(\"z[1]\")\n","    plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwuZMDU2UBVw","colab_type":"text"},"source":["---\n","### b. Generate Image\n","\n","This function generate &nbsp;$ n $ &nbsp;images from random latent space &nbsp;$ z $  &nbsp;. \n","\n","The default range of &nbsp;$ z $  &nbsp; is $(-1..1)$"]},{"cell_type":"code","metadata":{"id":"BSxQxNC8fbi6","colab_type":"code","colab":{}},"source":["def generate_image(decoder_model, z_range=(-1,1), n=5):\n","    a,b = z_range\n","    z_sample = np.random.uniform(a, b, (n, 2))\n","    x_decoded = decoder_model.predict(z_sample)\n","\n","    fig, ax = plt.subplots(1,n,figsize=(15,4.5))\n","    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","    for i in range(n):\n","        digit = x_decoded[i].reshape(28, 28)\n","        ax[i].imshow(digit, cmap='gray')\n","        ax[i].set_title(str(z_sample[i]))\n","        ax[i].axis('off')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_Jf6uYK8UJxE"},"source":["---\n","### c. Plot Interpolating Images\n","This function generate &nbsp;$n\\times n$&nbsp; interpolating images generated from latent space &nbsp;$z$&nbsp;\n","\n","The default range of &nbsp;$ z $  &nbsp; is $(-1..1)$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"it9d_KXYco2z","colab":{}},"source":["def plot_interpolating(decoder_model, z_range=(-1,1), n=20, \n","                       save=False, filename='img.png', \n","                       figsize=(10, 10)):\n","\n","    # display a 30x30 2D manifold of digits\n","    a,b = z_range\n","    digit_size = 28\n","    figure = np.zeros((digit_size * n, digit_size * n))\n","\n","    # linearly spaced coordinates corresponding to the 2D plot\n","    # of digit classes in the latent space\n","    grid_x = np.linspace(a, b, n)\n","    grid_y = np.linspace(a, b, n)[::-1]\n","    \n","    for i, yi in enumerate(grid_y):\n","        for j, xi in enumerate(grid_x):\n","            z_sample = np.array([[xi, yi]])\n","            x_decoded = decoder_model.predict(z_sample)\n","            digit = x_decoded[0].reshape(digit_size, digit_size)\n","            figure[i * digit_size: (i + 1) * digit_size,\n","                   j * digit_size: (j + 1) * digit_size] = digit\n","\n","    start_range    = digit_size // 2\n","    end_range      = n * digit_size + start_range + 1\n","    pixel_range    = np.arange(start_range, end_range, digit_size)\n","    sample_range_x = np.round(grid_x, 1)\n","    sample_range_y = np.round(grid_y, 1)\n","\n","    fig = plt.figure(figsize=figsize)\n","    plt.xticks(pixel_range, sample_range_x)\n","    plt.yticks(pixel_range, sample_range_y)\n","    plt.xlabel(\"z[0]\")\n","    plt.ylabel(\"z[1]\")\n","    plt.imshow(figure, cmap='Greys_r')\n","\n","    if save:\n","        plt.savefig(filename)\n","        plt.close(fig)\n","    else:\n","        print('range:(',a,':',b,')')\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nF4-ZTuvVbnK","colab_type":"text"},"source":["---\n","### d. Save Image Callback\n","\n","This class defines a Keras callback to save interpolating image generated each training epoch"]},{"cell_type":"code","metadata":{"id":"_GcWbmKyVfO2","colab_type":"code","colab":{}},"source":["class SaveImage(tf.keras.callbacks.Callback):\n","    def __init__(self, decoder=None, base_dir=None):\n","        super(SaveImage, self).__init__()\n","        self.decoder = decoder\n","        self.base_dir = base_dir\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        filename = self.base_dir+'/image'+str(epoch)+'.png'\n","        plot_interpolating(self.decoder, z_range=(-1,1), n=5,\n","                           save=True, filename=filename, \n","                           figsize=(5,5))\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Uit1y7BUYzK","colab_type":"text"},"source":["---\n","### e. Generate GIF\n","\n","This function generate a GIF animation formt the saved images"]},{"cell_type":"code","metadata":{"id":"jOFuzDwAUxVL","colab_type":"code","colab":{}},"source":["def show_gif(base_dir, anim_file):    \n","    with imageio.get_writer(anim_file, mode='I') as writer:\n","        filenames = glob.glob(base_dir+'/image*.png')\n","        filenames = sorted(filenames)\n","        last = -1\n","        for i,filename in enumerate(filenames):\n","            frame = (i**0.5)\n","            if round(frame) > round(last):\n","                last = frame\n","            else:\n","                continue\n","            image = imageio.imread(filename)\n","            writer.append_data(image)\n","            writer.append_data(image)\n","        image = imageio.imread(filename)\n","        writer.append_data(image)\n","    print('GIF saved as', anim_file)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6lfzFC4hco1n"},"source":["---\n","---\n","# [Part 1] Convolutional AutoEncoder\n","\n","Now let's build our Convolutional AutoEncoder\n","\n","<center>\n","<img src='https://miro.medium.com/max/1578/1*bY_ShNK6lBCQ3D9LYIfwJg@2x.png' width=60%>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gy7jHIYgco16"},"source":["---\n","## 1 - Conv-AE Encoder\n","\n","First, define the Encoder model\n","\n","The architechture is as follow:\n","<pre>\n","    * the input shape is 3-dimensional <font color='blue'><b>(28,28,1)</b></font>  \n","    * <b>conv</b> layer with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * <b>conv</b> layer with <font color='blue'><b>64</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * flatten layer\n","    * <b>dense</b> layer with <font color='blue'><b>16</b></font> neurons using relu activation\n","    * output <b>dense</b> layer with <font color='blue'><b>2</b></font> neuron without activation\n","</pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xc2RyhhXUYMU"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","*  build encoder model\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"daKRTjkRco1-","colab":{}},"source":["input_shape = (image_size, image_size, 1)\n","latent_dim  = 2\n","\n","# create Input() with shape of input_shape\n","# set the name as 'encoder_input'\n","encoder_input = ??\n","\n","# add conv2d layer to encoder_input with 32 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ?? (??)\n","\n","# add conv2d layer to x with 64 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ?? (??)\n","\n","# add flatten layer to x \n","x = ?? (??)\n","\n","# add dense layer to x with 16 neurons and relu activation\n","x = ?? (??)\n","\n","# add output layer to x using dense layer with latent_dim neurons, without activation\n","# set the name as 'encoder_output'\n","encoder_output = ?? (??)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9hYx4sLjVElL"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","* instantiate encoder model\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED\n"]},{"cell_type":"code","metadata":{"id":"2vd2Jci1fiWC","colab_type":"code","colab":{}},"source":["# call Model() function with input encoder_input and encoder_output\n","# set the model name as 'ae_encoder'\n","ae_encoder = ??\n","\n","ae_encoder.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ozGzjnABcZ0u","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"ae_encoder\"\n","encoder_input (InputLayer)   [(None, 28, 28, 1)]       0  \n","conv2d_? (Conv2D)            (None, 14, 14, 32)        320     \n","conv2d_? (Conv2D)            (None, 7, 7, 64)          18496   \n","flatten (Flatten)            (None, 3136)              0       \n","dense (Dense)                (None, 16)                50192   \n","encoder_output (Dense)       (None, 2)                 34      \n","=================================================================\n","Total params: 69,042"]},{"cell_type":"markdown","metadata":{"id":"2TlF8PHkuXYO","colab_type":"text"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"id":"EwJSDv_SD4dH","colab_type":"code","colab":{}},"source":["plot_model(ae_encoder, show_shapes=True, show_layer_names=False, dpi=65, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QBx8ND_Rco2F"},"source":["---\n","## 2 - Conv-AE Decoder\n","\n","Next, we define the Decoder model\n","\n","The architechture is as follow:\n","<pre>\n","    * the input shape is 1-dimensional <font color='blue'><b>(2,)</b></font>  \n","    * <b>dense</b> layer with <font color='blue'><b>7*7*64</b></font> neurons using relu activation\n","    * <b>reshape</b> layer to convert input into <font color='blue'><b>(7,7,64)</b></font> 3-dimensional matrix\n","    * <b>conv2dtranspose</b> layer with <font color='blue'><b>64</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * <b>conv2dtranspose</b> layer with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * output <b>conv2dtranspose</b> layer with <font color='blue'><b>1</b></font> filters of <font color='blue'><b>3x3</b></font>, and <b>sigmoid</b> activation\n","</pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yCbSCYNdYnb6"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","* build decoder model\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dpSyfO_Kco2F","colab":{}},"source":["# create Input() with shape of latent_dim\n","# set the name as 'decoder_input'\n","decoder_input = ??\n","\n","\n","# add dense layer to decoder_input with 7*7*64 neurons and relu activation\n","x = ?? (??)\n","\n","# add reshape layer to x to reshape back into (7, 7, 64)\n","x = ?? (??)\n","\n","# add con2dtranspose to x with 64 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ?? (??)\n","\n","# add con2dtranspose to x with 32 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ?? (??)\n","\n","\n","# add output layer to x using con2dtranspose layer with 1 filter, \n","# kernel size 3, sigmoid activation, and padding same\n","# set the name as 'decoder_output'\n","decoder_output = ?? (??)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7XJSOLVqVHaS"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","* instantiate decoder model\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED\n"]},{"cell_type":"code","metadata":{"id":"BRAanLi7fjOw","colab_type":"code","colab":{}},"source":["# call Model() function with input decoder_input and decoder_output\n","# set the model name as 'ae_decoder'\n","ae_decoder = ??\n","\n","ae_decoder.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z9UqF3aMzeYk"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"ae_decoder\"\n","decoder_input (InputLayer)   [(None, 2)]               0         \n","dense_? (Dense)              (None, 3136)              9408      \n","reshape_? (Reshape)          (None, 7, 7, 64)          0         \n","conv2d_transpose_? (Conv2DTr (None, 14, 14, 64)        36928     \n","conv2d_transpose_? (Conv2DTr (None, 28, 28, 32)        18464     \n","decoder_output (Conv2DTransp (None, 28, 28, 1)         289       \n","=================================================================\n","Total params: 65,089"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XARcYXAhubS6"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"id":"91LRuKsYEEJt","colab_type":"code","colab":{}},"source":["plot_model(ae_decoder, show_shapes=True, show_layer_names=False, dpi=65, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VUe3F9PFco2K"},"source":["---\n","## 3 - Conv-AutoEncoder \n","\n","Lastly, we combine the Encoder and Decoder into single AutoEncoder model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JkwhXVNNY4jI"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","*  instantiate DCAE model = Encoder + Decoder\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q-1fnu-1co2M","colab":{}},"source":["# call ae_encoder() function with input encoder_input\n","encoded  = ??\n","\n","# call ae_decoder() function with input encoded\n","decoder_output = ??\n","\n","# call Model() function with input encoder_input and decoder_output\n","# set the model name as 'conv_autoencoder'\n","autoenc = ??\n","\n","autoenc.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0wcH-Q9b0Zho"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"conv_autoencoder\"\n","encoder_input (InputLayer)   [(None, 28, 28, 1)]       0         \n","ae_encoder (Model)           (None, 2)                 69042     \n","ae_decoder (Model)           (None, 28, 28, 1)         65089     \n","Total params: 134,131"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XZ5UnJBfucFx"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"id":"HwFOid-8EP4T","colab_type":"code","colab":{}},"source":["plot_model(autoenc, show_shapes=True, show_layer_names=False, dpi=55, rankdir='LR', expand_nested=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sjZBkUvQco2P"},"source":["---\n","## 4 - Train Convolutional AutoEncoder\n","\n","The next step is to train the model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gAp5G0z_co2d"},"source":["---\n","### a. Compile Model\n","First let's compile using the model using adam optimizer and binary_crossentropy loss"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zJY6xzTT1KgG"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","compile `autoenc` model using `'adam'` optimizer and `'binary_crossentropy'` loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wXeYWg-4co2f","colab":{}},"source":["??"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i_bkV0BWco2j"},"source":["---\n","### b. Train DC-AE\n","\n","Then train the model for 50 epochs"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_qyMiVleco2k","colab":{}},"source":["batch_size  = 512\n","epochs      = 50\n","\n","base_dir = 'conv_ae'\n","tf.io.gfile.mkdir(base_dir)\n","myCallback = SaveImage(ae_decoder, base_dir)\n","\n","ae_hist = autoenc.fit(x_train, x_train,\n","                      epochs=epochs,\n","                      batch_size=batch_size,\n","                      validation_data=(x_test, x_test),\n","                      callbacks = [myCallback],\n","                      verbose=2\n","                      )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5NlSaTzb-4_g","colab_type":"text"},"source":["---\n","### c. Visualize Loss History"]},{"cell_type":"code","metadata":{"id":"6vc8XkSr-7wx","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [6, 4]\n","plt.plot(ae_hist.history['loss'])\n","plt.plot(ae_hist.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mhYqRboTco2o"},"source":["---\n","## 5 - Visualize Generated Images\n","\n","Now let's visualize the result"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nePogLatco2p"},"source":["---\n","### a. Latent Space Distribution\n","\n","Plots labels and MNIST digits as function of 2-dim latent vector\n","\n","You should see that the distribution of latent vector is clustered by class as AutoEncoder is intended to perform Dimensionality Reduction or Feature Extraction\n","\n","The clustered distribution is good for Classification purposes, but not for Image Generation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IxzT2Gdcco2u","colab":{}},"source":["batch_size=128\n","data_test = (x_test, y_test)\n","plot_latent_space(ae_encoder, data_test, batch_size=batch_size, vae=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FcmxJxT140z_","colab_type":"text"},"source":["---\n","### b. Reconstructed Images\n","\n","Next, let's visualzie the Reconstructed Images from several data test\n","\n","You'll see that using shallow ConvNet and several epochs, we can already perform dimensionality reduction, and reconstruct the reduced data back into its original image"]},{"cell_type":"code","metadata":{"id":"ckivKIeUHtif","colab_type":"code","colab":{}},"source":["ori_img = x_test[:10]\n","rec_img = autoenc.predict(ori_img, verbose=0)\n","\n","fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for i in range(10):\n","    ax[0,i].imshow(ori_img[i].reshape(28, 28), cmap='gray')\n","    ax[0,i].set_title(y_test[i])\n","    ax[0,i].axis('off')\n","    ax[1,i].imshow(rec_img[i].reshape(28, 28), cmap='gray')\n","    ax[1,i].axis('off')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9e6186O46VY","colab_type":"text"},"source":["---\n","### c. Randomly Generated\n","\n","Now let's try to generate &nbsp;$5$&nbsp; new images from a random latent vector &nbsp;$z$&nbsp;\n","\n","You should see that the result is quite good\n","\n","The network is able to generate new image from random input"]},{"cell_type":"code","metadata":{"id":"6ITBoNbjeHUI","colab_type":"code","colab":{}},"source":["generate_image(ae_decoder)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"En0SFKRM3KbA","colab_type":"text"},"source":["But what happened if we try to widen the vector random range to $(-5..5)$?\n","\n","You might see that some images get corrupted because the decoder model is not trained to generate images in that input area"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"izZWRt2E3I93","colab":{}},"source":["generate_image(ae_decoder, z_range=(-5,5))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Pi3rqMf4Lz3","colab_type":"text"},"source":["You can try to generate single image by running the cell below\n","\n","you can use predefined latent vector or generate a random vector"]},{"cell_type":"code","metadata":{"id":"2oIlnYH1ekva","colab_type":"code","colab":{}},"source":["# z_sample = np.random.uniform(-10, 10, (1, 2))\n","z_sample = np.array([[0,0]])\n","\n","x_decoded = ae_decoder.predict(z_sample)\n","digit = x_decoded.reshape(28, 28)\n","plt.imshow(digit, cmap='gray')\n","plt.axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e-jufV7zco2x"},"source":["---\n","### d. Plot Interpolation\n","\n","Now let's generate the image interpolation generated from a range of latent vectors\n","\n","First we generate $10\\times 10$ image ranged from $(-1..1)$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CbqFYuLRco22","colab":{}},"source":["plot_interpolating(ae_decoder, n=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JKRBy8oM4z8D","colab_type":"text"},"source":["Now let's try to widen the range. For that we deepen the interpolation to $20\\times 20$\n","\n","Again, you may notice that in some input ranges, the generated images begin to be unrecognizable"]},{"cell_type":"code","metadata":{"id":"QBj_cDV6hhUo","colab_type":"code","colab":{}},"source":["plot_interpolating( ae_decoder, n=20, z_range=(-10,10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ikYqrmMVUj16","colab_type":"text"},"source":["---\n","### e. Generate GIF\n","\n","Now this just for fun, we combine the saved image generated each epoch while training into a GIF animation\n","\n","open the result in Files tab"]},{"cell_type":"code","metadata":{"id":"MwLP7vdEXNqg","colab_type":"code","colab":{}},"source":["base_dir = 'conv_ae'\n","\n","show_gif(base_dir, 'conv_autoenc.gif')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31qoS0UTgSzz","colab_type":"text"},"source":["---\n","---\n","# [Part 2] Variational AutoEncoder\n","\n","As already mentioned before, the basic idea behind a **Variational Autoencoder** is that instead of mapping an input to fixed vector, input is mapped to a distribution. \n","\n","A variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process.\n","\n","<p align='center'>\n","<img src=\"https://miro.medium.com/max/3374/1*22cSCfmktNIwH5m__u2ffA.png\" width=80% />\n","\n","The only difference between the autoencoder and variational autoencoder is that bottleneck vector is replaced with two different vectors one representing the mean of the distribution and the other representing the standard deviation of the distribution.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_AfoEeoF1ta7","colab_type":"text"},"source":["---\n","## 0 - Random Sampling Function\n","\n","Function below is a helper function to generate a random latent vector &nbsp;$z$&nbsp; from input &nbsp;$mean$&nbsp; and &nbsp;$variance$"]},{"cell_type":"code","metadata":{"id":"Y0oNxGlGfvff","colab_type":"code","colab":{}},"source":["def sampling(args):\n","    \n","  z_mean, z_log_var = args\n","    \n","  batch = K.shape(z_mean)[0]\n","  dim = K.int_shape(z_mean)[1]\n","\n","  # by default, random_normal has mean=0 and std=1.0\n","  epsilon = K.random_normal(shape=(batch, dim))\n","  \n","  return z_mean + K.exp(0.5 * z_log_var) * epsilon"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FX5dojpagd5K","colab_type":"text"},"source":["---\n","## 1 - VAE Encoder\n","\n","This defines the generative model which takes a latent encoding as input, and outputs the parameters for a conditional distribution of the observation, i.e. $p(x|z)$. Additionally, we use a unit Gaussian prior $p(z)$ for the latent variable.\n","\n","<center>\n","<img src='https://miro.medium.com/max/1608/1*XYyWimolMhPDMg8qCNlcwg@2x.png' width=60%>\n","</center>\n","\n","Now to define the Encoder model\n","\n","The architechture is as follow:\n","<pre>\n","    * the input shape is 3-dimensional <font color='blue'><b>(28,28,1)</b></font>  \n","    * <b>conv</b> layer with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * <b>conv</b> layer with <font color='blue'><b>64</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * flatten layer\n","    * <b>dense</b> layer with <font color='blue'><b>16</b></font> neurons using relu activation\n","\n","    * output 1: <b>dense</b> layer with <font color='blue'><b>2</b></font> neuron without activation\n","    * output 2: <b>dense</b> layer with <font color='blue'><b>2</b></font> neuron without activation\n","</pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iiCNoyWmDuvW"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","*  build encoder model\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED"]},{"cell_type":"code","metadata":{"id":"AvtgKOdVgpXj","colab_type":"code","colab":{}},"source":["input_shape = (image_size, image_size, 1)\n","latent_dim  = 2\n","\n","# create Input() with shape of input_shape\n","# set the name as 'v_encoder_input'\n","v_encoder_input = ??\n","\n","# add conv2d layer to v_encoder_input with 32 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ?? (??)\n","\n","# add conv2d layer to x with 64 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ?? (??)\n","\n","# add flatten layer to x \n","x = ?? (??)\n","\n","# add dense layer to x with 16 neurons and relu activation\n","x = ?? (??)\n","\n","\n","# generate latent vector Q(z|X)\n","# add first output layer to x \n","# using dense layer with latent_dim neurons, without activation\n","# set the name as 'z_mean'\n","z_mean    = ?? (??)\n","\n","# add first output layer to x \n","# using dense layer with latent_dim neurons, without activation\n","# set the name as 'z_log_var'\n","z_log_var = ?? (??)\n","\n","# generate random sampling from z_mean and z_log_var \n","# as input for decoder model\n","z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n","\n","# combine all outputs into a single list\n","v_encoder_output = [z_mean, z_log_var, z]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y3USEi8gFXEh"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","* instantiate encoder model\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tYZuDR-zi9VJ","colab":{}},"source":["# call Model() function with input v_encoder_input and v_encoder_output\n","# set the model name as 'vae_encoder'\n","vae_encoder = ??\n","\n","vae_encoder.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ThZdA_eAFXE-"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"vae_encoder\"\n","v_encoder_input (InputLayer)    [(None, 28, 28, 1)]  0                                            \n","conv2d_? (Conv2D)               (None, 14, 14, 32)   320         v_encoder_input[0][0]            \n","conv2d_? (Conv2D)               (None, 7, 7, 64)     18496       conv2d_2[0][0]                   \n","flatten_? (Flatten)             (None, 3136)         0           conv2d_3[0][0]                   \n","dense_? (Dense)                 (None, 16)           50192       flatten_1[0][0]                  \n","z_mean (Dense)                  (None, 2)            34          dense_2[0][0]                    \n","z_log_var (Dense)               (None, 2)            34          dense_2[0][0]                    \n","z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n","                                                                 z_log_var[0][0]                  \n","Total params: 69,076"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5Lp3MxS9FXFB"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n8XGLD7Xi9VT","colab":{}},"source":["plot_model(vae_encoder, show_shapes=True, show_layer_names=False, dpi=65, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sfxCBy1UggRD","colab_type":"text"},"source":["---\n","## 2 - VAE Decoder\n","\n","Then we define the Decoder model exactly the same as the previous Vanilla Decoder\n","\n","The architechture is as follow:\n","<pre>\n","    * the input shape is 1-dimensional <font color='blue'><b>(2,)</b></font>  \n","    * <b>dense</b> layer with <font color='blue'><b>7*7*64</b></font> neurons using relu activation\n","    * <b>reshape</b> layer to convert input into <font color='blue'><b>(7,7,64)</b></font> 3-dimensional matrix\n","    * <b>conv2dtranspose</b> layer with <font color='blue'><b>64</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * <b>conv2dtranspose</b> layer with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * output <b>conv2dtranspose</b> layer with <font color='blue'><b>1</b></font> filters of <font color='blue'><b>3x3</b></font>, and <b>sigmoid</b> activation\n","</pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zCV36-EWFmiS"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","* build decoder model\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zxzgvFO1FmiX","colab":{}},"source":["# create Input() with shape of latent_dim\n","# set the name as 'v_decoder_input'\n","v_decoder_input = ??\n","\n","\n","# add dense layer to v_decoder_input with 7*7*64 neurons and relu activation\n","x = ?? (??)\n","\n","# add reshape layer to x to reshape back into (7, 7, 64)\n","x = ?? (??)\n","\n","# add con2dtranspose to x with 64 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ?? (??)\n","\n","# add con2dtranspose to x with 32 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ?? (??)\n","\n","\n","# add output layer to x using con2dtranspose layer with 1 filter, \n","# kernel size 3, sigmoid activation, and padding same\n","# set the name as 'v_decoder_output'\n","v_decoder_output = ?? (??)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s9mvtctgGUx-"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","* instantiate decoder model\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED\n"]},{"cell_type":"code","metadata":{"id":"dNRCC2SkjLzI","colab_type":"code","colab":{}},"source":["# call Model() function with input v_decoder_input and v_decoder_output\n","# set the model name as 'vae_decoder'\n","vae_decoder = ??\n","\n","vae_decoder.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Qc1w8goMGUyO"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"vae_decoder\"\n","v_decoder_input (InputLayer) [(None, 2)]               0         \n","dense_3 (Dense)              (None, 3136)              9408      \n","reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n","conv2d_transpose_2 (Conv2DTr (None, 14, 14, 64)        36928     \n","conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        18464     \n","v_decoder_output (Conv2DTran (None, 28, 28, 1)         289       \n","Total params: 65,089"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y_Sxl1iVGUyR"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"id":"Z07BVJtzj0l4","colab_type":"code","colab":{}},"source":["plot_model(vae_decoder, show_shapes=True, show_layer_names=False, dpi=65, rankdir='LR')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yzs6R-Emj8wx"},"source":["---\n","## 3 - Variational-AutoEncoder \n","\n","Now instantiate VAE model = VEncoder + VDecoder"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z9TtOcT2LI4l"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","*  instantiate VAE model = V Encoder + V Decoder\n","*  use Functional API\n","*  DO NOT FORGET TO NAME THE LAYER AS INSTRUCTED"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LhuNwFTFj8w8","colab":{}},"source":["# call vae_encoder() function with input v_encoder_input\n","v_encoded  = ??\n","\n","# there are three element in output list from vae_encoder: z_mean, z_log_var, and z itself\n","# we want to get the latent space z, so retrieve the third element of the list\n","v_encoded  = ??\n","\n","# call vae_decoder() function with input v_encoded\n","v_decoder_output = ??\n","\n","# call Model() function with input v_encoder_input and v_decoder_output\n","# set the model name as 'variational_autoencoder'\n","vae = ??\n","\n","vae.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F67JN--mK1bq"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"variational_autoencoder\"\n","v_encoder_input (InputLayer) [(None, 28, 28, 1)]       0         \n","vae_encoder (Model)          [(None, 2), (None, 2), (N 69076     \n","vae_decoder (Model)          (None, 28, 28, 1)         65089     \n","Total params: 134,165"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"69RLp3cBLYXZ"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ta0pIQacj8xD","colab":{}},"source":["plot_model(vae, show_shapes=True, show_layer_names=False, dpi=55, rankdir='LR', expand_nested=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6o0NpsCLil7Y","colab_type":"text"},"source":["---\n","## 4 - Train Variational AutoEncoder\n","\n","Next to train the Variational AutoEncoder"]},{"cell_type":"markdown","metadata":{"id":"X3Phn362isEL","colab_type":"text"},"source":["---\n","### a. Define Loss\n","\n","\n","<center>\n","<img src='https://miro.medium.com/max/2080/1*L1klrEWx4Y5Bz6qMqTbwAQ@2x.png' width=60%>\n","</center>\n","\n","VAEs train by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood:\n","\n","$$\\log p(x) \\ge \\text{ELBO} = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x, z)}{q(z|x)}\\right].$$\n","\n","then we calculate the KL term to optimize it, thus we have\n","$$\n","D_{KL} = \\frac{1}{2}\\sum_k\\Big(\\exp(\\Sigma(X))+\\mu^2(X)-1-\\Sigma(X)\\Big)\n","$$\n","\n","Now to calculate the total loss is defined by\n","\n","    VAE loss = mse_loss + kl_loss\n"]},{"cell_type":"code","metadata":{"id":"KDZoXcRuit0s","colab_type":"code","colab":{}},"source":["from tensorflow.keras.losses import mse\n","\n","# calculate construction loss\n","# --> mse loss\n","reconstruction_loss = mse(K.flatten(v_encoder_input), K.flatten(v_decoder_output))\n","reconstruction_loss *= 28 * 28\n","\n","# calculate KL Loss\n","kl_loss = K.exp(z_log_var) + K.square(z_mean) - 1 - z_log_var\n","kl_loss = K.sum(kl_loss, axis=-1)\n","kl_loss *= 0.5\n","\n","# calculate VAE Loss\n","vae_loss = K.mean(reconstruction_loss + kl_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wsuuu0LRiyDM","colab_type":"text"},"source":["---\n","### b. Compile Model\n","Add loss to model and compile it"]},{"cell_type":"code","metadata":{"id":"aZrOdUFncQB3","colab_type":"code","colab":{}},"source":["vae.add_loss(vae_loss)\n","\n","vae.compile(optimizer='adam')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLRDRYxPi4RO","colab_type":"text"},"source":["---\n","### c. Train DC-VAE\n","Then train the model for 50 epochs"]},{"cell_type":"code","metadata":{"id":"LT450MTpi63X","colab_type":"code","colab":{}},"source":["batch_size  = 512\n","epochs      = 50\n","\n","base_dir = 'var_ae'\n","tf.io.gfile.mkdir(base_dir)\n","myCallback = SaveImage(vae_decoder, base_dir)\n","\n","vae_hist = vae.fit(x_train, \n","                   epochs=epochs,\n","                   batch_size=batch_size,\n","                   validation_data=(x_test, None),\n","                   callbacks = [myCallback],\n","                   verbose=2\n","                   )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y2wghfCCA8Z9"},"source":["---\n","### d. Visualize Loss History"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dlNd9WcHA8aE","colab":{}},"source":["plt.rcParams['figure.figsize'] = [6, 4]\n","plt.plot(vae_hist.history['loss'])\n","plt.plot(vae_hist.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jKoQgW1-NvqA"},"source":["---\n","## 5 - Visualize Generated Images\n","\n","Now let's visualize the result"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i0rE6yjRNvqJ"},"source":["---\n","### a. Latent Space Distribution\n","\n","Plots labels and MNIST digits as function of 2-dim latent vector\n","\n","You should see that now the latent vector is more smoothly distributed across the space"]},{"cell_type":"code","metadata":{"id":"YZXBcOt2vfza","colab_type":"code","colab":{}},"source":["batch_size=128\n","data_test = (x_test, y_test)\n","\n","plot_latent_space(vae_encoder, data_test, batch_size=batch_size, vae=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QeQQV23YvoUR"},"source":["---\n","### b. Randomly Generated\n","\n","Now we skip directly to generate &nbsp;$5$&nbsp; new images from a random latent vector &nbsp;$z$&nbsp;\n","\n","You should see that the result is better\n","\n","The network is able to generate new image from random input"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y4BvOfShvoUT","colab":{}},"source":["generate_image(vae_decoder)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rhqp1oMqOJeB"},"source":["But what happened if we try to widen the vector random range to $(-5..5)$?\n","\n","You might see that the image is much better that using Vanilla AutoEncoder, even though the network is not trained to generate image from that range"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FRME7oCVOPeL","colab":{}},"source":["generate_image(vae_decoder, z_range=(-5,5))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lQSmciuVONJN"},"source":["You can try to generate single image by running the cell below\n","\n","you can use predefined latent vector or generate a random vector"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"462RacFDvoUX","colab":{}},"source":["z_sample = np.array([[0,0]])\n","x_decoded = vae_decoder.predict(z_sample)\n","digit = x_decoded.reshape(28, 28)\n","plt.imshow(digit, cmap='gray')\n","plt.axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9PnCFSNAOkGg"},"source":["---\n","### c. Plot Interpolation\n","\n","Next let's generate the image interpolation generated from a range of latent vectors\n","\n","First we generate $10\\times 10$ image ranged from $(-1..1)$\n","\n","You may see that the interpolation is done more smoothly"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MsIx4Zh4voUc","colab":{}},"source":["plot_interpolating(vae_decoder, n=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"h32dOssyOofg"},"source":["Now let's try to widen the range. For that we deepen the interpolation to $20\\times 20$\n","\n","You might see that the interpolation still results smooth images across wide range"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BfoD4trSvoUg","colab":{}},"source":["plot_interpolating( vae_decoder, n=20, z_range=(-5,5))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hB-U6_GYO0qR"},"source":["---\n","### d. Generate GIF\n","\n","Now this just for fun, we combine the saved image generated each epoch while training into a GIF animation\n","\n","open the result in Files tab"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"riB6_ZtvvoUm","colab":{}},"source":["base_dir = 'var_ae'\n","\n","show_gif(base_dir, 'var_autoenc.gif')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuujlTQYhhVW","colab_type":"text"},"source":["---\n","---\n","\n","# Congratulation, You've Completed Exercise 15\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"Mc76YLAkhhVW","colab_type":"text"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}