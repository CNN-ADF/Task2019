{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"CNN2019 - 06 - Advanced Optimization.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dLOFde7z7uv9","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","\n","# Task 6 - Advanced Optimization\n","\n","\n","In this assignment you will practice using Dropout layer as well various Parameter Update Schemes. The goals of this assignment are as follows:\n","* understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)\n","* implement Dropout Layer\n","* implement popular Parameter Update Schemes:\n","\n","      * Vanilla Stochastic Gradient Descent\n","      * SGD with Momentum Update\n","      * Nesterov Accelerated Gradient Update (NAG)\n","      * Adaptive Sub-Gradient Update (AdaGrad)\n","      * Root Mean Square Propagation Update (RMSProp)\n","      * Adaptive Moment Estimation Update (Adam)\n"]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1","colab_type":"text"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"id":"DvPSXMEIaFm1","colab_type":"text"},"source":["---\n","## 1 - Import Libraries\n","Import required libraries"]},{"cell_type":"code","metadata":{"id":"hsZYqgngcZzY","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tabulate import tabulate\n","import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk","colab_type":"code","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = 123456\n","Nama = \"\"\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4oLZzTjVcZ24","colab_type":"text"},"source":["---\n","## 2 - Layer API\n","\n","This part is exacly the same as Task 3\n","for that, we've already provide you the implementation of the basic layers"]},{"cell_type":"markdown","metadata":{"id":"tsiFt6eTokew","colab_type":"text"},"source":["---\n","### a. Affine Functions"]},{"cell_type":"code","metadata":{"id":"s214hq3vcZ24","colab_type":"code","colab":{}},"source":["def affine_forward(x, W, b ):\n","    \n","    N = x.shape[0]\n","    a1 = x.reshape(N, -1)\n","    v = np.dot(a1, W) + b    \n","    cache = (x, W, b)\n","    \n","    return v, cache\n","  \n","# --------------------------------\n","\n","def affine_backward(dout, cache):\n","    \n","    x, W, b = cache\n","    N = x.shape[0]\n","    dW = np.dot(x.reshape(N, -1).T,dout)\n","    db = np.sum(dout, axis=0, keepdims=True)\n","    dx = dout.dot(W.T).reshape(x.shape)\n","    \n","    return dx, dW, db\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"voiXjsH-onm_","colab_type":"text"},"source":["---\n","### b. ReLU Functions"]},{"cell_type":"code","metadata":{"id":"0bqdPxmWohxD","colab_type":"code","colab":{}},"source":["def relu_forward(x):\n","    out = x * (x > 0).astype(float)\n","    cache = x\n","    return out, cache\n","\n","# --------------------------------\n","\n","def relu_backward(dout, cache):    \n","    dx = dout * (cache >= 0)\n","    \n","    return dx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aA04_S3noppa","colab_type":"text"},"source":["---\n","### c. Softmax Functions"]},{"cell_type":"code","metadata":{"id":"TPNfcEGpojf-","colab_type":"code","colab":{}},"source":["def softmax(x):  \n","    x -= np.max(x)\n","    x_exp = np.exp(x)\n","    x_sum = np.sum(x_exp, axis = 1, keepdims = True)  \n","    score = x_exp / x_sum\n","    \n","    return score\n","\n","# --------------------------------\n","\n","def softmax_loss(score, y):\n","   \n","    num_examples = score.shape[0]\n","    number_list = range(num_examples)\n","    corect_logprobs = -np.log(score[number_list,y])\n","    loss = np.sum(corect_logprobs)/num_examples\n","    \n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    \n","    return loss, dscores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQWFba9V7uwl","colab_type":"text"},"source":["---\n","### d. Affine-Relu Block"]},{"cell_type":"code","metadata":{"id":"HqVkDt1R7uwm","colab_type":"code","colab":{}},"source":["def affine_relu_forward(X, W, b):\n","    \n","    act, fc_cache = affine_forward(X, W, b)\n","    out, relu_cache = relu_forward(act)\n","    \n","    cache = (fc_cache, relu_cache)\n","    return out, cache\n","\n","# --------------------------------\n","\n","def affine_relu_backward(dout, cache):\n","    \n","    fc_cache, relu_cache = cache\n","    dact = relu_backward(dout, relu_cache)\n","    dX, dW, db = affine_backward(dact, fc_cache)\n","    \n","    return dX, dW, db\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv","colab_type":"text"},"source":["---\n","## 3 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw","colab_type":"code","colab":{}},"source":["(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz","colab_type":"text"},"source":["---\n","## 4 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","colab_type":"code","colab":{}},"source":["X_val_ori = X_train_ori[-1000:,:]\n","y_val     = y_train[-1000:]\n","\n","X_train_ori = X_train_ori[:-1000, :]\n","y_train     = y_train[:-1000]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4","colab_type":"text"},"source":["---\n","## 5 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","colab_type":"code","colab":{}},"source":["X_train = X_train_ori.astype('float32')\n","X_val   = X_val_ori.astype('float32')\n","X_test  = X_test_ori.astype('float32')\n","\n","mean_image = np.mean(X_train, axis = 0)\n","X_train -= mean_image\n","X_val   -= mean_image\n","X_test  -= mean_image\n","\n","X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]*X_train.shape[3]))\n","X_val = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]*X_val.shape[3]))\n","X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]*X_test.shape[3]))\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n","\n","y_train = y_train.ravel()\n","y_val   = y_val.ravel()\n","y_test  = y_test.ravel()\n","\n","print('\\ny_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KjoKQI0B7uwq","colab_type":"text"},"source":["---\n","---\n","# [Part 1] Dropout Layer\n","\n","Dropout [1] is a technique for regularizing neural networks by **randomly** setting some features to **zero** during the forward pass of training. \n","\n","<center>\n","<img src=\"https://miro.medium.com/proxy/1*iWQzxhVlvadk6VAJjsgXgg.png\" width=500>\n"," </center>\n","\n","In this exercise you will implement a dropout layer and modify your fully-connected network to optionally use dropout.\n","\n","[1] [Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012](https://arxiv.org/abs/1207.0580)"]},{"cell_type":"markdown","metadata":{"id":"tKcMwLhZ8cHa","colab_type":"text"},"source":["---\n","## 0 - Mask Review\n","To implement Dropout, we'll make a mask matrix to filter any number (activation) below the probability value. You've implement something similar to this before in MaxPool backward, but let's review that"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"o_P0vndV8-HY"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement Filter example for dropout**\n","\n","Create a mask to filter any number below 5 from the given matrix\n"]},{"cell_type":"code","metadata":{"id":"o3sgQxYY9G01","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","x = np.random.randint(0,10,(5, 5))\n","\n","#Create a mask to filter any number below 5 from the given matrix\n","mask = ??\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dqUFB-0C9SsX"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VCBF8X8t9SsZ","colab":{}},"source":["print('x')\n","print(x)\n","print('\\nmask')\n","print(mask)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i8w8cosA9X-y"},"source":["**Expected Output**:\n","<pre>\n","x\n","[[7 0 4 9 6]\n"," [2 7 4 1 2]\n"," [2 2 1 3 1]\n"," [1 1 9 5 2]\n"," [3 6 3 0 2]]\n","\n","mask\n","[[False  True  True False False]\n"," [ True False  True  True  True]\n"," [ True  True  True  True  True]\n"," [ True  True False False  True]\n"," [ True False  True  True  True]]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SuR4Sin27uwr","colab_type":"text"},"source":["---\n","## 1 - Forward Dropout\n","\n","First we implement the Forward Function. \n","\n","* In here, you should implement the <font color='red'>**INVERTED DROPOUT**</font>, not the vanilla version of dropout.\n","\n","* Keep in mind that **prob** is the probability to <font color='blue'>**KEEP A NEURON OUTPUT**</font><br>This might be contrary to some sources, where it is referred to\n","    as the probability of dropping a neuron output.\n","\n","* Since dropout behaves differently during training and testing, make sure to implement the operation for both modes."]},{"cell_type":"markdown","metadata":{"id":"djb_j_PSoGHD","colab_type":"text"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement the forward pass for dropout**\n","\n"]},{"cell_type":"code","metadata":{"id":"7yl_UVMk7uws","colab_type":"code","colab":{}},"source":["def dropout_forward(x, dropout_param):\n","    \"\"\"\n","    Performs the forward pass for (INVERTED) dropout.\n","\n","    Inputs:\n","    - x             : Input data, of any shape\n","    - dropout_param : A dictionary with the following keys:\n","      - prob  : Dropout parameter. We keep each neuron output with probability p.\n","      - mode  : 'test' or 'train'. If the mode is train, then perform dropout;\n","                if the mode is test, then just return the input.\n","      - seed  : Seed for the random number generator. Passing seed makes this\n","                function deterministic, which is needed for gradient checking \n","                but not in real networks.\n","\n","    Outputs:\n","    - out     : Array of the same shape as x.\n","    - cache   : tuple (dropout_param, mask). In training mode, \n","                mask is the dropout mask that was used to multiply the input; \n","                in test mode, mask is None.\n","\n","    \"\"\"\n","    prob, mode = dropout_param['prob'], dropout_param['mode']\n","    if 'seed' in dropout_param:\n","        np.random.seed(dropout_param['seed'])\n","\n","    mask = None\n","    out = None\n","\n","    if mode == 'train':\n","      \n","        # create mask to drop, call np.random.rand in the same size of x \n","        rand = ??\n","        \n","        # then filter those with value below prob\n","        mask = ??\n","\n","        # drop neuron by multiplying x with mask\n","        x = ?? \n","        \n","        # return the drop result by first dividing x with prob (inverted schemes)\n","        out = ??\n","        \n","  \n","        \n","    elif mode == 'test':        \n","        \n","        # for test time, just return x as it is\n","        out = x\n","\n","    # store the mask and parameter for backward pass\n","    cache = (dropout_param, mask)\n","    out = out.astype(x.dtype, copy=False)\n","\n","    return out, cache\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4rDoDJnqv51","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"GFTRPfzs7uwx","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","x = np.random.randn(500, 500) + 10\n","\n","for pr in [0.25, 0.4, 0.7]:\n","    out, _ = dropout_forward(x, {'mode': 'train', 'prob': pr})\n","    out_test, _ = dropout_forward(x, {'mode': 'test', 'prob': pr})\n","\n","    print('Running tests with dropout probability of ', pr)\n","    print('Mean of input x                           :', x.mean())\n","    print('Mean of train-time output                 :', out.mean())\n","    print('Mean of test-time  output                 :', out_test.mean())\n","    print('Fraction of train-time output set to zero :', (out == 0).mean())\n","    print('Fraction of test-time  output set to zero :', (out_test == 0).mean())\n","    print('\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yiSRs_Dc7uw0","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","Running tests with dropout probability of  0.25\n","Mean of input x                           : 2.5035147792443206\n","Mean of train-time output                 : 10.014059116977283\n","Mean of test-time  output                 : 2.5035147792443206\n","Fraction of train-time output set to zero : 0.749784\n","Fraction of test-time  output set to zero : 0.749784\n","\n","\n","Running tests with dropout probability of  0.4\n","Mean of input x                           : 1.0003021979972544\n","Mean of train-time output                 : 2.5007554949931357\n","Mean of test-time  output                 : 1.0003021979972544\n","Fraction of train-time output set to zero : 0.900004\n","Fraction of test-time  output set to zero : 0.900004\n","\n","\n","Running tests with dropout probability of  0.7\n","Mean of input x                           : 0.7022154229440579\n","Mean of train-time output                 : 1.0031648899200825\n","Mean of test-time  output                 : 0.7022154229440579\n","Fraction of train-time output set to zero : 0.929764\n","Fraction of test-time  output set to zero : 0.929764\n"]},{"cell_type":"markdown","metadata":{"id":"DXhrq8nW7uw1","colab_type":"text"},"source":["---\n","## 2 - Backward Dropout\n","\n","Perform the backward pass for the <font color='red'>**INVERTED DROPOUT**</font>."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v50jnBVDsLFQ"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement the backward pass for dropout**\n","\n"]},{"cell_type":"code","metadata":{"id":"cnvpOyOT7uw1","colab_type":"code","colab":{}},"source":["def dropout_backward(dout, cache):\n","    \"\"\"\n","    Perform the backward pass for (inverted) dropout.\n","    \n","    Inputs:\n","    - dout: Upstream derivatives, of any shape\n","    - cache: (dropout_param, mask) from dropout_forward.\n","    \"\"\"\n","    \n","    # retrieve the parameter and mask filter\n","    dropout_param, mask = cache\n","    mode = dropout_param['mode']\n","\n","    \n","    if mode == 'train':\n","        \n","        # propagate the gradient only to neuron that is not dropped\n","        # drop the gradient by multiplying dout with mask\n","        dx = ??\n","        \n","        # retrieve the probability parameter\n","        prob = dropout_param['prob']\n","        \n","        # scale the gradient by dividing gradient dx with the probability\n","        dx = ??\n","        \n","    elif mode == 'test':\n","        \n","        # for test time, just return dout as it is        \n","        dx = dout\n","        \n","        \n","        \n","    return dx\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wwpOTPGjsqxP","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"DrSlm4mb7uw5","colab_type":"code","colab":{}},"source":["np.set_printoptions(precision=3)\n","np.random.seed(231)\n","x = np.random.randn(5, 5) + 3\n","dout = np.random.randn(*x.shape)\n","\n","dropout_param = {'mode': 'train', 'prob': 0.2, 'seed': 123}\n","out, cache = dropout_forward(x, dropout_param)\n","print('forward output:\\n')\n","print(out,'\\n')\n","\n","dx = dropout_backward(dout, cache)\n","print('dout:\\n')\n","print(dout,'\\n')\n","print('dx:\\n')\n","print(dx)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PPkJUJBo7uw8","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","forward output:\n","[[ 0.     0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.     0.   ]\n"," [ 0.     0.     0.    19.761  0.   ]\n"," [ 0.    14.26  19.448  0.     0.   ]\n"," [ 0.     0.     0.     0.     0.   ]] \n","\n","dout:\n","\n","[[-0.797  0.12  -0.657  0.269  0.334]\n"," [ 0.274  0.762 -0.696  0.292 -0.385]\n"," [ 0.123 -1.429  0.703 -0.859 -1.14 ]\n"," [-1.585 -0.015 -0.322  0.568 -0.2  ]\n"," [ 1.273  1.273  1.581 -1.756  0.922]] \n","\n","dx:\n","\n","[[-0.     0.    -0.     0.     0.   ]\n"," [ 0.     0.    -0.     0.    -0.   ]\n"," [ 0.    -0.     0.    -4.293 -0.   ]\n"," [-0.    -0.077 -1.608  0.    -0.   ]\n"," [ 0.     0.     0.    -0.     0.   ]]\n"]},{"cell_type":"markdown","metadata":{"id":"6QkYkjkS7uw-","colab_type":"text"},"source":["---\n","---\n","# [Part 2] Optimizers API\n","\n","Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. \n","\n","At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent.\n","\n","<br>\n","\n","\n","<center>\n","<img src='http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif' height=300><br><img src='https://image.ibb.co/gHFUTz/opt2.gif' height=250>\n","<img src='https://miro.medium.com/max/1240/1*Y2KPVGrVX9MQkeI8Yjy59Q.gif' height=250>\n","  \n","  \n","</center>\n","\n","<br>\n","\n","\n","For the next vanilla SGD, we give you the implementation as you've implement it many times before.\n","\n","But now we write it as a function, so <font color='red'>**Observe carefully**</font> how the implementation is done. \n","\n","Then complete the next exercises with similar implementation"]},{"cell_type":"markdown","metadata":{"id":"V2t8s5gU7uw_","colab_type":"text"},"source":["---\n","## 1 - Stochastic Gradient Descent\n","\n","The most basic optimization in Neural Network\n","<center>\n","<img src='https://image.ibb.co/em1Q1K/sgd.png' width=700>\n","</center>\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        perform gradient update\n","      </font></td>\n","    <td>:</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      W_{t+1}=W_t - \\alpha\\nabla f(W_t)\n","      \\end{align}\n","      $$</font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","<table> \n","  <tr>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      \\alpha\\\\W_t\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(lr) learning rate<br><br>(w)  old weight</pre>  </font>    \n","    </td>    \n","    <td>|<br>|<br>|<br>|</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      W_{t+1}\\\\\\nabla f(W_t)\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(new_w) new weight<br><br>(grad)  weight gradient </pre>  </font>    \n","    </td>\n","  </tr>\n","</table>\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"FPwNP9_g7uxA","colab_type":"code","colab":{}},"source":["def sgd(w, grad, optim_param):\n","    \"\"\"\n","    Performs vanilla stochastic gradient descent.\n","\n","    optim_param format:\n","    - learning_rate: Scalar learning rate.\n","    \"\"\"\n","    \n","    # set default value in parameter if it has not yet initialized\n","    optim_param.setdefault('lr', 1e-2)    \n","    \n","    # retrieve parameter\n","    lr = optim_param['lr']\n","\n","    # perform parameter update\n","    new_w = w - lr * grad\n","    \n","    return new_w"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zqyRyFKj5kSy","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"WY1OthpE7uxE","colab_type":"code","colab":{}},"source":["np.set_printoptions(precision=5)\n","np.random.seed(231)\n","w = np.random.randn(2, 2)+1\n","dout = np.random.randn(*w.shape)\n","param = {}\n","\n","print('dout:')\n","print(dout,'\\n')\n","\n","print('w:')\n","print(w,'\\n')\n","\n","w = sgd(w, dout, param)\n","print('w iteration 1:')\n","print(w,'\\n')\n","\n","w = sgd(w, dout, param)\n","print('w iteration 2:')\n","print(w)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mPhXnZl-7uxH","colab_type":"text"},"source":["---\n","## 2 - SGD With Momentum\n","\n","A method that helps accelerate SGD in the relevant direction and dampens oscillation in Vanilla SGD\n","\n","<center>\n","<img src='https://machinelearningnotepad.files.wordpress.com/2018/04/momentum-011.png?w=1140' height=180></img></center>\n","\n","<br>\n","\n","The formula for SGD With Momentum is as follow:\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        calculate the new velocity<br><br><br><br>\n","        perform momentum update\n","      </font></td>\n","    <td>:<br><br><br><br>:</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      v_t &= \\mu\\ v_{t-1}-\\alpha\\nabla f(W_t)\\\\\\\\\n","      W_{t+1}&=W_t + v_t\\\\\n","      \\end{align}\n","      $$</font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","<table> \n","  <tr>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      v_t\\\\\\mu\\\\\\alpha\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(v)   velocity<br><br>(mu)  momentum<br><br>(lr)  learning rate </pre>  </font>    \n","    </td>\n","    <td>|<br>|<br>|<br>|<br>|<br>|</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      W_t\\\\W_{t+1}\\\\\\nabla f(W_t)\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(w)     old weight<br><br>(new_w) new weight<br><br>(grad)  weight gradient </pre>  </font>    \n","    </td>\n","  </tr>\n","</table>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bZF8MDdzCjdI"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement SGD Update with Momentum**\n","\n"]},{"cell_type":"code","metadata":{"id":"cr4cV9-g7uxH","colab_type":"code","colab":{}},"source":["def momentum(w, grad, optim_param):\n","    \"\"\"\n","    Performs Stochastic Gradient Descent update with momentum.\n","\n","    optim_param format:\n","    - lr      : Scalar learning rate.\n","    - momentum: Scalar between 0 and 1 giving the momentum value.\n","                Setting momentum = 0 reduces to sgd.\n","    - velocity: A numpy array of the same shape as w and dw used to store a\n","                moving average of the gradients.\n","    \"\"\"\n","    \n","    # set default value in parameter if it has not yet initialized\n","    optim_param.setdefault('lr', 1e-2)\n","    optim_param.setdefault('momentum', 0.9)\n","    optim_param.setdefault('velocity', np.zeros_like(w))\n","        \n","    # retrieve parameters\n","    lr = optim_param['lr']\n","    mu = optim_param['momentum']\n","    v = optim_param['velocity']\n","    \n","    \n","    # calculate the new velocity\n","    v = ??\n","    \n","    # perform momentum update\n","    new_w = ??\n","\n","    # save the velocity for further update\n","    optim_param['velocity'] = v\n","        \n","    return new_w"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xpET1U-R5xRb","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"tvgNvUm37uxO","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","w = np.random.randn(2, 2)+1\n","dout = np.random.randn(*w.shape)\n","param = {}\n","\n","print('dout:')\n","print(dout,'\\n')\n","\n","print('w:')\n","print(w,'\\n')\n","\n","w = momentum(w, dout, param)\n","print('w iteration 1:')\n","print(w,'\\n')\n","\n","w = momentum(w, dout, param)\n","print('w iteration 2:')\n","print(w)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H3yJ0uvw7uxT","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","dout:\n","[[-0.07473 -0.77502]\n"," [-0.1498   1.86173]] \n","\n","w:\n","[[ 1.41794  2.3971 ]\n"," [-0.7859   0.29117]] \n","\n","w iteration 1:\n","[[ 1.41869  2.40485]\n"," [-0.78441  0.27255]] \n","\n","w iteration 2:\n","[[ 1.42011  2.41958]\n"," [-0.78156  0.23718]]\n"]},{"cell_type":"markdown","metadata":{"id":"bVhlJ2Lj7uxU","colab_type":"text"},"source":["---\n","## 3 - Nesterov Accelerated Gradient\n","\n","A method that helps direct and accelerate the gradient in Momentum Update\n","\n","\n","\n","<center>\n","<img src='https://golden-storage-production.s3.amazonaws.com/topic_images/7a00dcd221e745708101d89f4c4c2a5c.png'  height=200></img></center>\n","\n","\n","<br>\n","\n","[Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2)](http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf)\n"]},{"cell_type":"markdown","metadata":{"id":"Z5QOuTDIXqzC","colab_type":"text"},"source":["---\n","\n","<br>\n","\n","The basic formula for Nesterov Update is as follow:\n","\n","\n","<br>\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        calculate the new velocity<br>based on lookahead<br><br><br>\n","        perform nesterov update\n","      </font></td>\n","    <td>:<br><br><br><br>:</td>\n","    <td>\n","      <font size=3>\n","        $$\n","        \\begin{align}\n","        v_t &= \\mu\\ v_{t-1}-\\alpha\\nabla f(W_t+\\mu\\ v_{t-1})\\\\\\\\\n","        W_{t+1}&=W_t + v_t\\\\\n","        \\end{align}\n","        $$\n","      </font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","But we'll use the <font color='blue'>**Inverted Formula**</font>:\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        store the old velocity<br><br>\n","        calculate the new velocity<br><br>\n","        perform nesterov update\n","      </font></td>\n","    <td>:<br><br>:<br><br>:</td>\n","    <td>\n","      <font size=3>\n","        $$\n","        \\begin{align}\n","        v_{prev} &= v_t\\\\\n","        v_t &= \\mu\\ v_{t-1}-\\alpha\\nabla f(W_t)\\\\\n","        W_{t+1}&=W_t + (-\\mu)\\ v_{prev} + (1+\\mu)v_t\\\\\n","        \\end{align}\n","        $$\n","      </font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      v_t\\\\\\mu\\\\\\alpha\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(v)   velocity<br><br>(mu)  momentum<br><br>(lr)  learning rate </pre>  </font>    \n","    </td>\n","    <td>|<br>|<br>|<br>|<br>|<br>|</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      W_t\\\\W_{t+1}\\\\\\nabla f(W_t)\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(w)     old weight<br><br>(new_w) new weight<br><br>(grad)  weight gradient </pre>  </font>    \n","    </td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ib8LdnHwPaVY"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement Nesterov Accelerated Gradient Update**\n","\n"]},{"cell_type":"code","metadata":{"id":"yjPgY-lF7uxU","colab_type":"code","colab":{}},"source":["def nag(w, grad, optim_param):\n","    \"\"\"\n","    Performs Nesterov Accelerated Gradient Descent update.\n","\n","    optim_param format:\n","    - lr      : Scalar learning rate.\n","    - momentum: Scalar between 0 and 1 giving the momentum value.\n","                Setting momentum = 0 reduces to sgd.\n","    - velocity: A numpy array of the same shape as w and dw used to store a\n","                moving average of the gradients.\n","    \"\"\"\n","    # set default value in parameter if it has not yet initialized\n","    optim_param.setdefault('lr', 1e-2)\n","    optim_param.setdefault('momentum', 0.9)\n","    optim_param.setdefault('velocity', np.zeros_like(w))\n","        \n","    # retrieve parameters\n","    lr = optim_param['lr']\n","    mu = optim_param['momentum']\n","    v = optim_param['velocity']\n","    \n","    \n","    # store the previous velocity\n","    v_prev = ??\n","    \n","    # calculate the new velocity\n","    v = ??\n","    \n","    # perform nesterov update\n","    new_w = ??\n","\n","    # save the velocity for further update\n","    optim_param['velocity'] = v\n","    \n","    return new_w\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_3RDVad6QSa","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"RYa7_NEW7uxX","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","w = np.random.randn(2, 2)+1\n","dout = np.random.randn(*w.shape)\n","param = {}\n","\n","print('dout:')\n","print(dout,'\\n')\n","\n","print('w:')\n","print(w,'\\n')\n","\n","w = nag(w, dout, param)\n","print('w iteration 1:')\n","print(w,'\\n')\n","\n","w = nag(w, dout, param)\n","print('w iteration 2:')\n","print(w)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZ20oIn27uxa","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","dout:\n","[[-0.07473 -0.77502]\n"," [-0.1498   1.86173]] \n","\n","w:\n","[[ 1.41794  2.3971 ]\n"," [-0.7859   0.29117]] \n","\n","w iteration 1:\n","[[ 1.41936  2.41183]\n"," [-0.78306  0.2558 ]] \n","\n","w iteration 2:\n","[[ 1.42139  2.43283]\n"," [-0.779    0.20535]]\n"]},{"cell_type":"markdown","metadata":{"id":"HnGaMXl37uxb","colab_type":"text"},"source":["---\n","## 4 - Adaptive Sub-Gradient\n","\n","\n","Adagrad is an algorithm for gradient-based optimization that **adapts** the learning rate to the parameters. \n","\n","It performs\n","* **smaller updates** (i.e. low learning rates) for parameters associated with **frequently occurring features**, and\n","* **larger updates** (i.e. high learning rates) for parameters associated with **infrequent features**. \n","\n","For this reason, it is well-suited for dealing with sparse data\n","\n"," Adagrad greatly improved the robustness of SGD and used it for training large-scale neural nets\n"," \n","\n","<br>\n","\n","[Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://jmlr.org/papers/v12/duchi11a.html)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZN7iOghkXx-_","colab_type":"text"},"source":["---\n","The basic formula for ADAGRAD Update is as follow:\n","\n","\n","<br>\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        calculate<br>new gradient build-up<br><br><br>\n","        perform adagrad update\n","      </font></td>\n","    <td>:<br><br><br><br>:</td>\n","    <td>\n","      <font size=3>\n","        $$\n","        \\begin{align}\n","        g_t &= g_{t-1} + \\nabla f(W_t)^2\\\\\\\\\n","        W_{t+1}&=W_t -\\frac{\\alpha}{\\sqrt{g_t}+\\epsilon}\\nabla f(W_t)\\\\\n","        \\end{align}\n","        $$</font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      g_t\\\\\\alpha\\\\\\epsilon\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(gt)  gradient build-up<br><br>(lr)  learning rate<br><br>(e)   epsilon </pre>  </font>    \n","    </td>\n","    <td>|<br>|<br>|<br>|<br>|<br>|</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      W_t\\\\W_{t+1}\\\\\\nabla f(W_t)\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(w)     old weight<br><br>(new_w) new weight<br><br>(grad)  weight gradient </pre>  </font>    \n","    </td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Z1b15sQHWVHX"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement ADAGRAD Update**\n","\n"]},{"cell_type":"code","metadata":{"id":"m0uWX3us7uxb","colab_type":"code","colab":{}},"source":["def adagrad(w, grad, optim_param):\n","    \"\"\"\n","    Performs Adaptive Sub-Gradient update.\n","\n","    optim_param format:\n","    - lr: Scalar learning rate.\n","    - gt: A numpy array of the same shape as w and dw used to store a\n","          cache of the gradients build-up.\n","    - e : epsilon to prevent division by zero.\n","    \"\"\"\n","    \n","    # set default value in parameter if it has not yet initialized\n","    optim_param.setdefault('lr', 1e-2)\n","    optim_param.setdefault('cache', np.zeros_like(w))\n","    optim_param.setdefault('epsilon', 1e-8)\n","        \n","    # retrieve parameters\n","    lr = optim_param['lr']\n","    gt = optim_param['cache']    \n","    e = optim_param['epsilon']\n","    \n","    # calculate new gradient build-up\n","    gt = ??\n","    \n","    # perform adagrad update\n","    new_w = ??\n","    \n","    # save the gradient build-up for further update\n","    optim_param['cache'] = gt\n","    \n","    return new_w"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ui3TAykE6UNq","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"pcxVuWs37uxe","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","w = np.random.randn(2, 2)+1\n","dout = np.random.randn(*w.shape)\n","param = {}\n","\n","print('dout:')\n","print(dout,'\\n')\n","\n","print('w:')\n","print(w,'\\n')\n","\n","w = adagrad(w, dout, param)\n","print('w iteration 1:')\n","print(w,'\\n')\n","\n","w = adagrad(w, dout, param)\n","print('w iteration 2:')\n","print(w)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6NNoqdDj7uxg","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","dout:\n","[[-0.07473 -0.77502]\n"," [-0.1498   1.86173]] \n","\n","w:\n","[[ 1.41794  2.3971 ]\n"," [-0.7859   0.29117]] \n","\n","w iteration 1:\n","[[ 1.42794  2.4071 ]\n"," [-0.7759   0.28117]] \n","\n","w iteration 2:\n","[[ 1.43501  2.41417]\n"," [-0.76883  0.2741 ]]\n"]},{"cell_type":"markdown","metadata":{"id":"KANu9uLE7uxj","colab_type":"text"},"source":["---\n","## 5 - Root Mean Square Propagation\n","\n","RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in [Lecture 6e of his Coursera Class](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).\n","\n","It’s famous for not being published, yet being very well-known; most deep learning framework include the implementation of it out of the box.\n","\n","RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"30qv8phce-3B"},"source":["---\n","The basic formula for RMSProp Update is as follow:\n","\n","\n","<br>\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        calculate new decayed<br> gradient build-up<br><br><br>\n","        perform rmsprop update\n","      </font></td>\n","    <td>:<br><br><br><br>:</td>\n","    <td>\n","      <font size=3>\n","        $$\n","        \\begin{align}\n","        \\\\g_t &= \\gamma\\ g_{t-1} + (1-\\gamma)\\ \\nabla f(W_t)^2\\\\\\\\\n","        W_{t+1}&=W_t -\\frac{\\alpha}{\\sqrt{g_t}+\\epsilon}\\nabla f(W_t)\\\\\n","        \\end{align}\n","        $$</font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      g_t\\\\\\gamma\\\\\\alpha\\\\\\epsilon\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(gt)  gradient build-up<br><br>(dr)  decay rate<br><br>(lr)  learning rate<br><br>(e)   epsilon </pre>  </font>    \n","    </td>\n","    <td>|<br>|<br>|<br>|<br>|<br>|</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      W_t\\\\W_{t+1}\\\\\\nabla f(W_t)\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(w)     old weight<br><br>(new_w) new weight<br><br>(grad)  weight gradient </pre>  </font>    \n","    </td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZRxGsem6flgY"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement RMSProp Update**\n","\n"]},{"cell_type":"code","metadata":{"id":"oVj46hcq7uxk","colab_type":"code","colab":{}},"source":["def rmsprop(w, grad, optim_param):\n","    \"\"\"\n","    Uses the RMSProp update rule, which uses a moving average of squared\n","    gradient values to set adaptive per-parameter learning rates.\n","\n","    optim_param format:\n","    - lr        : Scalar learning rate.\n","    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n","                  gradient cache.\n","    - epsilon   : Small scalar used for smoothing to avoid dividing by zero.\n","    - cache     : Moving average of second moments of gradients.\n","    \"\"\"\n","    \n","    # set default value in parameter if it has not yet initialized\n","    optim_param.setdefault('lr', 1e-2)\n","    optim_param.setdefault('decay_rate', 0.99)\n","    optim_param.setdefault('epsilon', 1e-8)\n","    optim_param.setdefault('cache', np.zeros_like(w))\n","\n","\n","    # retrieve parameters\n","    lr = optim_param['lr']\n","    dr = optim_param['decay_rate']\n","    gt = optim_param['cache']\n","    e  = optim_param['epsilon']\n","\n","    # calculate new gradient build-up\n","    gt = ??\n","    \n","    # perform rmsprop update\n","    new_w = ??\n","    \n","    # save the gradient build-up for further update\n","    optim_param['cache'] = gt\n","    \n","    return new_w\n","  \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SDmmVdSMfkfs","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"pBH6v1iH7uxo","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","w = np.random.randn(2, 2)+1\n","dout = np.random.randn(*w.shape)\n","param = {}\n","\n","print('dout:')\n","print(dout,'\\n')\n","\n","print('w:')\n","print(w,'\\n')\n","\n","w = rmsprop(w, dout, param)\n","print('w iteration 1:')\n","print(w,'\\n')\n","\n","w = rmsprop(w, dout, param)\n","print('w iteration 2:')\n","print(w)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XooySUH77uxq","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","dout:\n","[[-0.07473 -0.77502]\n"," [-0.1498   1.86173]] \n","\n","w:\n","[[ 1.41794  2.3971 ]\n"," [-0.7859   0.29117]] \n","\n","w iteration 1:\n","[[ 1.51794  2.4971 ]\n"," [-0.6859   0.19117]] \n","\n","w iteration 2:\n","[[ 1.58883  2.56799]\n"," [-0.61502  0.12028]]\n"]},{"cell_type":"markdown","metadata":{"id":"ysUITZiu7uxs","colab_type":"text"},"source":["---\n","## 6 - Adaptive Moment Estimation Optimizer\n","\n","ADAM is another method that computes adaptive learning rates for each parameter. \n","\n","In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum.\n","\n","Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. \n","\n","\n","[Kingma, D. P., & Ba, J. L. (2015). Adam: a Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rBvy85FpWyt7"},"source":["---\n","The basic formula for ADAM Update is as follow:\n","\n","\n","<br>\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        first moment (mean)<br> build-up<br><br><br>\n","        second moment (variance)<br>build-up<br><br><br><br>\n","        bias correction<br><br><br><br><br>\n","        perform adam update<br><br>\n","      </font></td>\n","    <td>:<br><br><br><br><br>:<br><br><br><br><br>:<br><br><br><br><br>:<br><br></td>\n","    <td>\n","      <font size=3>\n","        $$\n","        \\begin{align}\n","        m_t &= \\beta_1\\ m_{t-1} + (1-\\beta_1)\\ \\nabla f(W_t)\\\\\\\\\n","        v_t &= \\beta_2\\ v_{t-1} + (1-\\beta_2)\\ \\nabla f(W_t)^2\\\\\\\\\n","        \\hat{m_t}&=\\frac{m_t}{1-(\\beta_1)^t} \\ \\ ;\\ \\ \n","        \\hat{v_t} =\\frac{v_t}{1-(\\beta_2)^t}\\\\\\\\\n","        W_{t+1}&=W_t -\\frac{\\alpha}{\\sqrt{\\hat{v_t}}+\\epsilon}\\hat{m_t}\n","        \\end{align}\n","$$</font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      \\beta_1\\\\\\beta_2\\\\m_t\\\\v_t\\\\\\hat{m_t}\\\\\\hat{v_t}\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(beta1) beta mean<br><br>(beta2) beta variance<br><br>(m)     1st moment (mean) buildup<br><br>(v)     2nd moment (variance) buildup<br><br>(mb)    1st moment (mean) bias<br><br>(mv)    2nd moment (variance) bias</pre>  </font>    \n","    </td>\n","    <td>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      \\alpha\\\\\\epsilon\\\\W_t\\\\W_{t+1}\\\\\\nabla f(W_t)\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(lr)    learning rate<br><br>(e)     epsilon <br><br>(w)     old weight<br><br>(new_w) new weight<br><br>(grad)  weight gradient </pre>  </font>    \n","    </td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HpwaA7XXWyuN"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement ADAM Update**\n","\n"]},{"cell_type":"code","metadata":{"id":"kzRNYgfT7uxs","colab_type":"code","colab":{}},"source":["def adam(w, grad, optim_param):\n","    \"\"\"\n","    Uses the Adam update rule, which incorporates moving averages of both the\n","    gradient and its square and a bias correction term.\n","\n","    config format:\n","    - lr      : Scalar learning rate.\n","    - beta1   : Decay rate for moving average of first moment of gradient.\n","    - beta2   : Decay rate for moving average of second moment of gradient.\n","    - epsilon : Small scalar used for smoothing to avoid dividing by zero.\n","    - m       : Moving average of first moment of gradient.\n","    - v       : Moving average of second moment of gradient (squared gradient).\n","    - t       : Iteration number.\n","    \"\"\"\n","    \n","    # set default value in parameter if it has not yet initialized\n","    optim_param.setdefault('lr', 1e-2)\n","    optim_param.setdefault('beta1', 0.9)\n","    optim_param.setdefault('beta2', 0.999)\n","    optim_param.setdefault('epsilon', 1e-8)\n","    optim_param.setdefault('m', np.zeros_like(w))\n","    optim_param.setdefault('v', np.zeros_like(w))\n","    optim_param.setdefault('t', 0)\n","\n","    # retrieve parameters\n","    lr = optim_param['lr']\n","    beta1 = optim_param['beta1']\n","    beta2 = optim_param['beta2']\n","    e = optim_param['epsilon']\n","    m = optim_param['m']\n","    v = optim_param['v']\n","    optim_param['t'] += 1\n","    \n","    \n","    # get the iteration number\n","    t = optim_param['t']\n","    \n","    # calculate first moment (mean) build-up\n","    m = ??\n","    \n","    # calculate second moment (variance) build-up\n","    v = ??\n","    \n","    # perform bias correction\n","    mb = ??\n","    vb = ??\n","    \n","    # perform adam update\n","    new_w = ??\n","    \n","    # save the mean and variance for further update\n","    optim_param['m'] = m\n","    optim_param['v'] = v\n","    \n","    \n","    return new_w"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3Oo7TMOcQff","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"Ep3lCrR-7uxu","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","w = np.random.randn(2, 2)+1\n","dout = np.random.randn(*w.shape)\n","param = {}\n","\n","print('dout:')\n","print(dout,'\\n')\n","\n","print('w:')\n","print(w,'\\n')\n","\n","w = adam(w, dout, param)\n","print('w iteration 1:')\n","print(w,'\\n')\n","\n","w = adam(w, dout, param)\n","print('w iteration 2:')\n","print(w)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pCXhMBTt7uxx","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","dout:\n","[[-0.07473 -0.77502]\n"," [-0.1498   1.86173]] \n","\n","w:\n","[[ 1.41794  2.3971 ]\n"," [-0.7859   0.29117]] \n","\n","w iteration 1:\n","[[ 1.42794  2.4071 ]\n"," [-0.7759   0.28117]] \n","\n","w iteration 2:\n","[[ 1.43794  2.4171 ]\n"," [-0.7659   0.27117]]\n"]},{"cell_type":"markdown","metadata":{"id":"Wdj5vj5G7uxy","colab_type":"text"},"source":["---\n","---\n","# [Part 3] Deep Neural Network API\n","\n","For this exercise, we provide you these codes to easily define architecture then train and test deep neural network\n","\n","You've seen implementation similar to this in the optional parts in the previous exercise"]},{"cell_type":"markdown","metadata":{"id":"v1Bkxaa5mX3Q","colab_type":"text"},"source":["---\n","## 1 - Weight Initializer\n","\n","Weight initialization function "]},{"cell_type":"code","metadata":{"id":"sFPN3JCUm15b","colab_type":"code","colab":{}},"source":["def init_weights_affine(d_in, d_out, std=1e-2):\n","    \"\"\"\n","    Weight initialization for affine layer\n","    \n","    Inputs:\n","    - d_in  : int, number of input dimension\n","    - d_out : int, number of output dimension\n","    - std   : standard deviation for generating weights\n","    - seed  : random seed\n","    \n","    Outputs:\n","    - W: list of Weights\n","    - b: list of biases\n","    \"\"\"\n","    \n","    W = std * np.random.randn(d_in, d_out).astype(np.float32)\n","    b = np.zeros((1, d_out)).astype(np.float32)\n","\n","    return W, b\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Itws6LKGcEl0","colab_type":"text"},"source":["---\n","## 2 - Model Initializer\n","\n","Model Initialization Function to initialize weights for each defined layer in architecture\n","\n","This function will receive a list of designed neural net **architecture**, and it will initialize weights according to the layer type and parameters given in the designed architecture.\n","\n","The architecture feed in is a list of layers that is stacked in a **Sequential Model** fashion. The Layer is also a list of named layer type followed by its required parameters.\n","\n","The Model initializer receive **3 kinds** of layer:\n","\n","<br>\n","<font size=10>\n","<table border=1 solid>\n","  <tr border=1>\n","    <th><font size=2>      Layer Name    </font></th>\n","    <th><font size=2>      Definition and Parameters    </font></th>\n","    <th colspan=2><font size=2>      Parameters    </font></th>\n","  </tr>\n","  <tr>\n","    <td><font size=3>      <pre>'input'</pre>    </font></td>\n","    <td><font size=2>      input layer, <br>to determine the input shape    </font></td>\n","    <td><font size=3>      <pre>(h, w, c)</pre>    </font></td>\n","    <td><font size=2>      tuple of input dimension     </font></td>\n","  </tr>\n","  <tr>\n","    <td><font size=3>      <pre>'affine'</pre>    </font></td>\n","    <td><font size=2>      affine layer, <br>followed by relu activation    </font></td>\n","    <td><font size=3>      <pre>d_out</pre>    </font></td>\n","    <td><font size=2>      integer, <br>number of output neurons     </font></td>\n","  </tr>\n","  <tr>\n","    <td><font size=3>      <pre>'dropout'</pre>    </font></td>\n","    <td><font size=2>      dropout layer    </font></td>\n","    <td><font size=3>      <pre>prob</pre>    </font></td>\n","    <td><font size=2>      float, <br>dropout probability     </font></td>\n","  </tr>\n","</table>\n","\n","</font>"]},{"cell_type":"code","metadata":{"id":"BStKpGcOcZ35","colab_type":"code","colab":{}},"source":["def init_model(architecture, std=1e-2):    \n","  \n","    \"\"\"\n","    Inputs:\n","    - architecture  : list of layer name and paramters\n","    \n","    \n","    Outputs:\n","    - neural net model, a tuple of:\n","      - architecture: list of layer name and paramters\n","      - params      : list of compact layer weights and parameters\n","    \n","    \"\"\"     \n","        \n","    # define output container\n","    params = {}\n","    d_in = 0\n","    \n","    # the first layer must be input layer\n","    assert(architecture[0][0]=='input')\n","    \n","    # loop read list of layer\n","    for i in range(len(architecture)):\n","      \n","        # read the layer information\n","        layer = architecture[i]\n","        \n","        # if the layer is input, get the input shape\n","        if layer[0]=='input':            \n","            d_in = layer[1]\n","            \n","        # if the layer is affine layer,\n","        # initialize weight and bias\n","        elif layer[0]=='affine':  \n","          \n","            # get the previous output shape\n","            d_out = layer[1]            \n","            \n","            # initialize weight and bias\n","            params['W'+str(i)], params['b'+str(i)] = init_weights_affine(d_in, d_out, std)            \n","            \n","            # set this layer output shape as previous output shape \n","            # for the next layer\n","            d_in = d_out\n","            \n","        # if the layer is affine dropout,\n","        elif layer[0]=='dropout':\n","          \n","            # get the dropout probability\n","            prob = layer[1]\n","            \n","            # compact the parameters\n","            params[i] = {'prob': prob, 'mode':'train'}\n","            \n","    # combine the weights and its parameters into a tuple of model\n","    model = (architecture, params)\n","            \n","    # return the network model\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXrMvpj-7ux1","colab_type":"text"},"source":["---\n","## 3 - Predict Function"]},{"cell_type":"code","metadata":{"id":"jepk73u_7ux1","colab_type":"code","colab":{}},"source":["def predict(model, X):    \n","    \"\"\"\n","    Inputs:\n","    - model : Network model architecture and weights\n","    - X     : Input data, of shape(N, D)\n","    \n","    Output:\n","    - y_pred : list of class prediction\n","    \"\"\"\n","    \n","    # get the model architecture and weights\n","    architecture, params = model\n","    \n","    # get number of layers\n","    n_layer = len(architecture)\n","    \n","    # set input X as the first activation\n","    act = X\n","    \n","    # loop i over n_layer-1\n","    for i in range(n_layer-1):\n","      \n","      # get layer architecture\n","      layer = architecture[i]\n","        \n","      # check layer type\n","      if layer[0]=='affine':\n","          # call affine relu forward function with input act, W[i], and b[i]\n","          act, _ = affine_relu_forward(act, params['W'+str(i)],  params['b'+str(i)])\n","          \n","      elif layer[0]=='dropout':\n","          # change mode to testing\n","          params[i]['mode'] = 'test'\n","          \n","          # call dropout forward function with input act and its param argument\n","          act, _ = dropout_forward(act, params[i])\n","          \n","          # change back mode to training\n","          params[i]['mode'] = 'train'\n","    \n","    # calculate last layer score by calling affine forward function using act, W[i+1], and b[i+1]\n","    last_layer, _ = affine_forward(act, params['W'+str(i+1)],  params['b'+str(i+1)])\n","    \n","    # take the maximum prediction from the last layer and use that column to get the class     \n","    y_pred = last_layer.argmax(axis=-1)\n","    \n","    return y_pred\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ZMxFcM77ux4","colab_type":"text"},"source":["---\n","## 4 - Training Function"]},{"cell_type":"code","metadata":{"id":"BT1TN0jO7ux5","colab_type":"code","colab":{}},"source":["def train(model, X, y, X_val, y_val,\n","                optimizer, optim_param,\n","                lr_decay=0.9, reg=0.25, epochs=20, \n","                batch_size=200, \n","                verbose=True, print_every=100):\n","    \"\"\"\n","    Inputs:\n","    - model      : Network model architecture and weights\n","    - X          : Array, shape (N, C, H, W) of training images\n","    - y          : Array, shape (N,) of labels for training images\n","    - X_val      : Array, shape (N_val, C, H, W) of validation images\n","    - y_val      : Array, shape (N_val,) of labels for validation images  \n","    - optimizer  : optimizer function\n","    - optim_param: optimizer parameters\n","    - lr_decay   : float, 0-1, decay rate to reduce learning rate each epoch\n","    - reg        : float, regularization rate\n","    - epochs     : int, number of training epoch\n","    - batch_size : int, number of batch used each step\n","    - verbose    : boolean, verbosity\n","    \n","    Outputs:\n","    - model      : Network model architecture and weights\n","    - history    : list of training history [loss, train_acc, val_acc]\n","    \n","    \"\"\"\n","    \n","      \n","    \n","    # extract input size\n","    num_train, dim = X.shape\n","    \n","    # assume y takes values 0...K-1 where K is number of classes\n","    num_classes = np.max(y) + 1 \n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","    \n","    # get the model architecture \n","    architecture, params = model  \n","    \n","    # get the layer number (including output layer)\n","    n_layer = len(architecture)\n","    \n","    # Run gradient to optimize W\n","    loss_history = []\n","    train_acc_history = [0]\n","    val_acc_history = [0]\n","    \n","    # dictionary for gradients\n","    grads = {}\n","    optim_params = {}\n","    \n","    # extract the optimizer parameters\n","    for i in range(1,n_layer):\n","        optim_params['W'+str(i)] = dict(optim_param)\n","        optim_params['b'+str(i)] = dict(optim_param)\n","                     \n","          \n","          \n","                     \n","    print('start training')\n","    \n","    for ep in range(epochs):\n","      \n","        # Shuffle data train index\n","        train_rows = np.arange(num_train)\n","        np.random.shuffle(train_rows)\n","        \n","        # split index into mini batches\n","        id_batch = np.split(train_rows, num_iter)\n","  \n","        for batch in id_batch:\n","      \n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","        \n","            # store all cache in dictionary\n","            cache = {}\n","\n","            # first layer activation input is X_batch\n","            act = X_batch\n","\n","\n","            # ------------------------------------------------\n","            # 1. Forward Pass\n","            # ------------------------------------------------\n","\n","            # loop i over n_layer-1\n","            for i in range(n_layer-1):\n","              \n","                # get layer architecture                \n","                layer = architecture[i]\n","\n","                # call the appropriate layer\n","                if layer[0]=='affine':\n","                    # call affine relu forward function with input act, W[i], and b[i]\n","                    act, cache[i] = affine_relu_forward(act, params['W'+str(i)],  params['b'+str(i)])\n","                    \n","                elif layer[0]=='dropout':\n","                    # call dropout forward function with input act and its param argument\n","                    act, cache[i] = dropout_forward(act, params[i])\n","\n","            # calculate last layer score by calling affine forward function using act, W[-1], and b[-1]\n","            last_layer, cache[i+1] = affine_forward(act, params['W'+str(i+1)],  params['b'+str(i+1)])\n","\n","            # calculate softmax score by calling softmax function using last_layer score\n","            softmax_score = softmax(last_layer)\n","\n","            \n","            \n","            # ------------------------------------------------\n","            # 2. Calculate Loss\n","            # ------------------------------------------------\n","\n","            # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n","            loss, dout = softmax_loss(softmax_score, y_batch)\n","\n","            # add regularization to the loss:\n","            #    for each weights, calculate the sum square, multiply regularization strength\n","            #    then add it to the loss\n","            for i in range(1,n_layer):\n","                w = params.get('W'+str(i),0)\n","                loss += reg * np.sum(w * w)\n","\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","\n","            # ------------------------------------------------\n","            # 3. Backward Pass\n","            # ------------------------------------------------    \n","\n","            # calculate last weights gradient by calling affine backward function using dout and cache[n_layer-1]\n","            dact, grads['W'+str(n_layer-1)],  grads['b'+str(n_layer-1)] = affine_backward(dout, cache[n_layer-1])\n","\n","\n","            #loop i from n_layer-2 down to 0\n","            for i in range(n_layer-2,-1,-1):\n","              \n","                # get layer architecture  \n","                layer = architecture[i]\n","\n","                # check layer type\n","                if layer[0]=='affine':\n","                    # call affine relu backward function with input dact and cache[i]\n","                    dact, grads['W'+str(i)],  grads['b'+str(i)] = affine_relu_backward(dact, cache[i])  \n","\n","                    # add regularization to gradient\n","                    grads['W'+str(i)] += 2 * reg * params['W'+str(i)]\n","\n","                elif layer[0]=='dropout': \n","                    # call dropout backward function with input dact and cache[i]\n","                    dact = dropout_backward(dact, cache[i])\n","\n","\n","            # perform parameter update according to the optimizer function\n","            # pass the weights, gradients, and its optimization parameters to optimizer function\n","            for i in range(1,n_layer):\n","                if 'W'+str(i) in params:\n","                    params['W'+str(i)] = optimizer(params['W'+str(i)], grads['W'+str(i)], optim_params['W'+str(i)])\n","                    params['b'+str(i)] = optimizer(params['b'+str(i)], grads['b'+str(i)], optim_params['b'+str(i)])\n","\n","                    \n","            # iteration count\n","            it +=1\n","\n","            if verbose and it % print_every == 1:\n","                print ('iteration',it,'/',(num_iter*epochs),'\\t(epoch', ep+1,'/',epochs, '): \\tloss =', loss)\n","                 \n","                  \n","        # record model at this epoch    \n","        model_epoch = (architecture, params) \n","        \n","        # At the end of one epoch\n","        # 1. Check accuracy\n","        #    calculate the training accuracy by calling predict function on model_epoch and X_batch\n","        #    and compare it to y_batch. Then calculate the mean correct (accuracy in range 0-1)\n","        train_acc = (predict(model, X_batch) == y_batch).mean()\n","        train_acc_history.append(train_acc)\n","\n","        # 2. Calculate the training accuracy by calling predict function on model_epoch and X_val\n","        #    and compare it to y_val. Then calculate the mean correct (accuracy in range 0-1)\n","        val_acc = (predict(model, X_val) == y_val).mean()\n","        val_acc_history.append(val_acc)\n","\n","        # 3. Decay learning rate\n","        #    multiply learning rate with decay\n","        #    see sigmoid train function\n","        optim_param['lr'] *= lr_decay\n","            \n","\n","    # store trained weights in model\n","    model = (architecture, params)      \n","    \n","    # store all trining history\n","    history = [loss_history, train_acc_history, val_acc_history]\n","    \n","    # return model and training history\n","    return model, history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J_HugjOZ7ux6","colab_type":"text"},"source":["---\n","---\n","# [Part 4] Comparing Dropout Layer\n","\n","For the next step, we'll train a** 4 Layered Neural Network** to classify CIFAR-10\n","\n","We'll compare the result between models trained **with** and **without** Dropout Layer\n"]},{"cell_type":"markdown","metadata":{"id":"lCvHgysJ7ux7","colab_type":"text"},"source":["---\n","## 1 - Model w/o Dropout\n","\n","First, let's train a 4 layered Neural Net Without Dropout with only **50 neurons** in each hidden layer,\n","\n","just to check our Deep Neural Network API (Sanity Check)\n","\n","The result should match exactly like your previous **Task 3** exercises"]},{"cell_type":"markdown","metadata":{"id":"AfbZLVEF7V04","colab_type":"text"},"source":["---\n","### a. Define Model"]},{"cell_type":"code","metadata":{"id":"kKc2hdRT7ux7","colab_type":"code","colab":{}},"source":["# architecture options:\n","# input, input_size\n","# affine, hidden_size\n","# dropout, dropout_prob\n","\n","architecture =[\n","    ['input', 32*32*3],\n","    ['affine', 50],\n","    ['affine', 50],\n","    ['affine', 50],\n","    ['affine', 10]\n","]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIMq_H5dFA0d","colab_type":"text"},"source":["Now print the architecture list"]},{"cell_type":"code","metadata":{"id":"GkthWmlFFIqL","colab_type":"code","colab":{}},"source":["for i in range(len(architecture)):\n","    layer = architecture[i]\n","    print('layer',i,':',layer)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehyHE8M0FFAM","colab_type":"text"},"source":["next, initialize the model"]},{"cell_type":"code","metadata":{"id":"nCC4F3HEcZ4C","colab_type":"code","colab":{}},"source":["model = init_model(architecture)\n","model[1].keys()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nOu04wHyxCRb","colab_type":"text"},"source":["---\n","### b. Train Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BNUtQPJgHFzM"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **four layer neural network** without **dropout** using **Vanilla SGD**\n"]},{"cell_type":"code","metadata":{"id":"9jbssLot7uyA","colab_type":"code","colab":{}},"source":["optim_param = {'lr':1e-2}\n","model, history = train(model, X_train, y_train, \n","                                 X_val, y_val,\n","                                 sgd, optim_param,\n","                                 lr_decay=0.95, reg=0.01,\n","                                 batch_size=200, epochs=20)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XPRTFnUFUYNB"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","loss should starts around 2.45 and ends around 1.39"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0JFHJFqnUYNC"},"source":["---\n","### c. Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"vWdJSQZaUYND","colab":{}},"source":["loss, train_acc, val_acc = history\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"id":"DB9L6fdu7uyH","colab_type":"text"},"source":["**EXPECTED RESULTS**:\n","\n","You should get almost the **exact same result** as your previous **Task 3** implementation"]},{"cell_type":"code","metadata":{"id":"bUNe-BTx7uyK","colab_type":"code","colab":{}},"source":["y_pred = predict(model, X_train)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","print('Training Accuracy   = %.1f%%' % (accuracy*100))\n","\n","y_pred = predict(model, X_val)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy = %.1f%%' % (accuracy*100))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lNUsuUZIUYNU"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should be able to get about <b>~57%</b> accuracy on training set and about <b>~52%</b> accuracy on validation set \n","using the initial run</pre>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DD0FjEQM7uyP","colab_type":"text"},"source":["---\n","## 2 - Model with Dropout\n","\n","Next, let's train a set of 4 layered Neural Nets but now using **Dropout** after each hidden layer\n","\n","We'll try **4** different Dropout probabilites from **1 to 0.25** in 0.25 increments\n","\n","<br>\n","\n","Note that with Dropout **probability=1** means we use all neurons (**no dropout**)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UOzFVeHi8a1M"},"source":["---\n","### a. Define Model\n","\n","For greater effect on the dropout, we'll use **500 neurons** in each hidden layer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jR0MrMnZ8a1T","colab":{}},"source":["# architecture options:\n","# input, input_size\n","# affine, hidden_size\n","# dropout, dropout_prob\n","\n","def model_dropout(prob):\n","  \n","    architecture =[\n","        ['input', 32*32*3],\n","        ['affine', 500],\n","        ['dropout', prob],\n","        ['affine', 500],\n","        ['dropout', prob],\n","        ['affine', 500],\n","        ['dropout', prob],\n","        ['affine', 10]\n","    ]\n","    \n","    return init_model(architecture)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8EX1en37uyP","colab_type":"text"},"source":["---\n","### b. Train Models\n","\n","Train model for each dropout probability and compare the train-val accuracy\n","\n","<font color='red'>*Note: it took **several minutes** to train it all*"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b5g-XKHx89bo"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **four layer neural network** with **dropout** using **Vanilla SGD**\n"]},{"cell_type":"code","metadata":{"id":"N_lRXAt37uyR","colab_type":"code","colab":{}},"source":["probs = [1, 0.75, 0.5, 0.25]\n","\n","models = {}\n","history = {}\n","\n","for prob in probs:\n","    print('-------------------\\n')\n","    print('Try Dropout =', prob)\n","\n","    models[prob] = model_dropout(prob)\n","    \n","    optim_param = {'lr':1e-2}\n","    optim       = sgd\n","    \n","    models[prob], history[prob] = train(models[prob], \n","                                        X_train, y_train, \n","                                        X_val, y_val,\n","                                        optim, optim_param,\n","                                        lr_decay=0.9, reg=0.005,\n","                                        batch_size=200, epochs=12,\n","                                        print_every=500)\n","    \n","    y_pred = predict(models[prob], X_train)\n","    accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","    print('Training Accuracy   = %.1f%%' % (accuracy*100))\n","\n","    y_pred = predict(models[prob], X_val)\n","    accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","    print('Validation Accuracy = %.1f%%' % (accuracy*100))\n","    print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BSJb1u6n-WYm"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","loss should starts around 3.3 and ends \n","  around 1.7 for dropout 1\n","  around 2.0 for dropout 0.75\n","  around 2.1 for dropout 0.5\n","  around 2.5 for dropout 0.25"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4goovYue-WYu"},"source":["---\n","### c. Visualize Training\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"XGwUfnQI7uyU","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.subplot(121)\n","for prob in probs:\n","    plt.plot(history[prob][1], label='prob: '+str(prob))\n","plt.title('Training Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.legend()\n","\n","plt.subplot(122)\n","for prob in probs:\n","    plt.plot(history[prob][2], label='prob: '+str(prob))\n","plt.title('Validation Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GzXfoEhc7uyW","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","The images should be similar to:\n","\n","|train |val |\n","|------|------|\n","|![plot1](https://image.ibb.co/jBmQoe/plot1.png) |![plot1](https://image.ibb.co/fiujgz/plot2.png)|\n"]},{"cell_type":"markdown","metadata":{"id":"T3thyBc17uyX","colab_type":"text"},"source":["You'll see that using dropout may decrease the training and validation accuracy, since not all neuron used during training\n","\n","However, using the right dropout, the validation accuracy may not decrease too much"]},{"cell_type":"markdown","metadata":{"id":"9GTEnJexsXS1","colab_type":"text"},"source":["---\n","### d. Compare Overfitting\n","\n","The main objective of using Dropout is using it as a Regularizer to prevent Overfitting.\n","\n","To show that, let's compare the training and validation accuracy over epoch for each Dropout Probability\n","\n","We know that the greater distance between train and validation accuracy means stronger overfitting"]},{"cell_type":"code","metadata":{"id":"bSL-fFR67uyY","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [15, 10]\n","i=0\n","for prob in probs:\n","    i+=1\n","    plt.subplot(2, 2, i)\n","    plt.plot(history[prob][1], label='train')\n","    plt.plot(history[prob][2], label='val')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Clasification prob: '+str(prob))\n","    plt.ylim(0,.9)\n","    plt.legend()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rS8U4Bq7uya","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","* You'll see that using dropout will **prevent overfitting** as the training accuracy and validation accuracy plot difference is as not far as without dropout\n","\n","* Thus, from this exercise, it can be seen that using dropout with **0.75 probability** is better\n","\n","<br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OcYjXJV-7uyb","colab_type":"text"},"source":["---\n","---\n","# [Part 5] Comparing Optimizers\n","\n","Now let's train another set of 4 Layered Neural Networks to compare the result between models trained with **various optimization schemas**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HB4DIwdhnmBf"},"source":["---\n","## a. Define Model\n","\n","For greater effect on the optimizer, we'll use **200 neurons** in each hidden layer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kl_bcGf_nmBn","colab":{}},"source":["# architecture options:\n","# input, input_size\n","# affine, hidden_size\n","# dropout, dropout_prob\n","\n","def model_optim():\n","  \n","    architecture =[\n","        ['input', 32*32*3],\n","        ['affine', 200],\n","        ['affine', 200],\n","        ['affine', 200],\n","        ['affine', 10]\n","    ]\n","    \n","    return init_model(architecture)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tYaGQwN37uyb","colab_type":"text"},"source":["---\n","## b. Train Models\n","\n","Train model for each optimization function using **learning_rate = 1e-4**, then compare the train-val accuracy\n","\n","\n","\n","<font color='red'>*Note: it took **several minutes** to train it all*"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H2iMRH7-r2Ls"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **four layer neural network** with **dropout** using **Vanilla SGD**\n"]},{"cell_type":"code","metadata":{"id":"wPTULyyU7uyb","colab_type":"code","colab":{}},"source":["optims = [sgd, momentum, nag, adagrad, rmsprop, adam]\n","\n","models_o = {}\n","history_o = {}\n","\n","for optim in optims:\n","    print('-------------------\\n')\n","    print('Try Optimizer =', optim.__name__)\n","    \n","    models_o[optim] = model_optim()\n","    \n","    optim_param = {'lr':1e-4}\n","    models_o[optim], history_o[optim]= train(models_o[optim], \n","                                           X_train, y_train, \n","                                           X_val, y_val,\n","                                           optim, optim_param,\n","                                           lr_decay=0.9, reg=0.005,\n","                                           batch_size=200, epochs=12,\n","                                           print_every=500)\n","\n","    y_pred = predict(models_o[optim], X_train)\n","    accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","    print('Training Accuracy   = %.1f%%' % (accuracy*100))\n","\n","    y_pred = predict(models_o[optim], X_val)\n","    accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","    print('Validation Accuracy = %.1f%%' % (accuracy*100))\n","    print()\n","    \n","   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rF2QoqTYq6I0"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","loss should starts around 2.6 and ends \n","  around 2.6 for Vanilla SGD\n","  around 1.9 for SGD with Momentum\n","  around 2.0 for Nesterov Update\n","  around 1.9 for AdaGrad Update\n","  around 1.1 for RMSProp Update\n","  around 1.2 for Adam Update"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UEnwFzctqc70"},"source":["---\n","## c. Visualize Training\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"ChWJGnUA7uyd","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [15, 5]\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.subplot(121)\n","for optim in optims:\n","    plt.plot(history_o[optim][1], label= optim.__name__)\n","plt.title('Training Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.legend()\n","\n","plt.subplot(122)\n","for optim in optims:\n","    plt.plot(history_o[optim][2], label= optim.__name__)\n","plt.title('Validation Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.legend()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OfzW18tF7uyg","colab_type":"text"},"source":["**Expected Output**:\n","\n","The images should be similar to:\n","\n","|train |val |\n","|------|------|\n","|![plot1](https://image.ibb.co/fmAbTe/plot3.png) |![plot1](https://image.ibb.co/kkXZEK/plot4.png)|\n"]},{"cell_type":"markdown","metadata":{"id":"kkqgGqNs7uyh","colab_type":"text"},"source":["Notice the difference in training and validation accuracy using different optimization method:\n","* You'll see that using **Vanilla SGD** with a really small learning rate, the loss is barely changing\n","\n","* Using **SGD with Momentum**, **Nesterov Accelerated**, or **AdaGrad** will make the network keep learning (updating its weights) even with small learning rate\n","\n","* **SGD with Momentum** and **Nesterov Accelerated** even start to picking up its speed to match the **AdaGrad**\n","\n","* The learning speed is greatly increased when using **RMSProp** and **Adam Update**\n","\n"]},{"cell_type":"code","metadata":{"id":"b1T5wLil7uyh","colab_type":"code","colab":{}},"source":["for optim in optims:\n","    plt.plot(history_o[optim][0][::300], label=optim.__name__)\n","plt.title('Loss history (per 300 iteration)')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSM4boh-7uyl","colab_type":"text"},"source":["Detailed Loss for each Optimization Schemes"]},{"cell_type":"code","metadata":{"id":"1T4hd_uw7uyl","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [15, 10]\n","i=0\n","for optim in optims:\n","    i+=1\n","    plt.subplot(3, 2, i)\n","    plt.plot(history_o[optim][0], label=optim.__name__)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"E_cvccUjtBqf"},"source":["---\n","## d. Compare Overfitting\n","\n","Now Let's compare the overfitting rate"]},{"cell_type":"code","metadata":{"id":"hYELicdN7uym","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [15, 10]\n","i=0\n","for optim in optims:\n","    i+=1\n","    plt.subplot(3, 2, i)\n","    plt.plot(history_o[optim][1], label='train')\n","    plt.plot(history_o[optim][2], label='val')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy '+optim.__name__)\n","    plt.ylim(0,.85)\n","    plt.legend()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m1GYS5Rk7uyn","colab_type":"text"},"source":["---\n","---\n","# Congratulation, You've Completed Exercise 6\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"aP4DOKSs7uys","colab_type":"text"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}