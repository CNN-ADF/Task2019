{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNN2019 - 07 - TensorFlow.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"hPwFEN34mNUP","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sjlcHwermNUS","colab_type":"text"},"source":["\n","# Task 7 - TensorFlow\n","\n","\n","You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. \n","\n","You've also worked hard to make your code efficient and vectorized.\n","\n","For the this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, **TensorFlow**\n","\n","The goals of this assignment are as follows:\n","\n","    * Use TensorFlow at three different levels of abstraction,\n","    * Barebone TensorFlow: work directly with low-level TensorFlow graphs.\n","    * Keras Sequential API: use tf.keras.Sequential to define a linear feed-forward network.\n","    * Keras Model API: use tf.keras.Model to define arbitrary neural network architecture.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pMlC1Kwa9Dl0"},"source":["---\n","## About Tensorflow\n","\n","\n","<img src=\"https://www.gstatic.com/devrel-devsite/va3a0eb1ff00a004a87e2f93101f27917d794beecfd23556fc6d8627bba2ff3cf/tensorflow/images/lockup.svg\" alt=\"tensorflow\" width=\"300px\"/>\n","\n","[TensorFlow](https://www.tensorflow.org/) is a **Deep Learning Library**, developed by the Google Brain Team within the Google Machine Learning Intelligence research organization, for the purposes of machine learning and artificial neural network research.\n","\n","TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropogation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n","\n","\n","\n","**Tensorflow Key Features**\n","\n","* Define, optimize and efficiently calculate mathematical expressions involving multi-dimensional arrays (tensors).\n","* Programming support from deep neural networks and machine learning techniques.\n","* Use of GPU computing and automatic memory optimization.\n","* High computing capability across machines and large data sets.\n","\n","TensorFlow is available with Python and C ++ support, but the Python API is more supported and easier to learn.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zf0ePYsfVHvc","colab_type":"text"},"source":["---\n","## About Keras\n","\n","<img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" alt=\"keras\" width=\"300px\"/>\n","\n","[Keras](https://keras.io/) is a very modular and minimalist **Deep Learning Library**, written in Python and capable of running on TensorFlow or Theano. This library was developed with a focus on enabling fast experiments.\n","\n","At first Keras was developed to help users to easily use Theano and Tensorflow's which at the time was very technical and complex in implementation.\n","\n","Since Tensorflow version 1.5, Keras was adopted by Google and since then the built API has been included in the Tensorflow distribution."]},{"cell_type":"markdown","metadata":{"id":"MHosKd7SVR78","colab_type":"text"},"source":["---\n","\n","Working with Tensorflow will give us benefits:\n","\n","* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n","\n","* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n","\n","* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n","\n","* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "]},{"cell_type":"markdown","metadata":{"id":"QGarRj9tWulZ","colab_type":"text"},"source":["---\n","## GPU Runtime\n","Since we're going to use TensorFlow, we can utilize the GPU to accelerate the process\n","\n","For that, make sure that this Colaboratory file is set to use GPU\n","\n","* select **Runtime** in taskbar\n","* select **Change Runtime Type**\n","* choose Hardware accelerator **GPU**\n","\n","<center>\n","  \n","![gpu](https://i.ibb.co/QX3Brf0/gpu.png)\n"]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1","colab_type":"text"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"id":"12SMnaunBw96","colab_type":"text"},"source":["---\n","## 1 - Import Libraries\n","\n","Import requiered libraries"]},{"cell_type":"code","metadata":{"id":"hsZYqgngcZzY","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tabulate import tabulate\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","tf.logging.set_verbosity(tf.logging.WARN)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk","colab_type":"code","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv","colab_type":"text"},"source":["---\n","## 2 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw","colab_type":"code","colab":{}},"source":["(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz","colab_type":"text"},"source":["---\n","## 3 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","colab_type":"code","cellView":"both","colab":{}},"source":["X_val_ori = X_train_ori[-1000:,:]\n","y_val     = y_train[-1000:]\n","\n","X_train_ori = X_train_ori[:-1000, :]\n","y_train     = y_train[:-1000]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4","colab_type":"text"},"source":["---\n","## 4 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","colab_type":"code","cellView":"both","colab":{}},"source":["X_train = X_train_ori.astype('float32')\n","X_val   = X_val_ori.astype('float32')\n","X_test  = X_test_ori.astype('float32')\n","\n","mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n","std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n","\n","X_train = (X_train - mean_pixel) / std_pixel\n","X_val = (X_val - mean_pixel) / std_pixel\n","X_test = (X_test - mean_pixel) / std_pixel\n","\n","X_train = X_train.astype('float32')\n","X_val = X_val.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n","\n","y_train = y_train.ravel()\n","y_val   = y_val.ravel()\n","y_test  = y_test.ravel()\n","\n","print('\\ny_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdyboSVd4e0n","colab_type":"text"},"source":["---\n","## 5 - Dataset object\n","\n","For the Barebones TensorFlow that we will implament later, we need a mechanism to iterate the batch through the datasets.\n","\n","For our own convenience we'll define a lightweight `Dataset` class which lets us iterate over data and labels. \n","\n","This is not the most flexible or most efficient way to iterate through data, but it will serve our purposes."]},{"cell_type":"code","metadata":{"id":"kGoifbe04e0r","colab_type":"code","colab":{}},"source":["class Dataset(object):\n","    def __init__(self, X, y, batch_size, shuffle=False):\n","        \"\"\"\n","        Construct a Dataset object to iterate over data X and labels y\n","        \n","        Inputs:\n","        - X: Numpy array of data, of any shape\n","        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n","        - batch_size: Integer giving number of elements per minibatch\n","        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n","        \"\"\"\n","        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n","        self.X, self.y = X, y\n","        self.batch_size, self.shuffle = batch_size, shuffle\n","\n","    def __iter__(self):\n","        N, B = self.X.shape[0], self.batch_size\n","        idxs = np.arange(N)\n","        if self.shuffle:\n","            np.random.shuffle(idxs)\n","        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n","\n","\n","train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n","val_dset   = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n","test_dset  = Dataset(X_test, y_test, batch_size=64)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QHFeJLZ8X7T-","colab_type":"text"},"source":["---\n","---\n","# [Part 1] Barebones Three-Layer ConvNet\n","\n","Here you will complete the implementation of the function three_layer_convnet which will perform the forward pass of a three-layer convolutional network. \n","\n","\n","A three-layer convolutional network with the following architecture:\n","\n","<pre>\n","|             |             |                  |\n","| <font color='red'>conv</font> - <font color=''>relu</font> | <font color='red'>conv</font> - <font color=''>relu</font> | <font color='brown'>affine</font> - <font color=''>softmax</font> |\n","|             |             |                  |\n","|       1     |      2      |        3         |\n","</pre>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"RmNQ2a0UasSB","colab_type":"text"},"source":["---\n","## 1 - Forward Function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PKTnzE8cNdDJ"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Implement three_layer_convnet() function\n","\n","\n","**HINT**: For convolutions: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d; be careful with padding!\n","\n","**HINT**: For biases: https://www.tensorflow.org/performance/xla/broadcasting\n"]},{"cell_type":"code","metadata":{"id":"U_an9EjvYxaV","colab_type":"code","colab":{}},"source":["def three_layer_convnet(x, params):\n","    \"\"\"\n","    A three-layer convolutional network with the architecture described above.\n","    \n","    Inputs:\n","    - x     : A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images\n","    - params: A list of TensorFlow Tensors giving the weights and biases for the\n","              network; should contain the following:\n","      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving\n","                 weights for the first convolutional layer.\n","      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the\n","                 first convolutional layer.\n","      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)\n","                 giving weights for the second convolutional layer\n","      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the\n","                 second convolutional layer.\n","      - fc_w   : TensorFlow Tensor giving weights for the fully-connected layer.\n","      - fc_b   : TensorFlow Tensor giving biases for the fully-connected layer.\n","    \"\"\"\n","    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n","    scores = None\n","    \n","    # ------------------------------------------------\n","    # 1. Forward Pass\n","    # ------------------------------------------------\n","    \n","    # padding valid for first convolutional layer\n","    paddings = tf.constant([[0,0], [2,2], [2,2], [0,0]])    \n","    x = tf.pad(x, paddings, 'CONSTANT')\n","    \n","    # call tf.nn.conv2d function with input x, conv_w1, strides=[1, 1, 1, 1], and padding='valid'\n","    conv1 = ??\n","    \n","    # add conv1 with bias conv_b1\n","    conv1 = ??\n","    \n","    # call tf.nn.relu function with input conv1\n","    relu1 = ??\n","    \n","    # padding valid for second convolutional layer\n","    paddings = tf.constant([[0,0], [1,1], [1,1], [0,0]])\n","    relu1 = tf.pad(relu1, paddings, 'CONSTANT')\n","    \n","    # call tf.nn.conv2d function with input relu1, conv_w2, strides=[1, 1, 1, 1], and padding='valid'\n","    conv2 = ??\n","\n","    # add conv2 with bias conv_b2\n","    conv2 = ??\n","    \n","    # call tf.nn.relu function with input conv2\n","    relu2 = ??\n","    \n","    # get the output shape of second convolutional layer\n","    out_shape = tf.shape(relu2)[0]\n","    \n","    # flatten output by calling tf.reshape with input relu2 and (outshape, -1)\n","    relu2 = ??\n","    \n","    # calculate output score by performing affine layer calculation\n","    # call tf.matmul function with input relu2 and fc_w\n","    # then add with bias fc_b\n","    scores = ??\n","    \n","    \n","    return scores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHagrJH04e2i","colab_type":"text"},"source":["---\n","Check your implementations\n","\n","We use the `three_layer_convnet` function to set up the computational graph, then run the graph on a batch of zeros just to make sure the function doesn't crash, and produces outputs of the correct shape.\n","\n","When you run this function, `scores_np` should have shape `(64, 10)`."]},{"cell_type":"code","metadata":{"id":"tWYVhWDX4e2k","colab_type":"code","colab":{}},"source":["def three_layer_convnet_test():\n","    tf.reset_default_graph()\n","\n","    with tf.device('/device:GPU:0'):\n","        x = tf.placeholder(tf.float32)\n","        conv_w1 = tf.zeros((5, 5, 3, 6))\n","        conv_b1 = tf.zeros((6,))\n","        conv_w2 = tf.zeros((3, 3, 6, 9))\n","        conv_b2 = tf.zeros((9,))\n","        fc_w = tf.zeros((32 * 32 * 9, 10))\n","        fc_b = tf.zeros((10,))\n","        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n","        scores = three_layer_convnet(x, params)\n","\n","    # Inputs to convolutional layers are 4-dimensional arrays with shape\n","    # [batch_size, height, width, channels]\n","    x_np = np.zeros((64, 32, 32, 3))\n","    \n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        scores_np = sess.run(scores, feed_dict={x: x_np})\n","        print('scores_np has shape: ', scores_np.shape)\n","\n","with tf.device('/cpu:0'):\n","    three_layer_convnet_test()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ozGzjnABcZ0u","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," scores_np has shape:  (64, 10)"]},{"cell_type":"markdown","metadata":{"id":"8bfPN7_g4e2s","colab_type":"text"},"source":["---\n","## 2 - Training Step\n","\n","We now define the `training_step` function which sets up the part of the computational graph that performs a single training step. \n","\n","This will take three basic steps:\n","\n","1. Compute the loss\n","2. Compute the gradient of the loss with respect to all network weights\n","3. Make a weight update step using (stochastic) gradient descent.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6SUahghIck4j"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Implement training_step() function\n","\n","\n","We need to use a few new TensorFlow functions to do all of this:\n","- For computing the cross-entropy loss we'll use `tf.nn.sparse_softmax_cross_entropy_with_logits`: \n"," - https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n","\n","- For averaging the loss across a minibatch of data we'll use `tf.reduce_mean`:\n"," - https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n","\n","- For computing gradients of the loss with respect to the weights we'll use `tf.gradients`: \n"," - https://www.tensorflow.org/api_docs/python/tf/gradients\n","\n","- We'll mutate the weight values stored in a TensorFlow Tensor using `tf.assign_sub`: \n"," - https://www.tensorflow.org/api_docs/python/tf/assign_sub\n","\n","- We'll add a control dependency to the graph using `tf.control_dependencies`: \n"," - https://www.tensorflow.org/api_docs/python/tf/control_dependencies\n"]},{"cell_type":"code","metadata":{"id":"gvkVrbn44e2t","colab_type":"code","colab":{}},"source":["def training_step(scores, y, params, lr):\n","    \"\"\"\n","    Set up the part of the computational graph which makes a training step.\n","\n","    Inputs:\n","    - scores : TensorFlow Tensor of shape (N, C) giving classification scores\n","               for the model.\n","    - y      : TensorFlow Tensor of shape (N,) giving ground-truth labels for scores;\n","               y[i] == c means that c is the correct class for scores[i].\n","    - params : List of TensorFlow Tensors giving the weights of the model\n","    - lr     : Python scalar giving the learning rate to use for \n","               gradient descent step.\n","      \n","    Returns:\n","    - loss : A TensorFlow Tensor of shape () (scalar) giving the loss for this\n","             batch of data; evaluating the loss also performs a gradient descent \n","             step on params (see above).\n","    \"\"\"\n","    \n","    \n","    # ------------------------------------------------\n","    # 2. Calculate Loss\n","    # ------------------------------------------------\n","    # First compute the loss; the first line gives losses for each example in\n","    # the minibatch, and the second averages the losses acros the batch\n","    \n","    # call tf.nn.sparse_softmax_cross_entropy_with_logits function\n","    # with input labels=y and logits=scores\n","    losses = ??\n","    \n","    # call tf.reduce_mean function with input loses\n","    loss = ??\n","\n","    \n","    # ------------------------------------------------\n","    # 3. Backward Pass\n","    # ------------------------------------------------    \n","\n","    # Compute the gradient of the loss with respect to each parameter of the the\n","    # network. This is a very magical function call: TensorFlow internally\n","    # traverses the computational graph starting at loss backward to each element\n","    # of params, and uses backpropagation to figure out how to compute gradients;\n","    # it then adds new operations to the computational graph which compute the\n","    # requested gradients, and returns a list of TensorFlow Tensors that will\n","    # contain the requested gradients when evaluated.\n","    \n","    # call tf.gradients function with input loss and params\n","    grad_params = ??\n","    \n","    \n","    # ------------------------------------------------\n","    # 4. Weight Update\n","    # ------------------------------------------------  \n","    # Make a gradient descent step on all of the model parameters.\n","    new_weights = []   \n","    for w, grad_w in zip(params, grad_params):\n","        new_w = tf.assign_sub(w, lr * grad_w)\n","        new_weights.append(new_w)\n","\n","    # Insert a control dependency so that evaluting the loss causes a weight\n","    # update to happen; see the discussion above.\n","    with tf.control_dependencies(new_weights):\n","        return tf.identity(loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RApe6m4h4e22","colab_type":"text"},"source":["---\n","## 3 - Training Loop\n","Now we set up a basic training loop using low-level TensorFlow operations. We will train the model using stochastic gradient descent without momentum. The `training_step` function sets up the part of the computational graph that performs the training step, and the function `train_loop` iterates through the training data, making training steps on each minibatch, and periodically evaluates accuracy on the validation set."]},{"cell_type":"code","metadata":{"id":"zTd13fCU4e24","colab_type":"code","colab":{}},"source":["def train_loop(model_fn, init_fn, lr):\n","    \"\"\"\n","    Train a model on CIFAR-10.\n","    \n","    Inputs:\n","    - model_fn : A Python function that performs the forward pass of the model\n","                 using TensorFlow; it should have the following signature:\n","                 scores = model_fn(x, params) where x is a TensorFlow Tensor giving a\n","                 minibatch of image data, params is a list of TensorFlow Tensors holding\n","                 the model weights, and scores is a TensorFlow Tensor of shape (N, C)\n","                 giving scores for all elements of x.\n","    - init_fn  : A Python function that initializes the parameters of the model.\n","                 It should have the signature params = init_fn() where params is a list\n","                 of TensorFlow Tensors holding the (randomly initialized) weights of the\n","                 model.\n","    - lr       : Python float giving the learning rate to use for SGD.\n","    \"\"\"\n","    # First clear the default graph\n","    tf.reset_default_graph()\n","    is_training = tf.placeholder(tf.bool, name='is_training')\n","    \n","    # Set up the computational graph for performing forward and backward passes,\n","    # and weight updates.\n","    with tf.device('/device:GPU:0'):\n","        # Set up placeholders for the data and labels\n","        \n","        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n","        y = tf.placeholder(tf.int32, [None])\n","        \n","        # Initialize the model parameters\n","        params = init_fn()           \n","        \n","        # Forward pass of the model\n","        scores = model_fn(x, params) \n","        \n","        # Backward pass of the model\n","        loss = training_step(scores, y, params, lr)\n","\n","    # Now we actually run the graph many times using the training data\n","    with tf.Session() as sess:\n","        # Initialize variables that will live in the graph\n","        sess.run(tf.global_variables_initializer())\n","        for t, (x_np, y_np) in enumerate(train_dset):\n","            # Run the graph on a batch of training data; recall that asking\n","            # TensorFlow to evaluate loss will cause an SGD step to happen.\n","            feed_dict = {x: x_np, y: y_np}\n","            loss_np = sess.run(loss, feed_dict=feed_dict)\n","            \n","            # Periodically print the loss and check accuracy on the val set\n","            if t % 100 == 0:\n","                print('Iteration %d, loss = %.4f' % (t, loss_np))\n","                check_accuracy(sess, val_dset, x, scores, is_training)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0OoBUj34e2_","colab_type":"text"},"source":["---\n","## 4 - Check Accuracy\n","When training the model we will use the following function to check the accuracy of our model on the training or validation sets. Note that this function accepts a TensorFlow Session object as one of its arguments; this is needed since the function must actually run the computational graph many times on the data that it loads from the dataset `dset`.\n","\n","Also note that we reuse the same computational graph both for taking training steps and for evaluating the model; however since the `check_accuracy` function never evalutes the `loss` value in the computational graph, the part of the graph that updates the weights of the graph do not execute on the validation data."]},{"cell_type":"code","metadata":{"id":"rXSbMLLw4e3B","colab_type":"code","colab":{}},"source":["def check_accuracy(sess, dset, x, scores, is_training=None):\n","    \"\"\"\n","    Check accuracy on a classification model.\n","    \n","    Inputs:\n","    - sess  : A TensorFlow Session that will be used to run the graph\n","    - dset  : A Dataset object on which to check accuracy\n","    - x     : A TensorFlow placeholder Tensor where input images should be fed\n","    - scores: A TensorFlow Tensor representing the scores output from the\n","              model; this is the Tensor we will ask TensorFlow to evaluate.\n","      \n","    Returns: Nothing, but prints the accuracy of the model\n","    \"\"\"\n","    num_correct, num_samples = 0, 0\n","    for x_batch, y_batch in dset:\n","      \n","        feed_dict = {x: x_batch, is_training: 0}\n","        scores_np = sess.run(scores, feed_dict=feed_dict)\n","        \n","        y_pred = scores_np.argmax(axis=1)\n","        \n","        num_samples += x_batch.shape[0]\n","        num_correct += (y_pred == y_batch).sum()\n","        \n","    acc = float(num_correct) / num_samples\n","    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NKFtzx-z4e3L","colab_type":"text"},"source":["---\n","## 5 - Weight Initialization\n","We'll use the following utility method to initialize the weight matrices for our models using Kaiming's normalization method.\n","\n","[1] He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*, ICCV 2015, https://arxiv.org/abs/1502.01852"]},{"cell_type":"code","metadata":{"id":"MvzsrZvF4e3O","colab_type":"code","colab":{}},"source":["def kaiming_normal(shape):\n","    \"\"\"\n","    defining Kaiming He weight initialization\n","    \"\"\"\n","    \n","    if len(shape) == 2:\n","        fan_in, fan_out = shape[0], shape[1]\n","    elif len(shape) == 4:\n","        fan_in, fan_out = np.prod(shape[:3]), shape[3]\n","    return tf.random_normal(shape) * np.sqrt(2.0 / fan_in)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nRXk-L5-fLT9"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement the `three_layer_convnet_init` function to initialize the weights\n","    \n","Use network with the following architecture:\n","<pre>\n","|                    |                    |                  |\n","| <font color='red'>32 5x5 conv</font> - <font color=''>relu</font> | <font color='red'>16 3x3 conv</font> - <font color=''>relu</font> | <font color='brown'>affine</font> - <font color=''>softmax</font> |\n","|                    |                    |                  |\n","|          1         |         2          |        3         |\n","</pre>\n","<br>\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"uo6hGvHi4e3b","colab_type":"code","colab":{}},"source":["def three_layer_convnet_init():\n","    \"\"\"\n","    Initialize the weights of a Three-Layer ConvNet, for use with the\n","    three_layer_convnet function defined above.\n","    \n","    Inputs: None\n","    \n","    Returns a list containing:\n","    - conv_w1 : TensorFlow Variable giving weights for the first conv layer\n","    - conv_b1 : TensorFlow Variable giving biases for the first conv layer\n","    - conv_w2 : TensorFlow Variable giving weights for the second conv layer\n","    - conv_b2 : TensorFlow Variable giving biases for the second conv layer\n","    - fc_w    : TensorFlow Variable giving weights for the fully-connected layer\n","    - fc_b    : TensorFlow Variable giving biases for the fully-connected layer\n","    \"\"\"\n","    params = None\n","    \n","    # call tf.Variable function with input kaiming_normal function\n","    # the input shape for kaiming_normal function is the shape of the first kernel\n","    # the shape is [fw, fh, fc, fn]; which is [5, 5, 3, 32]\n","    conv_w1 = tf.Variable(kaiming_normal([??, ??, ??, ??]))\n","    \n","    # call tf.Variable with input np.zeros and dtype=tf.float32\n","    # the input shape for zeros function is the number of kernel of the first layer\n","    # which is 32\n","    conv_b1 = tf.Variable(np.zeros([??]), dtype=tf.float32)\n","    \n","    # call tf.Variable function with input kaiming_normal function\n","    # the input shape for kaiming_normal function is the shape of the second kernel\n","    # the shape is [fw, fh, fc, fn]; which is [3, 3, 32, 16]\n","    conv_w2 = ??\n","    \n","    # call tf.Variable with input np.zeros and dtype=tf.float32\n","    # the input shape for zeros function is the number of kernel of the second layer\n","    # which is 16\n","    conv_b2 = ??\n","    \n","    # call tf.Variable function with input kaiming_normal function\n","    # the input shape for kaiming_normal function is the shape of the affine layer\n","    # which is [32*32*16, 10]\n","    fc_w    = tf.Variable(kaiming_normal([32*32*16,10]))\n","    \n","    # call tf.Variable with input np.zeros and dtype=tf.float32\n","    # the input shape for zeros function is the number of affine output\n","    # which is 10\n","    fc_b    = ??\n","    \n","    \n","    # combine the weights\n","    params  = (conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b)\n","    \n","    return params"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aUhnuMh34e3Z","colab_type":"text"},"source":["---\n","## 6 - Train Model\n","\n","We will now use TensorFlow to train a three-layer ConvNet on CIFAR-10.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kGZsFwrZjsSG"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train a three-layer ConvNet\n","    \n","You don't need to do any hyperparameter tuning, but you should see accuracies above 43% after one epoch of training.\n"]},{"cell_type":"code","metadata":{"id":"qE3Z5XLIf9Me","colab_type":"code","colab":{}},"source":["learning_rate = 3e-3\n","train_loop(three_layer_convnet, three_layer_convnet_init, learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GHPgxrGhkRlA"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," the loss should start around 4 and end around 1.6\n"," with accuracy start around 10% and end around 48%"]},{"cell_type":"markdown","metadata":{"id":"PHOSFVyY4e3f","colab_type":"text"},"source":["---\n","---\n","# [Part 2] Keras Model API\n","Implementing a neural network using the low-level TensorFlow API is a good way to understand how TensorFlow works, but it's a little inconvenient - we had to manually keep track of all Tensors holding learnable parameters, and we had to use a control dependency to implement the gradient descent update step. \n","\n","This was fine for a small network, but could quickly become unweildy for a large complex model.\n","\n","Fortunately TensorFlow provides higher-level packages such as `tf.keras` which make it easy to build models out of modular, object-oriented layers\n","\n","In this part of the notebook we will define neural network models using the `tf.keras.Model` API. \n"]},{"cell_type":"markdown","metadata":{"id":"rQohd54TIn1x","colab_type":"text"},"source":["---\n","## 1 - One Hot Matrix\n","\n","When using Keras for multiclass classification, the first step that we have to do is convert the target into what is known as one-hot matrix. \n","\n","It change the target label into a sparse matrix with size of class number, with one in the index of the label and zeros in everywhere else\n","\n","With Keras, we can use **to_categorical** functions from **tf.keras.utils** module"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nHpftiWxTX-h"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Change target vector **y_train**, **y_val**, and **y_test** each into a One-Hot Matrix using **to_categorical** method"]},{"cell_type":"code","metadata":{"id":"neQU8KqDIn1_","colab_type":"code","colab":{}},"source":["# import the module\n","??\n","\n","\n","y_train_hot = ??\n","y_val_hot   = ??\n","y_test_hot  = ??\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g4oEUl-cTeRk","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"r3nr-z2OTgCA","colab_type":"code","colab":{}},"source":["print('y_train_hot.shape =',y_train_hot.shape)\n","print('y_val_hot.shape   =',y_val_hot.shape)\n","print('y_test_hot.shape  =',y_test_hot.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y8fSVlhsTj4s"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," y_train_hot.shape = (49000, 10)\n"," y_val_hot.shape   = (1000, 10)\n"," y_test_hot.shape  = (10000, 10)"]},{"cell_type":"markdown","metadata":{"id":"J_xTz69xIn2G","colab_type":"text"},"source":["example of 10 first one-hot label from training data"]},{"cell_type":"code","metadata":{"id":"Gl6iGzGgIn2I","colab_type":"code","colab":{}},"source":["print('         class:\\ni | y |  0 1 2 3 4 5 6 7 8 9\\n---------------------------------')\n","for i in range(10):\n","    print(i, '|', y_train[i], '|', y_train_hot[i,:].astype('int'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P4HHsdvdTk08"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","          class:\n"," i | y |  0 1 2 3 4 5 6 7 8 9\n"," ---------------------------------\n"," 0 | 6 | [0 0 0 0 0 0 1 0 0 0]\n"," 1 | 9 | [0 0 0 0 0 0 0 0 0 1]\n"," 2 | 9 | [0 0 0 0 0 0 0 0 0 1]\n"," 3 | 4 | [0 0 0 0 1 0 0 0 0 0]\n"," 4 | 1 | [0 1 0 0 0 0 0 0 0 0]\n"," 5 | 1 | [0 1 0 0 0 0 0 0 0 0]\n"," 6 | 2 | [0 0 1 0 0 0 0 0 0 0]\n"," 7 | 7 | [0 0 0 0 0 0 0 1 0 0]\n"," 8 | 8 | [0 0 0 0 0 0 0 0 1 0]\n"," 9 | 3 | [0 0 0 1 0 0 0 0 0 0]"]},{"cell_type":"markdown","metadata":{"id":"ZyT-JsZqWbhX","colab_type":"text"},"source":["---\n","## 2 - Two Layer Neural Net\n","\n","Keras is a Deep Learning API that was built as an independent open source project by more than 700 contributors. \n","\n","During its construction until, Keras is constantly updated so that there are often changes in the technical side of writing code. One of them is how to define a network model.\n","\n","There are two basic model building in Keras, using linear [**Sequential**](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model, or using more advance graphical [**Models**](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n"]},{"cell_type":"code","metadata":{"id":"MWObKeb8xa8I","colab_type":"code","colab":{}},"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras import Model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qxmOJgNJcpvq","colab_type":"text"},"source":["\n","For this example, let's create a **2 layer neural network** with **100 neurons** in its hidden layer\n","\n","There are four types of layers that we will use to build this model:\n","\n","     * Input layer      to receive input shape\n","     * Flatten layer    to reshape input into one-dimensional matrix for neural network\n","     * Dense layer      to add affine fully connected layer\n","     * Activation layer to add nonlinearity\n","     \n","For other types of layers, you can look it in **[tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)**"]},{"cell_type":"code","metadata":{"id":"kzXvQ69tcrO6","colab_type":"code","colab":{}},"source":["\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hmtjFMeOw9zR","colab_type":"text"},"source":["---\n","### a. Old Sequential API\n","\n","The first way to build a model using Keras, and one of the oldest ways, is to initialize the [**Sequential**](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model object, \n","\n","then one by one we add the layers that we want to stack as follows\n"]},{"cell_type":"code","metadata":{"id":"2iLF9zBoyIPb","colab_type":"code","colab":{}},"source":["model = Sequential(name='my_model_1')\n","model.add(Input((32,32,3)))         # input layer to receive image 32x32x3\n","model.add(Flatten())                # Flatten layer to reshape input into 3072x1\n","model.add(Dense(100))               # First affine layer (hidden layer) with 100 neurons\n","model.add(Activation('sigmoid'))    # Sigmoid activation function\n","model.add(Dense(10))                # Second affine layer (output layer) with 10 neuron\n","model.add(Activation('softmax'))    # Softmax activation function\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yiat15JzIM5u","colab_type":"text"},"source":["---\n","### b. Compact Sequential API\n","\n","The update on the Keras layer API allows us to add the input shape into the first layer directly.\n","It also allows up to add the activation function directly from the `Dense` layer without adding the` Activation` layer. \n"]},{"cell_type":"code","metadata":{"id":"K3oJwZl5IGmg","colab_type":"code","colab":{}},"source":["# create model new\n","model = Sequential(name='my_model_2')\n","model.add(Flatten(input_shape=(32,32,3)))    # Flatten layer to receive image 32x32x3 and reshape it into 3072x1\n","model.add(Dense(100, activation='sigmoid'))  # First affine layer (hidden layer) with 100 neurons and sigmoid activation\n","model.add(Dense(10,  activation='softmax'))  # Second affine layer (output layer) with 10 neuron and softmax activation\n","      \n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IZY1O_Jx_q52","colab_type":"text"},"source":["---\n","### c. Functional Model API\n","The third way to build models is to use **functional API** which allows us to build more complex model graphs, for example, having many input and output. After the graph is defined, group layers into an object using [**Models**](https://www.tensorflow.org/api_docs/python/tf/keras/Model) module\n","\n","The following is an example of building a model using **functional API**"]},{"cell_type":"code","metadata":{"id":"HNlMwWsI_q54","colab_type":"code","colab":{}},"source":["from tensorflow.keras import Model\n","\n","# create model graph\n","in_node  = Input(shape=(32,32,3))               # define input node to receive image 32x32x3\n","x        = Flatten() (in_node)                  # define x node as Flatten layer that receive input node\n","x        = Dense(100, activation='sigmoid')(x)  # pass x node to Dense hidden layer\n","out_node = Dense(10,  activation='softmax')(x)  # define output node as Dense layer that receive x node\n","\n","# initialize the model\n","model = Model(in_node, out_node, name='my_model_3')\n","\n","model.summary()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5BBi5KC_q56","colab_type":"text"},"source":["### d. Sequential API using Constructor\n","The fourth way is a new way of building a Sequential model. \n","\n","Similar to the first and second ways, but we can directly register the layers in the list when initializing the Sequential object as follows"]},{"cell_type":"code","metadata":{"id":"R0xzjgWQ_q56","colab_type":"code","colab":{}},"source":["# create model compact sequential\n","\n","model = Sequential([\n","  Flatten(input_shape=(32,32,3)),   # Flatten layer to receive image 32x32x3 and reshape it into 3072x1\n","  Dense(100, activation='sigmoid'), # First affine layer (hidden layer) with 100 neurons and sigmoid activation\n","  Dense(10,  activation='softmax')  # Second affine layer (output layer) with 10 neuron and softmax activation\n","], name='my_model_4')\n","\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJrNchsf_q58","colab_type":"text"},"source":["The four models defined above are the same model. You can see it from the summaries that all model has $308,310$ total parameters (weights)"]},{"cell_type":"markdown","metadata":{"id":"1VwdNli3yHlA","colab_type":"text"},"source":["---\n","## 3 - Compile Model\n","\n","Here we have to compile the model by registering a loss function and its optimization function"]},{"cell_type":"markdown","metadata":{"id":"_VGXaJ3DYPC8","colab_type":"text"},"source":["### a. Loss function\n","there are various types of **loss functions** for various cases such as:\n","* `categorical crossentropy` &nbsp; for multi-class classifications\n","* `binary crossentropy` &nbsp; for binary classification\n","* `mean squared error` &nbsp; for regression\n","* and many others\n","\n","see [tf.keras losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses) ufor more information\n"]},{"cell_type":"markdown","metadata":{"id":"IAzV6-xjYRrU","colab_type":"text"},"source":["---\n","### b. Optimization function\n","There are also various types of optimization functions such as:\n","* `sgd` &nbsp;for standard&nbsp; `stochastic gradient descent`, including &nbsp;`nesterov`\n","* `rmsprop` &nbsp;which is a further development of 'sgd'\n","* `adam` &nbsp;as a standard optimization function at this time\n","* and many others\n","\n","see [tf.keras optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) for more information"]},{"cell_type":"markdown","metadata":{"id":"nnS1Zr0gY8Fs","colab_type":"text"},"source":["---\n","\n","### c. Metrics\n","we can also add additional metrics to see model performance during training such as accuracy metrics"]},{"cell_type":"code","metadata":{"id":"V94Cyio-yOAH","colab_type":"code","colab":{}},"source":["# Compile model\n","model.compile(loss='categorical_crossentropy', \n","              optimizer='sgd', metrics=['accuracy'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YoOA7l_eZDCo","colab_type":"text"},"source":["---\n","## 4 - Train Model\n","\n","Now we can train the model by calling the &nbsp; `fit` &nbsp; function\n","\n","Run the training process for epoch=5 with batch size=100"]},{"cell_type":"code","metadata":{"id":"ZjxJaQ96yNvc","colab_type":"code","colab":{}},"source":["num_epochs = 5\n","batch_size = 100\n","\n","history = model.fit(X_train, y_train_hot, \n","                    validation_data=(X_val, y_val_hot),\n","                    epochs=num_epochs, \n","                    batch_size=batch_size, \n","                    verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_rqeYZVDeXkb"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," the loss should start around 1.9 and end around 1.7\n"," with training accuracy start around 31% and end around 41%"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UyaM5xaZerqS"},"source":["you can further train the model simply by re-run the cell with &nbsp; `fit` &nbsp; function"]},{"cell_type":"markdown","metadata":{"id":"vJ0U_NkcUZ9z","colab_type":"text"},"source":["---\n","## 5 - Visualize Training History\n","\n","Visualzie the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"QlJpKQBKUjRV","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.subplot(121)\n","# Plot training & validation accuracy values\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'])\n","\n","plt.subplot(122)\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ibLtwIAIn5V","colab_type":"text"},"source":["---\n","## 6 - Evaluate Model\n","Next, let's evaluate the accuracy of the models that have been trained\n","\n","we should get accuracy around `41%`"]},{"cell_type":"code","metadata":{"id":"VTD6WFU2In5W","colab_type":"code","colab":{}},"source":["scores = model.evaluate(X_test, y_test_hot, verbose=1)\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-IRHHBt48r8","colab_type":"text"},"source":["---\n","---\n","# [Part 3] Keras Three-Layer ConvNet\n","\n","For this part, let's recreate the 3-layer Convolutional Neural Net that we tried in part 1, but now using Keras Model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8EAjJVOV62wH","colab_type":"text"},"source":["---\n","## 1 - Define Model\n","\n","Define a three-layer convnet with the same architecture from Part 1.\n","\n","You can use any model building style that fits to your taste\n","\n","A three-layer convolutional network with the following architecture:\n","\n","<pre>\n","|                       |                       |         |                 |\n","| <font color='red'>32 @5x5 Conv2D</font> - <font color=''>relu</font> | <font color='red'>16 @3x3 Conv2D</font> - <font color=''>relu</font> | Flatten | <font color='brown'>Dense</font> - <font color=''>softmax</font> |\n","|                       |                       |         |                 |\n","|            1          |           2           |         |        3        |\n","</pre>\n","<br>\n","\n","For the **Convolution Layer**, use the **tensorflow.keras.layers.Conv2D**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rzTxlipd6z4k"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    define model:\n","    * 32 5x5 Conv2D layer with relu activation and padding valid\n","    * 16 3x3 Conv2D layer with relu activation and padding valid\n","    * Flatten layer\n","    * Dense layer with softmax activation\n","\n"]},{"cell_type":"code","metadata":{"id":"Bp22uBi3yNsk","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Conv2D\n","\n","myConv = ??\n","\n","myConv.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GqcK7CcuQ3to"},"source":["**Expected Output**:\n","<pre>\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_? (Conv2D)            (None, 28, 28, 32)        2432      \n","_________________________________________________________________\n","conv2d_? (Conv2D)            (None, 26, 26, 16)        4624      \n","_________________________________________________________________\n","flatten_? (Flatten)          (None, 10816)             0         \n","_________________________________________________________________\n","dense_? (Dense)              (None, 10)                108170    \n","=================================================================\n","Total params: 170,906\n","Trainable params: 170,906\n","Non-trainable params: 0"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aMgsfDzx86hd"},"source":["---\n","## 2 - Compile Model\n","\n","Here we have to compile the model by registering a loss function and its optimization function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fas8HCS186ho","colab":{}},"source":["# Compile model\n","myConv.compile(loss='categorical_crossentropy', \n","               optimizer='sgd', metrics=['accuracy'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ceVXKQw--AX4","colab_type":"text"},"source":["---\n","## 3 - Model Checkpoint Callback\n","\n","When using Keras, we can add several callback functions to the &nbsp; `.fit()` &nbsp; training function. \n","\n","A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. \n","\n","One of the very useful callback functions is **ModelCheckpoint**.\n","\n","You know that when we train neural networks, training accuracy and validation accuracy should always increase with epoch iteration. But at some point, while training accuracy continues to increase, validation accuracy may go down. Which indicates that the network has **overfit** the data. \n","\n","Obviously, the model that we want to use is not the model at the end of training epoch, but the one just before the validation accuracy drops.\n","\n","We can use the **ModelCheckpoint** to periodically save the model only when validation accuracy is increased"]},{"cell_type":"code","metadata":{"id":"J7U1u2eOBB8u","colab_type":"code","colab":{}},"source":["import os\n","os.mkdir('./model')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbdKCAjIAxkB","colab_type":"code","colab":{}},"source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","filepath = './model/my_model.h5'\n","\n","myCheckpoint = ModelCheckpoint(filepath, \n","                               monitor='val_acc',\n","                               save_best_only=True,\n","                              )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YYeyP2awCGz_","colab_type":"text"},"source":["---\n","## 4 - Early Stopping Callback\n","\n","Another useful callback function is **EarlyStopping**\n","\n","With this, we can monitor the training, and when the loss or accuracy is no longer improving for several epochs, we can terminate the training session and investigate what happened. \n","\n","After that, we can think of what we can do to improve training accuracy, then continue the training.\n","\n","For this example, we'll monitor the validation loss, and when the loss is not decreasing after 5 epochs, we terminate the training.\n","\n","Other callback functions can be seen at [tf.keras.callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks)\n"]},{"cell_type":"code","metadata":{"id":"xU_HHXL3DpmB","colab_type":"code","colab":{}},"source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","myStopping = EarlyStopping(monitor='val_loss',\n","                          patience=5\n","                          )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U2JoUp-2In5T","colab_type":"text"},"source":["---\n","## 5 - Live Training Plot Callback\n","\n","For this part, let's add a custom callback function to perform live training plot between training and validation accuracy\n"]},{"cell_type":"code","metadata":{"id":"o0ClR0iuIn5T","colab_type":"code","colab":{}},"source":["from IPython.display import clear_output\n","\n","\n","def translate_metric(x):\n","    translations = {'acc': \"Accuracy\", 'loss': \"Log-loss (cost function)\"}\n","    if x in translations:\n","        return translations[x]\n","    else:\n","        return x\n","\n","class PlotLosses(tf.keras.callbacks.Callback):\n","    def __init__(self, figsize=None):\n","        super(PlotLosses, self).__init__()\n","        self.figsize = figsize\n","\n","    def on_train_begin(self, logs={}):\n","\n","        self.base_metrics = [metric for metric in self.params['metrics'] if not metric.startswith('val_')]\n","        self.logs = []\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        self.logs.append(logs)\n","\n","        clear_output(wait=True)\n","        plt.figure(figsize=self.figsize)\n","        \n","        for metric_id, metric in enumerate(self.base_metrics):\n","            plt.subplot(1, len(self.base_metrics), metric_id + 1)\n","            \n","            plt.plot(range(1, len(self.logs) + 1),\n","                     [log[metric] for log in self.logs],\n","                     label=\"training\")\n","            if self.params['do_validation']:\n","                plt.plot(range(1, len(self.logs) + 1),\n","                         [log['val_' + metric] for log in self.logs],\n","                         label=\"validation\")\n","            plt.title(translate_metric(metric))\n","            plt.xlabel('epoch')\n","            plt.legend(loc='center right')\n","        \n","        plt.tight_layout()\n","        plt.show();\n","        \n","myTrainPlot = PlotLosses(figsize=(10, 4))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0x4A8MFY86hz"},"source":["---\n","## 6 - Train Model\n","\n","Now we can train the model by calling the &nbsp; `fit` &nbsp; function\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DZGeEp2DOaV3"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Train a three-layer ConvNet for epoch=25 with batch size=100\n","    "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gLXVILtZ86h1","colab":{}},"source":["num_epochs = 25\n","batch_size = 100\n","\n","history = myConv.fit(X_train, y_train_hot, \n","                     validation_data=(X_val, y_val_hot),\n","                     epochs=num_epochs, \n","                     batch_size=batch_size, \n","                     callbacks=[myTrainPlot, myCheckpoint, myStopping],\n","                     verbose=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4sDZ6RCK86h6"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training should lasts in about 20 epochs\n","the training loss should start around 1.8 and end around 0.5\n","while validation loss start around 1.5 then plateau around 1.1"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7hHvxzOF9jlq"},"source":["---\n","## 7 - Evaluate Model\n","Next, let's evaluate the accuracy of the models that have been trained\n","\n","First, load the best model from checkpoint"]},{"cell_type":"code","metadata":{"id":"L26NQQx8NKLg","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import load_model\n","\n","myConv = load_model('./model/my_model.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"92FsWJQsNZzn","colab_type":"text"},"source":["Evaluate the model using &nbsp; `X_test` &nbsp; and &nbsp; `y_test_hot`"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0EPBwyyB9jl1","colab":{}},"source":["scores = myConv.evaluate(X_test, y_test_hot, verbose=1)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"coc1tJzANl6e"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","you should get around 63% of testing accuracy"]},{"cell_type":"markdown","metadata":{"id":"5JECsUGsN94_","colab_type":"text"},"source":["---\n","---\n","# [Part 4] CIFAR-10 Open-ended Challenge\n","\n","In this section you can experiment with whatever ConvNet architecture you'd like on CIFAR-10.\n","\n","You should experiment with **architectures**, **hyperparameters**, **loss functions**, **regularization**, or anything else you can think of to train a model \n","\n","You should achieve <font color='blue' size='5'><b>at least 75% accuracy</b></font> on the **validation** set <font color='red' size='4'><b>within 10-20 epochs</b></font>. \n"]},{"cell_type":"markdown","metadata":{"id":"qbYFGiXWOPX4","colab_type":"text"},"source":["---\n","## Some things you can try:\n","- **Filter size**: Above we used 5x5 and 3x3; is this optimal?\n","\n","- **Number of filters**: Above we used 16 and 32 filters. Would more or fewer do better?\n","\n","- **Pooling**: We didn't use any pooling above. Would this improve the model?\n","\n","- **Normalization**: Would your model be improved with batch normalization, layer normalization, group normalization, or some other normalization strategy?\n","\n","- **Network architecture**: The ConvNet above has only three layers of trainable parameters. Would a deeper model do better?\n","\n","- **Global average pooling**: Instead of flattening after the final convolutional layer, would global average pooling do better? This strategy is used for example in Google's Inception network and in Residual Networks.\n","\n","- **Regularization**: Would some kind of regularization improve performance? Maybe weight decay or dropout?\n","\n","- **Optimization**: You've seen various advanced optimization function. Maybe changing the optimization using Adam or RMSProp will increase the accuracy?\n","\n","<br><center>\n","<font color='red' size='4'><b>--- You must design YOUR OWN Architecture --- <br>\n","--- And train it from scratch --- </b></font>"]},{"cell_type":"markdown","metadata":{"id":"0eYZkoAQO-S2","colab_type":"text"},"source":["---\n","## Tips for training\n","For each network architecture that you try, you should tune the learning rate and other hyperparameters. \n","\n","When doing this there are a couple important things to keep in mind:\n","\n","- If the parameters are working well, you should see improvement within a few hundred iterations\n","\n","- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n","\n","- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n","\n","- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set."]},{"cell_type":"markdown","metadata":{"id":"CM0a9PvRPhqa","colab_type":"text"},"source":["<center>\n","<h2><font color='blue'>--- Go Wild, Have Fun, and Happy Training!  --- </font></h2>"]},{"cell_type":"markdown","metadata":{"id":"AW4Ees_zVLBJ","colab_type":"text"},"source":["---\n","## 1 - Define Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rq_8xfRAQIKO"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Design your Convolutional Neural Network Architecture\n","\n","    "]},{"cell_type":"code","metadata":{"id":"euoKYCFcQNOf","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import *\n","\n","myModel = ??\n","\n","\n","myModel.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jJ9XUD5iVP26"},"source":["---\n","## 2 - Train Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OwRct_FtVP3A"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Compile the model\n","    Train the model\n","    "]},{"cell_type":"code","metadata":{"id":"lQCmkzyXSAeg","colab_type":"code","colab":{}},"source":["# Compile model\n","myModel.compile(??)\n","\n","num_epochs = ??\n","batch_size = ??\n","\n","history = myModel.fit(??)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y8bwtzimVUfs"},"source":["---\n","## 3 - Evaluate Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SOzUKWj5Q_La"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    evaluate your model on test set\n","    "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QZMNJP7hQ-G5","colab":{}},"source":["myModel = load_model(??)\n","\n","train_scores = myModel.evaluate(X_train, y_train_hot, verbose=1)\n","val_scores   = myModel.evaluate(X_val, y_val_hot, verbose=1)\n","test_scores  = myModel.evaluate(X_test, y_test_hot, verbose=1)\n","\n","print(\"\\nTraining Accuracy:   %.2f%%\" % (train_scores[1]*100))\n","print(\"Validation Accuracy: %.2f%%\" % (val_scores[1]*100))\n","print(\"Testing Accuracy:    %.2f%%\" % (test_scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Q2Lgw7gSwqD"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","you should get above 80% of accuracy for train, val, and test set"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AzjAfyGhVkPK"},"source":["---\n","## 4 - Test Model on New Image\n","\n","For this part, you have to test your model on new image\n","\n","First of all, search for five images on the Internet, then list the URLs to the code below.\n","\n","The five images must belong to the 10 CIFAR-10 classes that the model recognizes."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pnU03-WEVkPW"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    define five image urls\n","    one image has been given for an example, you can change it"]},{"cell_type":"code","metadata":{"id":"CkFlqQoIF0YD","colab_type":"code","colab":{}},"source":["!wget -q -O 'data_test_0.jpg' 'https://ichef.bbci.co.uk/news/912/cpsprodpb/160B4/production/_103229209_horsea.png'\n","!wget -q -O 'data_test_1.jpg' '??'\n","!wget -q -O 'data_test_2.jpg' '??'\n","!wget -q -O 'data_test_3.jpg' '??'\n","!wget -q -O 'data_test_4.jpg' '??'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j5wH83YDXj0a","colab_type":"text"},"source":["Run and Recognize the images"]},{"cell_type":"code","metadata":{"id":"OBaJnd2gF6X_","colab_type":"code","colab":{}},"source":["import cv2 as cv\n","from PIL import Image\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","\n","for i in range(5):\n","  new_img = Image.open('data_test_'+str(i)+'.jpg')\n","  new_img = np.array(new_img)\n","  new_img2 = cv.resize(new_img, (32,32), interpolation=cv.INTER_AREA)\n","  plt.imshow(new_img2)\n","  plt.axis('off')\n","  plt.show()\n","\n","  new_img2 = (new_img2 - mean_pixel) / std_pixel\n","  pred = myModel.predict(new_img2)\n","  class_id = np.argmax(pred)\n","  print('predicted id   :',class_id)\n","  print('predicted class:', class_names[class_id])\n","  print('--------------------------------\\n\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 7\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bvBk-u7_92ub"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}