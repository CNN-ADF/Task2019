{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"CNN2019 - 13.2 - Network Visualization (TensorFlow).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tKkCjEtlhjT5","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fnrUNY5Er7UE","colab_type":"text"},"source":["# Task 13 part 2 - Network Visualization (TensorFlow)\n","\n","A convolutional neural network typically has multiple convolutional layers (hence, the name).\n","\n","Conceptually, we understand each convolutional layer extracts spatial features from their inputs.  Earlier layer detects low-level features like color, texture, lines, curves, etc.  Later layers detect higher abstraction like eyes, tail, etc.\n","\n","\n","In this notebook, we use a pre-trained convolutional neural network to see what kind of input images strongly activate filters in convolutional layers.\n","\n","This notebook code is largely based on the blog article **How convolutional neural networks see the world** by **Francois Chollet** [2]."]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1","colab_type":"text"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"id":"3iIKvXgjjCn-","colab_type":"text"},"source":["---\n","## 1 - Install TensorFlow 2\n","\n","If Tensorflow 2 is not already installed, install it first"]},{"cell_type":"code","metadata":{"id":"VitP-OLAjCGz","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu -q"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOPPPUZXi5EX","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","tf.__version__"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aIpLk-Iej1RC"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," '2.0.0'"]},{"cell_type":"markdown","metadata":{"id":"DvPSXMEIaFm1","colab_type":"text"},"source":["---\n","## 2 - Import Libraries\n","Import required libraries"]},{"cell_type":"code","metadata":{"id":"Z389HkeEvQ6J","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow.keras.backend as K\n","tf.compat.v1.disable_eager_execution()\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import scipy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grVDHPIghjT9","colab_type":"text"},"source":["---\n","---\n","# [Part 1] Load Pretrained Model\n","\n","We could use any other convolutional neural network, but here we use the pre-trained VGG16 available in Keras.  \n"]},{"cell_type":"markdown","metadata":{"id":"-Ph4CMkhhjUI","colab_type":"text"},"source":["---\n","## 1 - Load VGG16 Model\n","The following shows the available convolutional layers in VGG16."]},{"cell_type":"code","metadata":{"id":"b44bj6kahjUA","colab_type":"code","colab":{}},"source":["from tensorflow.keras.applications.vgg16 import VGG16\n","model = VGG16(weights='imagenet', include_top=False)\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cMt-NcuehjUX","colab_type":"text"},"source":["---\n","## 2 - Helper Function\n","\n","\n","Typically, the input to the VGG16 model is an image to classify such as cats and dogs. \n","\n","Here, however, we use a randomly generated noise image and feed it to VGG16 model to calculate filter activations and their gradients."]},{"cell_type":"markdown","metadata":{"id":"gQAMWpgChjUZ","colab_type":"text"},"source":["---\n","### a. Random Image Generator\n","\n","We start the visualization from a random image"]},{"cell_type":"code","metadata":{"id":"nGIUI-A8hjUa","colab_type":"code","colab":{}},"source":["def make_random_image(img_height=128, img_width=128, mean=127, std=10):\n","    return np.random.normal(loc=mean, scale=std, size=(img_height, img_width, 3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKOKaNrChjUf","colab_type":"code","colab":{}},"source":["input_img = make_random_image()\n","\n","plt.imshow(input_img.astype('uint8'))\n","plt.xticks([])\n","plt.yticks([])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8BfjEFebhjUk","colab_type":"text"},"source":["---\n","### b. Get Layer by Name\n","\n","function to find a layer object given a model and layer name "]},{"cell_type":"code","metadata":{"id":"5ntW99-dhjUm","colab_type":"code","colab":{}},"source":["def find_layer(model, layer_name):\n","    for layer in model.layers:\n","        if layer.name == layer_name:\n","            return layer\n","    return None"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2ZMgLPNhjUp","colab_type":"text"},"source":["---\n","### c. Convert to Image\n","\n","Function to convert result data into 0-255 image data"]},{"cell_type":"code","metadata":{"id":"E7QbeskbhjUq","colab_type":"code","colab":{}},"source":["def convert_to_image(x):\n","    # normalize data\n","    # set the std to 0.1 and the mean to 0.5\n","    x -= x.mean()\n","    x /= (x.std() + 1e-5)\n","    x *= 0.1\n","    \n","    # clip to [0, 1]\n","    x += 0.5\n","    x = np.clip(x, 0, 1)\n","    \n","    \n","    # convert to RGB array\n","    x *= 255\n","    x = np.clip(x, 0, 255).astype('uint8')\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"um0vdsxQhjUv","colab_type":"text"},"source":["---\n","---\n","\n","# [Part 2] Filter Visualization\n"]},{"cell_type":"markdown","metadata":{"id":"wPHODRBFxbB6","colab_type":"text"},"source":["---\n","## 1 - Layer Visualization Function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xc2RyhhXUYMU"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement this function. \n","* Perform gradient ascent using keras backend to generate filter visualization\n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"-QLVUxfchjUw","colab_type":"code","colab":{}},"source":["def visualize_layer(model, layer_name, filter_index, input_img, steps=50, step_size=1.0):\n","    \n","    # get layer from model based on layer_name \n","    # by calling find_layer() function with input model and layer_name\n","    layer = ??\n","    \n","    # maximize the mean activation of the filter of the layer\n","    activation = K.mean(layer.output[:, :, :, filter_index])\n","    \n","    # backward pass\n","    # calculate gradients of the activations of the filter of the layer\n","    # call K.gradients() function with input activation and model.input\n","    grads = ??\n","\n","    \n","    # get the gradient value\n","    grads = grads[0]\n","    \n","    # normalize the gradients to avoid very small/large gradients\n","    # normalized gradients is the old gradient divided by Root-Mean-Square of the gradients\n","    # add small number (e.g. 1e-5) in the division to avoid division by zero\n","    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n","    \n","    # create function to calculate the mean activation \n","    # and the gradients which depend on the mean activation\n","    step_function  = K.function([model.input], [activation, grads])\n","        \n","    # adjust input image suitable for the calculate function\n","    input_img = np.copy(input_img)                         # make a copy to preserve the original\n","    input_img = np.float64(input_img)                      # make sure it's float type\n","    input_data = input_img.reshape((1, *input_img.shape))  # reshape to one record image data\n","\n","    # maximize the activation using the gradient ascent\n","    # (nudge the image data with the gradients)\n","    for i in range(steps):\n","        _, grads_value = step_function([input_data])\n","        input_data += grads_value * step_size\n","    result = input_data[0]\n","    \n","    # convert the result into image using convert_to_image() function\n","    image_result = ??\n","    \n","    \n","    return image_result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IhG9f4t7xv4V","colab_type":"text"},"source":["---\n","## 2 - Visualize a Filter"]},{"cell_type":"markdown","metadata":{"id":"ebm_OtCHhjU1","colab_type":"text"},"source":["The following is an example of an input image that strongly activates the first filter in the layer `block4_conv1`.\n","\n","You can change the `layer_name` and `filter_index`\n"]},{"cell_type":"code","metadata":{"id":"t9Ux-m9OhjU2","colab_type":"code","colab":{}},"source":["layer_name   = 'block4_conv1'\n","filter_index = 0\n","input_img    = make_random_image()\n","\n","\n","result = visualize_layer(model, \n","                         layer_name=layer_name, \n","                         filter_index=filter_index, \n","                         input_img=input_img)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E5q-CQsDyAOV","colab_type":"text"},"source":["Visualize the filter"]},{"cell_type":"code","metadata":{"id":"8rtcTIPNyC2V","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(15,5))\n","plt.imshow(result)\n","plt.xticks([])\n","plt.yticks([])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40YOWy4MhjU6","colab_type":"text"},"source":["---\n","## 3 - Visualize 20 Filters\n","Let's examine some filters in a layer. In the function below, we're going to select 20 filters randomly, and view it's visualization"]},{"cell_type":"code","metadata":{"id":"i7zoKDAhhjU7","colab_type":"code","colab":{}},"source":["def show_filters(model, layer_name, steps=80):\n","  \n","    input_img = make_random_image()\n","    print('Showing 20 random filters for layer', layer_name)\n","    plt.figure(figsize=(16,16))\n","    num_filter = find_layer(model, 'block5_conv3').output.shape[3]\n","    \n","    for i in range(20):\n","        idx = np.random.randint(0,num_filter)\n","        result = visualize_layer(model, layer_name, filter_index=idx, input_img=input_img, steps=steps)    \n","        plt.subplot(4, 5, i+1)\n","        plt.imshow(result)\n","        plt.gca().set_title(idx)\n","        plt.xticks([])\n","        plt.yticks([])\n","        \n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bjlwFWru02OJ"},"source":["Visualize the filter"]},{"cell_type":"code","metadata":{"id":"05gT7jB1hjU-","colab_type":"code","colab":{}},"source":["layer_name   = 'block4_conv3'\n","\n","show_filters(model, layer_name, steps=80)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xPouEDLz5cRF","colab_type":"text"},"source":["---\n","---\n","# [Part 3] VGG Face Visualization\n","\n","Now for comparison, Here you are provided a vgg model that has been trained on face dataset.\n","\n","The model was trained to perform various task from face detection to face recognition\n","\n","So it made sense that the filters will form facial attributes"]},{"cell_type":"markdown","metadata":{"id":"kMmGdflshjUR","colab_type":"text"},"source":["---\n","## 1 - Load VGG Face\n","\n","Download and load the model provided\n","\n"]},{"cell_type":"code","metadata":{"id":"bRAsF8EP581G","colab_type":"code","colab":{}},"source":["!wget -O 'vgg_face_notop.h5' 'https://github.com/CNN-ADF/Task2019/raw/master/resources/vgg_face_notop.h5' -q"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Zht-4eDhjUT","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import load_model\n","\n","face_model = load_model('vgg_face_notop.h5')\n","\n","face_model.summary()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tBScmVmV9tfi"},"source":["---\n","## 2 - Visualize 20 Filters\n","Let's examine some filters in a layer. In the function below, we're going to select 20 filters randomly, and view it's visualization\n","\n","WARNING,<br>\n","you may see some disturbing features \t(￣ヘ￣)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b7_kXM9L9wvW","colab":{}},"source":["layer_name   = 'block5_conv2'\n","\n","show_filters(face_model, layer_name, steps=80)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXPoylwyhjVD","colab_type":"text"},"source":["---\n","---\n","# [Part 4] Little Deep Dreams\n","\n","If we throw a Sky image into this experiment and let a filter to nudge the image, what do we get?\n","\n","This is a similar idea used in **Inceptionism: Going Deeper into Neural Networks** [3]."]},{"cell_type":"markdown","metadata":{"id":"qR_RBkXr1q2h","colab_type":"text"},"source":["---\n","## 1 - Load Image"]},{"cell_type":"code","metadata":{"id":"ffCMlzDmkdby","colab_type":"code","colab":{}},"source":["!wget -O 'sky1024px.jpg' 'https://images.squarespace-cdn.com/content/57c8515459cc68fba78d9781/1494175003092-5PQRBFQHQ31HXEWW9O07/sky1024px.jpg' -q\n","\n","\n","sky_img = plt.imread('sky1024px.jpg') \n","plt.imshow(sky_img)\n","plt.axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GotX4AW21tzh","colab_type":"text"},"source":["---\n","## 2 - Hallucinate the Image\n","\n","Now we perform the filter visualization to the image.\n","\n","It will, in sense, hallucinate the appearance of the selected filters inside the image"]},{"cell_type":"code","metadata":{"id":"DKnmGhGNhjVE","colab_type":"code","colab":{}},"source":["layer_name='block5_conv2'\n","filter_index=15\n","\n","result = visualize_layer(model, \n","                         layer_name=layer_name,\n","                         filter_index=filter_index, \n","                         input_img=sky_img, \n","                         steps=100)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PIZ5T02L1nKb"},"source":["Visualize the result"]},{"cell_type":"code","metadata":{"id":"Zl04BWvH1lCB","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(18,8))\n","plt.subplot(121)\n","plt.imshow(sky_img)\n","plt.axis('off')\n","plt.subplot(122)\n","plt.imshow(result)\n","plt.axis('off')\n","plt.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_eWsPrN1_A9j"},"source":["---\n","## 3 - Hallucinate the Image using VGG Face\n","\n","Now let's try to hallucinate the image using VGG Face Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n1Ru4feM_A9q","colab":{}},"source":["layer_name='block5_conv2'\n","filter_index=153\n","\n","result = visualize_layer(face_model, \n","                         layer_name=layer_name,\n","                         filter_index=filter_index, \n","                         input_img=sky_img, \n","                         steps=100)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VcLLGUAA_A90"},"source":["Visualize the result"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FjcEZVvS_A92","colab":{}},"source":["plt.figure(figsize=(18,8))\n","plt.subplot(121)\n","plt.imshow(sky_img)\n","plt.axis('off')\n","plt.subplot(122)\n","plt.imshow(result)\n","plt.axis('off')\n","plt.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9WUaDGqlhjVH","colab_type":"text"},"source":["---\n","---\n","# [Part 4] Deep Dreams using Inception V3\n","\n","To build a better Deep Dream Image, let's use InceptionV3 model for this exercise\n"]},{"cell_type":"markdown","metadata":{"id":"-9K2DE0Y_o3Y","colab_type":"text"},"source":["---\n","## 1 - Load Inception Model\n","\n","Here we load InceptionV3 model that was trained on Imagenet"]},{"cell_type":"code","metadata":{"id":"0R9nciFJhjVI","colab_type":"code","colab":{}},"source":["from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array\n","\n","model = InceptionV3(weights='imagenet',include_top=False)\n","\n","# model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-n0blthUhjVL","colab_type":"text"},"source":["---\n","## 2 - Helper Function\n","Notice that using previous helper function, the image result became darker\n","\n","To clear the image, we prepare another helper function\n","\n","Deep Dream function from google achieve better result by hallucinating the image in different scaling size with multiple filters. For that we prepare resize function for the image"]},{"cell_type":"code","metadata":{"id":"y3Z7l3ojhjVM","colab_type":"code","colab":{}},"source":["def preprocess_image(image_path):\n","    # Util function to open, resize and format pictures\n","    # into appropriate tensors.\n","    img = load_img(image_path)\n","    img = img_to_array(img)\n","    img = np.expand_dims(img, axis=0)\n","    img = preprocess_input(img)\n","    return img\n","\n","def resize_img(img, size):\n","    img = np.copy(img)\n","    factors = (1,\n","               float(size[0]) / img.shape[1],\n","               float(size[1]) / img.shape[2],\n","               1)\n","    return scipy.ndimage.zoom(img, factors, order=1)\n","\n","def deprocess_image(x):\n","    x = x.reshape((x.shape[1], x.shape[2], 3))\n","    x /= 2.\n","    x += 0.5\n","    x *= 255.\n","    x = np.clip(x, 0, 255).astype('uint8')\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JnQ0OtLNhjVQ","colab_type":"text"},"source":["---\n","## 3 - Feature List\n","\n","Now, define list of filter that will be used to hallucinate the image\n","\n","You can change or add the feature and coefficient to use. \n","\n","See the list of layer name in `summary()` method"]},{"cell_type":"code","metadata":{"id":"U33jRFeChjVR","colab_type":"code","colab":{}},"source":["dream_filter = {\n","    'features': {\n","        'mixed2' : 0.2,\n","        'mixed4' : 1.5,\n","        'mixed5' : 1.0,\n","        'mixed8' : 3.3,\n","        'mixed10': 2.5,\n","    },\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ebN2peXLhjVU","colab_type":"text"},"source":["---\n","## 4 - Deep Dream Function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nmFRj6xwAJH3"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Complete this function. \n"]},{"cell_type":"code","metadata":{"id":"s9FC6b1YhjVV","colab_type":"code","colab":{}},"source":["def deep_dream(model, input_img_path, steps=20, step_size=0.01):\n","    \n","    K.set_learning_phase(0)\n","\n","    img = preprocess_image(input_img_path)\n","    original_shape = img.shape[1:3]\n","    successive_shapes = [original_shape]\n","    \n","    loss = 0\n","    \n","    for layer_name in dream_filter['features']:\n","    \n","        # get layer object from model based on layer_name \n","        # by calling find_layer() function with input layer_name\n","        layer = ??\n","        \n","        # get coefficient for features in each layer\n","        coeff = dream_filter['features'][layer_name]\n","        \n","        # define scaling for loss function\n","        scaling = K.prod(K.cast(K.shape(layer.output), 'float32'))\n","        \n","        # define loss function\n","        loss += coeff * K.sum(K.square(layer.output[:, 2: -2, 2: -2, :])) / scaling\n","     \n","    # backward pass\n","    # calculate gradients of the activations of the filter of the layer\n","    # call K.gradients() function with input loss and model.input\n","    grads = ??\n","    \n","    # get the gradient value\n","    grads = grads[0]\n","    \n","    # normalize the gradients to avoid very small/large gradients\n","    grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon())\n","    \n","    # create function to calculate total loss\n","    step_function  = K.function([model.input], [loss, grads])\n","        \n","    \n","    # prepare setting for scaling process\n","    num_octave = 5           # Number of scales at which to run gradient ascent\n","    octave_scale = 1.5       # Size ratio between scales    \n","    for i in range(1, num_octave):\n","        shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n","        successive_shapes.append(shape)   \n","    successive_shapes = successive_shapes[::-1]\n","    original_img = np.copy(img)\n","    shrunk_original_img = resize_img(img, successive_shapes[0])   \n","    \n","    # perform Deep Dream in multiple image scale\n","    for shape in successive_shapes:        \n","        print('Processing image shape', shape)\n","        \n","        # resize image by calling resize_img() function with input img and shape\n","        img = ??\n","        \n","        # perform gradient ascent\n","        for i in range(steps):\n","            _, grad_value = step_function([img])\n","            img += grad_value * step_size\n","        \n","        # resize back image\n","        upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n","        same_size_original = resize_img(original_img, shape)\n","        lost_detail = same_size_original - upscaled_shrunk_original_img\n","\n","        # hallucinate the image by adding the lost_detail to img\n","        img = ??\n","        \n","        \n","        # resize original image by calling resize_img() function \n","        # with input original_img and shape\n","        shrunk_original_img = resize_img(original_img, shape)\n","        \n","    \n","    # convert the result into image by calling deprocess_image() function with input img\n","    # image_result = deprocess_image(np.copy(img))\n","    image_result = deprocess_image(img)    \n","    \n","    return image_result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b1TqYLv0Bjo4","colab_type":"text"},"source":["---\n","## 5 - Perform Deep Dream"]},{"cell_type":"code","metadata":{"id":"qrazniT8hjVZ","colab_type":"code","colab":{}},"source":["result = deep_dream(model, 'sky1024px.jpg', steps=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4-MgJs_BvqA","colab_type":"text"},"source":["Visualize the result"]},{"cell_type":"code","metadata":{"id":"m3OGNx_aBvOT","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(18,8))\n","plt.subplot(121)\n","plt.imshow(sky_img)\n","plt.axis('off')\n","\n","plt.subplot(122)\n","plt.imshow(result)\n","plt.axis('off')\n","plt.tight_layout()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuujlTQYhhVW","colab_type":"text"},"source":["---\n","---\n","\n","# Congratulation, You've Completed Exercise 13 part 2"]},{"cell_type":"markdown","metadata":{"id":"r_70FMtEhjVc","colab_type":"text"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}