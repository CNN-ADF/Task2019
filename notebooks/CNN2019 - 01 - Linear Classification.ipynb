{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"281px"},"toc_section_display":true,"toc_window_display":true},"colab":{"name":"CNN2019 - 01 - Linear Classification.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"Nde_vGtp9Dlr","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"pqfEjzA09Dlx","colab_type":"text"},"source":["# Simple Linear Classification\n","\n","In this assignment you will practice putting together a simple image classification pipeline, based on the SVM and Softmax classifier. \n","\n","The goals of this assignment are as follows:\n","* understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)\n","* understand the train/val/test splits and the use of validation data for hyperparameter tuning.\n","* develop proficiency in writing efficient vectorized code with numpy\n","\n","* implement and apply a Softmax classifier\n","* implement and apply a Multiclass Support Vector Machine (SVM) classifier\n","\n","* understand the differences and tradeoffs between these classifiers"]},{"cell_type":"markdown","metadata":{"id":"pMlC1Kwa9Dl0","colab_type":"text"},"source":["---\n","## 0 - Import necessary libraries and informations"]},{"cell_type":"code","metadata":{"id":"SWJMf_Kx9Dl3","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","np.set_printoptions(precision=8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M8molz1J9Dl-","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"CkzQo8MY9DmB","colab_type":"code","colab":{}},"source":["NIM = 123456\n","Nama = \"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNXQVKWr9DmH","colab_type":"text"},"source":["---\n","## 1 - Load CIFAR-10 Dataset\n","\n","* First, Obtain Cifar-10 dataset.\n","  There are from various source in Internet like [Keras](https://keras.io/datasets/), [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/datasets), or any other source\n","* Next you will prepare the dataset by first:\n"," * visualizing data\n"," * split into training, validation, and testing set\n"," * normalize data"]},{"cell_type":"markdown","metadata":{"id":"FJ4WkMiR9DmI","colab_type":"text"},"source":["---\n","### a. Import Data ***CIFAR-10***\n","<br>\n","\n","**EXERCISE**: \n","* `load Cifar-10 dataaset.`\n","* `It should consists of 50000 32x32 images for training and 10000 of the same shape for testing.`"]},{"cell_type":"code","metadata":{"id":"tsjp0Rou9DmL","colab_type":"code","colab":{}},"source":["(X_train, y_train), (X_test, y_test) = ??\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'forse', 'ship', 'truck']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_drJAPL9DmR","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"0xkyOKsX9DmV","colab_type":"code","colab":{}},"source":["print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)\n","print('X_test.shape  =',X_test.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SFmi0aMw9Dmb","colab_type":"text"},"source":["**Expected Output**: \n","<pre>\n","X_train.shape = (50000, 32, 32, 3)\n","y_train.shape = (50000, 1)\n","X_test.shape  = (10000, 32, 32, 3)\n","y_test.shape  = (10000, 1)\n"]},{"cell_type":"markdown","metadata":{"id":"6YNXtE039Dmd","colab_type":"text"},"source":["---\n","### b. Visualizing Data\n","\n","\n","Show the first 20 images from X_train"]},{"cell_type":"code","metadata":{"id":"oh3fg51s9Dmf","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train[i+j*10])\n","        ax[j,i].set_title(classes[y_train[i+j*10,0]])\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAKLJdJK9Dml","colab_type":"text"},"source":["---\n","### c. Split Training Data\n","<br>\n","\n","**EXERCISE**: \n","* Cut the `last 1000 data` from `Training Set`, and save it as `Validation Set`"]},{"cell_type":"code","metadata":{"id":"lQ8nhpld9Dmm","colab_type":"code","colab":{}},"source":["X_val = ??\n","y_val = ??\n","\n","X_train = ??\n","y_train = ??\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ISmCCfEH9Dmr","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"pSi7HV2g9Dmu","colab_type":"code","colab":{}},"source":["print('X_val.shape   =',X_val.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pd-rnDbi9Dmy","colab_type":"text"},"source":["**Expected Output**: \n","<pre>X_val.shape   = (1000, 32, 32, 3)\n","y_val.shape   = (1000, 1)\n","X_train.shape = (49000, 32, 32, 3)\n","y_train.shape = (49000, 1)"]},{"cell_type":"markdown","metadata":{"id":"Lzcjo5I49Dm2","colab_type":"text"},"source":["---\n","### d. Normalizing Data\n","<br>\n","\n","**EXERCISE**: \n","* Normalize `X_train`, `X_val`, and `X_test` by *zero-centering* them:\n","    1. calculate the `mean` of training data `X_train`\n","    * subtract `X_train`, `X_val`, and `X_test` using mean of `X_train`"]},{"cell_type":"code","metadata":{"id":"FcpIfZc89Dm3","colab_type":"code","colab":{}},"source":["X_train = X_train.astype('float32')\n","X_val = X_val.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","mean_image = ??\n","X_train = ??\n","X_val = ??\n","X_test = ??"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MOMpLNHT9Dm6","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"BWC1p_HN9Dm7","colab_type":"code","colab":{}},"source":["print('np.mean(X_train) =',np.mean(X_train))\n","print('np.mean(X_val)   =',np.mean(X_val))\n","print('np.mean(X_test)  =',np.mean(X_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnveLwD19Dm-","colab_type":"text"},"source":["**Expected Output**: \n","<pre>np.mean(X_train) = -6.6769658e-06\n","np.mean(X_val)   = 0.89910334\n","np.mean(X_test)  = 0.83958524"]},{"cell_type":"markdown","metadata":{"id":"rhc2VSMs9Dm_","colab_type":"text"},"source":["---\n","### e. Reshape Data\n","<br>\n","\n","**EXERCISE**: \n","* Reshape each data in `X_train`, `X_val`, and `X_test` into 1-dimensional matrix \n","\n","*Hint: use `np.reshape()`*"]},{"cell_type":"code","metadata":{"id":"w-H-e8_c9Dm_","colab_type":"code","colab":{}},"source":["X_train = ??\n","X_val = ??\n","X_test = ??"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pOje5Ipx9DnD","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"G7JNj6Iu9DnE","colab_type":"code","colab":{}},"source":["print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hh7U-aDf9DnJ","colab_type":"text"},"source":["**Expected Output**: \n","<pre>X_train.shape = (49000, 3072)\n","X_val.shape   = (1000, 3072)\n","X_test.shape  = (10000, 3072)\n"]},{"cell_type":"markdown","metadata":{"id":"Qj004Z-R9DnJ","colab_type":"text"},"source":["**EXERCISE**:\n","* Reshape `y_train`, `y_val`, and `y_test` into a vector \n","\n","*Hint: use `np.ravel()`*"]},{"cell_type":"code","metadata":{"id":"-8OeUi9Z9DnK","colab_type":"code","colab":{}},"source":["y_train = ??\n","y_val = ??\n","y_test = ??"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmCy39OH9DnM","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"dtLgvhjN9DnO","colab_type":"code","colab":{}},"source":["print('y_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jiVm8QKG9DnR","colab_type":"text"},"source":["**Expected Output**: \n","<pre>y_train.shape = (49000,)\n","y_val.shape   = (1000,)\n","y_test.shape  = (10000,)\n"]},{"cell_type":"markdown","metadata":{"id":"YWdxD4yY9DnS","colab_type":"text"},"source":["---\n","## 2 - Linear Function\n","* Complete the forward and backward function of basic linear regression function\n"]},{"cell_type":"markdown","metadata":{"id":"BCWkIzxS9DnU","colab_type":"text"},"source":["---\n","### a. Forward Function\n","\n","<br>\n","\n","**EXERCISE**: \n","* Implement `forward function` for Linear Classifier as follow:\n","\n","$$\n","\\begin{align}\n","f(x, W, b) = x.W + b\n","\\end{align}\n","$$"]},{"cell_type":"code","metadata":{"id":"r5NBU3R09DnU","colab_type":"code","colab":{}},"source":["def forward(x, W, b):   \n","    v = ??    \n","    \n","    return v"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2RdksEU79DnY","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"tpCXyrSs9DnY","colab_type":"code","colab":{}},"source":["X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n","\n","np.random.seed(2019)\n","W = np.random.random((3, 2)) \n","b = np.zeros((1, 2))\n","\n","v = forward(X, W, b)\n","\n","print('v.shape =', v.shape, '\\n')\n","print('v =')\n","print(v)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6grD-Rsj9Dnc","colab_type":"text"},"source":["**Expected Output**: \n","<pre>v.shape = (4, 2) \n","\n","v =\n","[[0.88049907 0.29917202]\n"," [1.50446903 0.93704942]\n"," [1.78398128 0.69225253]\n"," [2.40795124 1.33012993]]"]},{"cell_type":"markdown","metadata":{"id":"vVopw_gk9Dnd","colab_type":"text"},"source":["---\n","### b. Backward Function\n","<br>\n","\n","**EXERCISE**: \n","* Implement `backward function` for Linear Classifier as follow:\n","\n","\n","$$\n","\\begin{align*}\n","\\partial W & = x^T.\\partial out \\\\\n","\\partial b & = \\sum \\partial out \\\\\n","\\partial x & = \\partial out.W^T \\\\\n","\\end{align*}\n","$$\n","\n","*Hint: use `axis=0` and `keepdims=True` to calculate $\\partial b$*"]},{"cell_type":"code","metadata":{"id":"zbgHRRHX9Dne","colab_type":"code","colab":{}},"source":["def backward(dout, x, W, b):\n","    dW = ??\n","    db = ??\n","    dx = ??\n","    \n","    return dW, db, dx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m8fq8hUI9Dng","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"HwnaN5QY9Dng","colab_type":"code","colab":{}},"source":["np.random.seed(2019)\n","dout = np.random.random((4, 2)) \n","dW, db, dX = backward(dout, X, W, b)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kr_7dy609Dnj","colab_type":"code","colab":{}},"source":["print('dW.shape =', dW.shape)\n","print('dW =')\n","print(dW)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZmBC6KsR9Dnl","colab_type":"text"},"source":["**Expected Output**: \n","<pre>\n","dW.shape = (3, 2) \n","dW =\n","[[1.58269734 1.20237818]\n"," [1.32616823 1.54108356]\n"," [3.11014951 2.23333609]]"]},{"cell_type":"code","metadata":{"id":"YViZwMF79Dnl","colab_type":"code","colab":{}},"source":["print('db.shape =', db.shape)\n","print('db =', db)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xr0Er_ix9Dnr","colab_type":"text"},"source":["**Expected Output**: \n","<pre>db.shape = (1, 2)\n","db = [[3.11014951 2.23333609]]"]},{"cell_type":"code","metadata":{"id":"8o5k8LUv9Dnr","colab_type":"code","colab":{}},"source":["print('dX.shape =', dX.shape)\n","print('dX =')\n","print(dX)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VNebgGmG9Dnt","colab_type":"text"},"source":["**Expected Output**: \n","<pre>dX.shape = (4, 3)\n","dX =\n","[[0.9707924  0.81448293 0.91311394]\n"," [0.81448293 0.79622609 0.74024004]\n"," [0.91311394 0.74024004 0.86478251]\n"," [0.98945638 1.01428543 0.88849893]]\n"]},{"cell_type":"markdown","metadata":{"id":"dsBraIv69Dnu","colab_type":"text"},"source":["---\n","## 3 - Softmax Function\n","\n","* A very important concept to understand in numpy is `\"broadcasting\"`. \n","* It is very useful for performing mathematical operations between arrays of different shapes. \n","* For the full details on broadcasting, you can read the official [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."]},{"cell_type":"markdown","metadata":{"id":"GEb-4C9C9Dnw","colab_type":"text"},"source":["---\n","### a. Softmax Score\n","<br>\n","\n","**EXERCISE**: \n","* Implement a `softmax score function` using numpy."]},{"cell_type":"markdown","metadata":{"id":"GWVsslj19Dnw","colab_type":"text"},"source":["\n","$\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n}$\n","\n","- $ n : \\text{number of data}$\n","- $ m : \\text{number of label}$\n","\n","$\\text{$x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$}$ \n","\n","thus we have\n","$$softmax(x) = softmax\\begin{bmatrix}\n","    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n","    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n","\\end{bmatrix} = \\begin{bmatrix}\n","    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n","    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n","\\end{bmatrix}$$\n","\n","it is equal to\n","$$softmax(x)  = \\begin{pmatrix}\n","    softmax\\text{(first row of x)}  \\\\\n","    softmax\\text{(second row of x)} \\\\\n","    ...  \\\\\n","    softmax\\text{(last row of x)} \\\\\n","\\end{pmatrix} $$"]},{"cell_type":"code","metadata":{"id":"kGx1tnkt9Dny","colab_type":"code","colab":{}},"source":["def softmax(x):\n","    \"\"\"Calculates the softmax for each row of the input x.\n","\n","Argument:\n","    x -- A numpy matrix of shape (n,m)\n","\n","    Returns:\n","    score -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","          -- normalized log probabilities score\n","    \"\"\"\n","    \n","    # 1. COMPUTE THE CLASS PROBABILITIES    \n","    \n","    # shift x by subtracting with its maximum value . Use np.max(...)\n","    x = ??\n","    \n","    # Apply exp() element-wise to x. Use np.exp(...).    \n","    x_exp = ??\n","\n","    # Create a vector X_sum that sums each row of X_exp. Use np.sum(..., axis = 1, keepdims = True).\n","    x_sum = ??\n","    \n","    # Compute softmax(x) score by dividing X_exp by X_sum. It should automatically use numpy broadcasting.\n","    score = ??\n","    \n","    return score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6rLwS9af9Dn1","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"scUi41em9Dn2","colab_type":"code","colab":{}},"source":["x = np.array([\n","    [1, 3, 0, 1 ,1],\n","    [0, 2, 0, 1, 9],\n","    [0, 1, 0, 1, 0]])\n","\n","np.random.seed(2019)\n","W = np.random.random((5, 2)) \n","b = np.zeros((1, 2))\n","\n","score = forward(x, W, b)\n","\n","print(\"score = \" )\n","print(score,'\\n')\n","print(\"softmax(score) = \" )\n","print(softmax(score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coz3paZb9Dn4","colab_type":"text"},"source":["**Expected Output**:\n","<pre>score = \n","[[4.35897229 3.61566867]\n"," [9.88257553 5.83070915]\n"," [1.32616823 1.54108356]] \n","\n","softmax(score) = \n","[[0.67771785 0.32228215]\n"," [0.98290735 0.01709265]\n"," [0.44647702 0.55352298]]"]},{"cell_type":"markdown","metadata":{"id":"ZlkgZgQp9Dn4","colab_type":"text"},"source":["---\n","### b. Softmax Loss\n","<br>\n","\n","**EXERCISE**: \n","* Implement a `softmax loss` function using numpy. \n"," \n","- Loss of the `i-th data` is the normalized log probability of the score at the class should be<br>\n","  $$L_i =  -\\log(\\frac{e^{X_{y_i}}}{\\sum_{j}e^{X_{j}}})$$<br><br>\n"," \n","- The Softmax Loss is the average of all data loss<br>\n","  $$L = \\frac{1}{N}\\sum^N_{i=1}L_i$$"]},{"cell_type":"code","metadata":{"id":"mi1S0uZp9Dn5","colab_type":"code","colab":{}},"source":["def softmax_loss(score, y):\n","    \"\"\"Calculates the softmax loss for each row of the input x.\n","    \n","    Argument:\n","    score -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","          -- normalized log probabilities score\n","          \n","    y     -- A numpy vector of shape(n,)\n","          -- containing training labels;\n","          -- y[i] = c means that X[i] has label c, where 0 <= c < C.\n","\n","    Returns:\n","    dscore -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","           -- gradient score of softmax\n","    \"\"\"\n","    \n","    num_examples = score.shape[0]\n","        \n","    # 2. COMPUTE THE LOSS : average cross-entropy loss\n","    \n","    #make a number list containing [1 2 3 ... n]\n","    number_list = ??\n","    \n","    # calculate the correct log probability of score[number_list,y] by applycing -np.log(...)\n","    corect_logprobs = ??\n","    \n","    # average the correct log probability, use np.sum then divide it by num_examples\n","    loss = ??\n","    \n","    \n","    # 3. COMPUTE THE GRADIENT ON SCORES\n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    \n","    return loss, dscores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wkV7BrMh9Dn7","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"y_6zil3N9Dn8","colab_type":"code","colab":{}},"source":["x = np.array([\n","    [1, 3, 0, 1 ,1],\n","    [0, 2, 0, 1, 9],\n","    [0, 1, 0, 1, 0]])\n","\n","y = np.array([\n","    0, 1, 0\n","])\n","\n","np.random.seed(2019)\n","W = np.random.random((5, 2)) \n","b = np.zeros((1, 2))\n","\n","score = forward(x, W, b)\n","\n","print(\"score = \" )\n","print(score, '\\n')\n","\n","score = softmax(score)\n","print(\"softmax(score) = \" )\n","print(score,'\\n')\n","\n","loss, dscore = softmax_loss(score, y)\n","print('loss   =',loss, '\\n')\n","print('dscore =')\n","print(dscore)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBvXw_F89DoD","colab_type":"text"},"source":["**Expected Output**:\n","<pre>score = \n","[[4.35897229 3.61566867]\n"," [9.88257553 5.83070915]\n"," [1.32616823 1.54108356]] \n","\n","softmax(score) = \n","[[0.67771785 0.32228215]\n"," [0.98290735 0.01709265]\n"," [0.44647702 0.55352298]] \n","\n","loss   = 1.7548327929166458 \n","\n","dscore =\n","[[-0.10742738  0.10742738]\n"," [ 0.32763578 -0.32763578]\n"," [-0.18450766  0.18450766]]"]},{"cell_type":"markdown","metadata":{"id":"vl2_OZBK9DoE","colab_type":"text"},"source":["---\n","## 4 - Train a Softmax Classifier\n","* Combine linear function and softmax function that you have completed to train a multiclass softmax classifier"]},{"cell_type":"markdown","metadata":{"id":"_KO3lGVv9DoF","colab_type":"text"},"source":["---\n","### a. Weight Initialization\n","<br>\n","\n","**EXERCISE**: \n","* Implement `Weight initialization function`\n","    - receive input size `n`, number of neuron `d`, and standard deviation `std`\n","    - generate normal random matrix `W` of size `(n x d)` and multiply it by `std`\n","    - generate zeros matrix `b` of size `(1 x d)`"]},{"cell_type":"code","metadata":{"id":"HsDrps-i9DoF","colab_type":"code","colab":{}},"source":["def initialize_weights(n, d, std):\n","    W = ??\n","    b = ?\n","    \n","    return W, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITTqUp4c9DoJ","colab_type":"text"},"source":["---\n","### b. Training Function\n","<br>\n","\n","**EXERCISE**: \n","* Implement **Training Function**\n","    * call `forward function`\n","    * call `softmax function`\n","    * call `softmax_loss function`\n","    * call `backward function`\n","    * implement `weight update`"]},{"cell_type":"code","metadata":{"id":"ISpmp2Oj9DoJ","colab_type":"code","colab":{}},"source":["def train(X, y, W=None, b=None, learning_rate=1e-6, reg=1e4, num_iters=100, batch_size=200, verbose=False):\n","    \n","    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","    \n","    # get number of training data and its dimension from X\n","    num_train, dim = ??\n","    \n","    # initialize new weights if W matrix is not provided\n","    # call initialize_weights function using dim, num_classes, and std=0.001\n","    if W is None:\n","        W, b = ??\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","                     \n","    for it in range(num_iters):\n","        X_batch = None\n","        y_batch = None\n","\n","        # Randomly select indices from training examples\n","        train_rows = np.arange(num_train)\n","        idxs = np.random.choice(train_rows, batch_size, replace=False)\n","  \n","        X_batch = X[idxs]\n","        y_batch = y[idxs]\n","\n","\n","        # calculate class score by calling forward function using X_batch, W, and b\n","        scores = ??\n","        \n","        # calculate softmax score by calling softmax function using scores\n","        softmaX_score = ??\n","        \n","        # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n","        loss, dout = ??\n","    \n","        # append the loss history\n","        loss_history.append(loss)\n","\n","        # calculate weights gradient by calling backward function using dout, X_batch, W, and b\n","        dW, db, _ = ??\n","        \n","        # perform regulatization gradient\n","        dW += reg*W\n","        \n","        # perform parameter update by subtracting W and b with a fraction of dW and db\n","        # according to the learning rate\n","        W -= ??\n","        b -= ??\n","        \n","        if verbose and it % 100 == 0:\n","            print ('iteration', it,'/',num_iters, ': loss =', loss)\n","    return loss_history, W, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XbGhYnV49DoL","colab_type":"text"},"source":["---\n","### c. Train the Softmax Classifier\n","\n","Try the training Function using the initial parameter"]},{"cell_type":"code","metadata":{"id":"RsvCPm849DoM","colab_type":"code","colab":{}},"source":["loss, W, b = train(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLHo9U5O9DoQ","colab_type":"text"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"C0lzRGK49DoR","colab_type":"code","colab":{}},"source":["plt.plot(loss)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l_dspMjV9DoT","colab_type":"text"},"source":["**Expected Output**:\n","The graph should show that the loss is generally decreased as the training goes"]},{"cell_type":"markdown","metadata":{"id":"z6iAfJWB9DoU","colab_type":"text"},"source":["---\n","### d. Predict Function\n","<br>\n","\n","**EXERCISE**: \n","* Implement **Predict Function**\n","    * call `forward function`"]},{"cell_type":"code","metadata":{"id":"DqxCYfon9DoU","colab_type":"code","colab":{}},"source":["def predict(X, W, b):    \n","    y_pred = np.zeros(X.shape[1])\n","\n","    \n","    # calculate class score by calling forward function using X, W, and b\n","    y_pred = ??\n","    \n","    # take the maximum prediction and use that column to get the class     \n","    y_pred = y_pred.argmax(axis=-1)\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCpnbDZO9DoY","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"6JAjNgRU9DoY","colab_type":"code","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict(X_train, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =', accuracy*100,'%')\n","\n","print('Training label  =', y_test[:15])\n","print('Predicted label =', y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OplASJoa9Dob","colab_type":"text"},"source":["**Expected Output**:\n","\n","You should get about **`27-29%`** accuracy on training set using the initial run\n","\n","---"]},{"cell_type":"code","metadata":{"id":"LMeU07P59Doc","colab_type":"code","colab":{}},"source":["y_pred = predict(X_val, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_test[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTMNd3Ud9Dof","colab_type":"text"},"source":["**Expected Output**:\n","\n","You should also get about **`27-29%`** accuracy on validation set\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"markdown","metadata":{"id":"eSC5w5Bw9Dof","colab_type":"text"},"source":["---\n","## 5 - Hyperparameter Tuning\n","* After you find that the classifier is working properly, next\n","* Find the best Learning Rate and Regularization Strength\n","<br>\n","<br>\n","\n","\n","**EXERCISE**: \n","* Use the `validation set` to tune hyperparameters (regularization strength and learning rate). \n","     * Greedily loop over `learning_rates` and `regulatization` to get all combination\n","     \n","     \n","* You **should** experiment with different ranges for the learning rates and regularization strengths; \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"IZSdjo7V9Dof","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","results = {}\n","best_val = -1\n","best_W = None\n","best_b = None\n","learning_rates = [1e-6, 1e-7, 5e-7]\n","regularization_strengths = [2.5e4, 5e4]\n","\n","iterations = 2000\n","\n","# Greedily loop over learning_rates and regularization_strengths\n","for ??:\n","    for ??:\n","        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, rate, reg))\n","        # call train function using the learning rate and regularization selected\n","        loss, W, b = train(X_train, y_train, \n","                              learning_rate=??, reg=??,\n","                              num_iters=??, verbose=False)\n","        \n","        # call predict function using pretrained W and b on X_train and X_val to evaluate\n","        y_train_pred = predict(??, ??, ??)\n","        y_val_pred = predict(??, ??, ??)\n","        \n","        # calculate the accuracy\n","        train_accuracy = np.mean(y_train == y_train_pred)\n","        val_accuracy = np.mean(y_val == y_val_pred)\n","        \n","        print ('rate = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n","            rate, reg, train_accuracy, val_accuracy))\n","        \n","        # store the result accuracy combination\n","        results[(rate, reg)] = (train_accuracy, val_accuracy)\n","        \n","        # store the best Weight and Bias\n","        if val_accuracy > best_val:\n","            best_W = W\n","            best_b = b\n","            best_val = val_accuracy\n","\n","\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr',lr,',reg',reg,', train accuracy: ',train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n","    \n","print()\n","print('best validation accuracy achieved during cross-validation:',best_val*100,'%')\n","print('best Weights and bias are stored in `best_W` and `best_b`')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvoX3R9b9Doi","colab_type":"text"},"source":["**Expected Output**:\n","\n","if you are careful you should be able to get a classification accuracy of **over 35%** on the validation set\n"]},{"cell_type":"markdown","metadata":{"id":"fyocgmtR9Doi","colab_type":"text"},"source":["---\n","## 6 - Try Another Hyperparameter Combination\n","\n","Try to add another combination of learning rates and regularization strength hyperparameter, and repeat the `step 5`\n"]},{"cell_type":"markdown","metadata":{"id":"7mAms-8F9Doi","colab_type":"text"},"source":["---\n","## 7 - Test the Trained Weights\n","\n","<br>\n","\n","**EXERCISE**: \n","* Test the **Best Weights and Bias** to the `X_test`"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"2rxRWuP59Doj","colab_type":"code","colab":{}},"source":["y_pred = predict(??, ??, ??)\n","\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label      =',y_test[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lM85_AmY9Dol","colab_type":"text"},"source":["**Expected Output**:\n","\n","You should reach at least  **32%** accuracy on test set\n"]},{"cell_type":"markdown","metadata":{"id":"JEOTrqQw9Dom","colab_type":"text"},"source":["---\n","## 8 - Vizualize the Trained Weights \n","\n","Visualize the learned weights for each class for each class\n"]},{"cell_type":"markdown","metadata":{"id":"YPsnDOay9Dom","colab_type":"text"},"source":["---\n","### a. Retrieve one class weights\n","Try to visualize one trained weights.<br><br>\n","**Note:** You can change the `id`"]},{"cell_type":"code","metadata":{"id":"Ga_gct3-9Dom","colab_type":"code","colab":{}},"source":["id = 0\n","\n","# get the id-th column from best weight\n","w = ??\n","\n","print('w.shape =',w.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mroPU69R9Doo","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","w.shape = (3072,)\n"]},{"cell_type":"markdown","metadata":{"id":"1WMK5MRI9Dop","colab_type":"text"},"source":["---\n","### b. Normalize the weight for visualization\n","Image plot can only receive input range [0..1] for float type, or [0..255] for integer type, and the weights are not in those range.\n","\n","So we need to normalize. Here we use MinMaxScaler from sklearn toolkit"]},{"cell_type":"code","metadata":{"id":"SCoMNGas9Dop","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler((0.05,.95))\n","w = scaler.fit_transform(w.reshape(-1, 1))\n","\n","print('w.shape =',w.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hJ4PJU79Dor","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","w.shape = (3072, 1)"]},{"cell_type":"markdown","metadata":{"id":"hpUlVHil9Dor","colab_type":"text"},"source":["---\n","### c. Visualize the weight \n","Now reshape the weight back to (32,32,3), and plot the image. Use `np.reshape()`"]},{"cell_type":"code","metadata":{"id":"op2LEL-x9Dos","colab_type":"code","colab":{}},"source":["w = ??\n","\n","plt.figure(figsize=(2,2))\n","plt.imshow(w)\n","plt.title(classes[id])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KHoUj9Tg9Dot","colab_type":"text"},"source":["---\n","### d. Do it to the other weights"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"c-9SIJDu9Dot","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(1,10,figsize=(18,5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for i in range(0, 10):\n","    # get the i-th weight from best_W\n","    w = ??\n","    \n","    # normalize the weight (see previous example)\n","    w = ??\n","    \n","    # reshape the weight\n","    w = ??\n","    \n","    ax[i].imshow(w)\n","    ax[i].axis('off')\n","    ax[i].set_title(classes[i])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ucAtH6r9Dox","colab_type":"text"},"source":["**Expected Output**:\n","\n","You should see that the weights visualization should  match the ilustration below\n","\n","![cifar10](http://cs231n.github.io/assets/templates.jpg)\n"]},{"cell_type":"markdown","metadata":{"id":"xpOR1Uww9Doz","colab_type":"text"},"source":["---\n","## 9 - Linear Classifier: Multiclass SVM"]},{"cell_type":"markdown","metadata":{"id":"0NwIDkX09Doz","colab_type":"text"},"source":["---\n","### a. SVM Loss\n","\n","<br>\n","\n","**EXERCISE**: \n","* Implement `Multiclass SVM Loss` or `Hinge Loss`"]},{"cell_type":"code","metadata":{"id":"mp767_w69Do0","colab_type":"code","colab":{}},"source":["def svm_loss(score, y):\n","    \"\"\"Calculates the smb loss for each row of the input x.\n","    \n","    Argument:\n","    score -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","          -- normalized log probabilities score\n","          \n","    y     -- A numpy vector of shape(n,)\n","          -- containing training labels;\n","          -- y[i] = c means that X[i] has label c, where 0 <= c < C.\n","\n","    Returns:\n","    dscore -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","           -- gradient score of softmax\n","    \"\"\"\n","    \n","    num_examples = score.shape[0]\n","    \n","    \n","    # 1. COMPUTE THE SVM LOSS\n","    \n","    # get the correct class score. Use np.choose( , ) using y and score.T\n","    correct_scores = ??\n","    \n","    # calculate margin: score.T subtracted by correct_score, then add by 1\n","    margin = ??\n","    \n","    # remove all margin below 0. Use np.maximum(0, margin)\n","    margin = ??\n","        \n","    # COMPUTE THE LOSS \n","    # calculate SVM loss of each data: sum of margin of non-target class\n","    # simply total all the margin (use np.sum()), and subtract it by 1\n","    loss_i = ??\n","    \n","    \n","    \n","    # average loss_i by num_examples\n","    loss = ??\n","    \n","    \n","    # 2. COMPUTE THE GRADIENT ON SCORES\n","    # margins for the correct class have already been set to 0\n","    dscores = (margin.T > 0).astype(float)\n","    marginsSum = dscores.sum(1) - 1\n","    dscores[range(dscores.shape[0]), y] = -marginsSum\n","        \n","    return loss, dscores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ukym7aAK9Do1","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"YxevLrVy9Do1","colab_type":"code","colab":{}},"source":["x = np.array([\n","    [9, 2, 5, 0, 0],\n","    [7, 5, 0, 0 ,0],\n","    [1, 0, 3, 5 ,0]])\n","\n","y = np.array([\n","    0, 0, 1\n","])\n","\n","np.random.seed(20)\n","W = np.random.random((5, 2)) \n","b = np.zeros((1, 2))\n","\n","score = forward(x, W, b)\n","\n","print(\"score = \" )\n","print(score)\n","print()\n","\n","loss, dscore = svm_loss(score, y)\n","print('loss   =',loss)\n","print('dscore =')\n","print(dscore)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKjCMyCY9Do3","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n","score = \n","[[ 7.2556866  13.16988641]\n"," [ 8.57456925 10.36318348]\n"," [ 2.58920427  5.5655412 ]]\n","\n","loss   = 3.234271348285095\n","dscore =\n","[[-1.  1.]\n"," [-1.  1.]\n"," [ 0. -0.]]\n"]},{"cell_type":"markdown","metadata":{"id":"rGGXv4Zn9Do3","colab_type":"text"},"source":["---\n","\n","Notice the loss gradients resulted from SVM Loss. \n","\n","What's the difference between those gradients and Softmax Gradients?\n","\n","What are the effects?"]},{"cell_type":"markdown","metadata":{"id":"AR0Ptu929Do3","colab_type":"text"},"source":["---\n","### b. Train Multiclass SVM Classifier\n","<br>\n","\n","**EXERCISE**: \n","* The same as the training function using Softmax before, complete the train function below, \n","* but rather than using `softmax_loss`, use `svm_loss`"]},{"cell_type":"code","metadata":{"id":"jYNpfn4g9Do4","colab_type":"code","colab":{}},"source":["def train_svm(X, y, W=None, b=None, learning_rate=1e-6, reg=1e4, num_iters=100, batch_size=200, verbose=False):\n","    num_train, dim = X.shape\n","    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","    \n","    if W is None:\n","        W = 0.001 * np.random.randn(dim, num_classes)\n","    if b is None:\n","        b = np.zeros((1,num_classes))\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","                     \n","    for it in range(num_iters):\n","        X_batch = None\n","        y_batch = None\n","\n","        # Randomly select indices from training examples\n","        train_rows = np.arange(num_train)\n","        idxs = np.random.choice(train_rows, batch_size, replace=False)\n","  \n","        X_batch = X[idxs]\n","        y_batch = y[idxs]\n","\n","\n","        # calculate class score by calling forward function using X_batch, W, and b\n","        scores = ??\n","                \n","        # evaluate loss and gradient by calling svm_loss function using score and y_batch\n","        loss, dout = ??\n","    \n","        # append the loss history\n","        loss_history.append(loss)\n","\n","        # calculate weights gradient by calling backward function using dout, X_batch, W, and b\n","        dW, db, _ = ??\n","        \n","        # perform regulatization gradient\n","        dW += reg*W\n","        \n","        # perform parameter update by subtracting W and b with a fraction of dW and db\n","        # according to the learning rate\n","        W -= ??\n","        b -= ??\n","        \n","        if verbose and it % 100 == 0:\n","            print ('iteration', it,'/',num_iters, ': loss =', loss)\n","    return loss_history, W, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfplBGaT9Do5","colab_type":"text"},"source":["---\n","### c. Hyperparameter Tuning for Multiclass SVM Classifier\n","<br>\n","\n","**EXERCISE**: \n","* Again, complete the codes below similar to the Hyperparameter tuning for `Softmax Classifier`"]},{"cell_type":"code","metadata":{"id":"4_kPgMF79Do6","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","results = {}\n","best_val = -1\n","best_W = None\n","best_b = None\n","learning_rates = [1e-6, 1e-7, 5e-7]\n","regularization_strengths = [3e4, 5e4]\n","\n","iterations = 2000\n","\n","# Greedily loop over learning_rates and regularization_strengths\n","for ??:\n","    for ??:\n","        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, rate, reg))\n","        # call train function using the learning rate and regularization selected\n","        loss, W, b = train(X_train, y_train, \n","                              learning_rate=??, reg=??,\n","                              num_iters=??, verbose=False)\n","        \n","        # call predict function using pretrained W and b on X_train and X_val to evaluate\n","        y_train_pred = predict(??, ??, ??)\n","        y_val_pred = predict(??, ??, ??)\n","        \n","        # calculate the accuracy\n","        train_accuracy = np.mean(y_train == y_train_pred)\n","        val_accuracy = np.mean(y_val == y_val_pred)\n","        \n","        print ('rate = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n","            rate, reg, train_accuracy, val_accuracy))\n","        \n","        # store the result accuracy combination\n","        results[(rate, reg)] = (train_accuracy, val_accuracy)\n","        \n","        # store the best Weight and Bias\n","        if val_accuracy > best_val:\n","            best_W = W\n","            best_b = b\n","            best_val = val_accuracy\n","\n","            \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr',lr,',reg',reg,', train accuracy: ', train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n","    \n","print()\n","print('best validation accuracy achieved during cross-validation:', best_val*100, '%')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HACKUtEn9Do7","colab_type":"text"},"source":["**Expected Output**:\n","\n","You should reach `at least`  **`30%`** accuracy on cross-validation"]},{"cell_type":"markdown","metadata":{"id":"KFFir43e9Do8","colab_type":"text"},"source":["---\n","### d. Visualize the Weights Trained\n","<br>\n","\n","**EXERCISE**: \n","* complete the implementation to show the trained weight visualizations"]},{"cell_type":"code","metadata":{"id":"qyG_2yWO9Do8","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(1,10,figsize=(18,5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for i in range(0, 10):\n","    # get the i-th weight\n","    w = ??\n","    \n","    # normalize the weight\n","    w = ??\n","    \n","    # reshape the weight\n","    w = ??\n","    \n","    ax[i].imshow(w)\n","    ax[i].axis('off')\n","    ax[i].set_title(classes[i])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGwFLO299Do9","colab_type":"text"},"source":["### Notice the difference in weight visualization\n","Describe what your visualized SVM weights look like. Think on why they look they way that they do.\n","\n","---\n","---\n","\n","# Congratulation, You've Completed Exercise 1"]},{"cell_type":"markdown","metadata":{"id":"7_Hkr9kg9Do-","colab_type":"text"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}