{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nde_vGtp9Dlr"
   },
   "source": [
    "![title](https://image.ibb.co/erDntK/logo2018.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqfEjzA09Dlx"
   },
   "source": [
    "# Simple Linear Classification\n",
    "\n",
    "In this assignment you will practice putting together a simple image classification pipeline, based on the SVM and Softmax classifier. \n",
    "\n",
    "The goals of this assignment are as follows:\n",
    "* understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)\n",
    "* understand the train/val/test splits and the use of validation data for hyperparameter tuning.\n",
    "* develop proficiency in writing efficient vectorized code with numpy\n",
    "\n",
    "* implement and apply a Softmax classifier\n",
    "* implement and apply a Multiclass Support Vector Machine (SVM) classifier\n",
    "\n",
    "* understand the differences and tradeoffs between these classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMlC1Kwa9Dl0"
   },
   "source": [
    "---\n",
    "## 0 - Import necessary libraries and informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWJMf_Kx9Dl3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8molz1J9Dl-"
   },
   "source": [
    "Write down your Name and Student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkzQo8MY9DmB"
   },
   "outputs": [],
   "source": [
    "## --- start your code here ----\n",
    "\n",
    "NIM = 123456\n",
    "Nama = \"\"\n",
    "\n",
    "## --- end your code here ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNXQVKWr9DmH"
   },
   "source": [
    "---\n",
    "## 1 - Load CIFAR-10 Dataset\n",
    "\n",
    "* First, Obtain Cifar-10 dataset.\n",
    "  There are from various source in Internet like [Keras](https://keras.io/datasets/), [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/datasets), or any other source\n",
    "* Next you will prepare the dataset by first:\n",
    " * visualizing data\n",
    " * split into training, validation, and testing set\n",
    " * normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJ4WkMiR9DmI"
   },
   "source": [
    "---\n",
    "### a. Import Data ***CIFAR-10***\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* `load Cifar-10 dataaset.`\n",
    "* `It should consists of 50000 32x32 images for training and 10000 of the same shape for testing.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsjp0Rou9DmL"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = ??\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'forse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_drJAPL9DmR"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xkyOKsX9DmV"
   },
   "outputs": [],
   "source": [
    "print('X_train.shape =',X_train.shape)\n",
    "print('y_train.shape =',y_train.shape)\n",
    "print('X_test.shape  =',X_test.shape)\n",
    "print('y_test.shape  =',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFmi0aMw9Dmb"
   },
   "source": [
    "**Expected Output**: \n",
    "<pre>\n",
    "X_train.shape = (50000, 32, 32, 3)\n",
    "y_train.shape = (50000, 1)\n",
    "X_test.shape  = (10000, 32, 32, 3)\n",
    "y_test.shape  = (10000, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YNXtE039Dmd"
   },
   "source": [
    "---\n",
    "### b. Visualizing Data\n",
    "\n",
    "\n",
    "Show the first 20 images from X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oh3fg51s9Dmf"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "for j in range(0,2):\n",
    "    for i in range(0, 10):\n",
    "        ax[j,i].imshow(X_train[i+j*10])\n",
    "        ax[j,i].set_title(classes[y_train[i+j*10,0]])\n",
    "        ax[j,i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAKLJdJK9Dml"
   },
   "source": [
    "---\n",
    "### c. Split Training Data\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Cut the `last 1000 data` from `Training Set`, and save it as `Validation Set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQ8nhpld9Dmm"
   },
   "outputs": [],
   "source": [
    "X_val = ??\n",
    "y_val = ??\n",
    "\n",
    "X_train = ??\n",
    "y_train = ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISmCCfEH9Dmr"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSi7HV2g9Dmu"
   },
   "outputs": [],
   "source": [
    "print('X_val.shape   =',X_val.shape)\n",
    "print('y_val.shape   =',y_val.shape)\n",
    "print('X_train.shape =',X_train.shape)\n",
    "print('y_train.shape =',y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pd-rnDbi9Dmy"
   },
   "source": [
    "**Expected Output**: \n",
    "<pre>X_val.shape   = (1000, 32, 32, 3)\n",
    "y_val.shape   = (1000, 1)\n",
    "X_train.shape = (49000, 32, 32, 3)\n",
    "y_train.shape = (49000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lzcjo5I49Dm2"
   },
   "source": [
    "---\n",
    "### d. Normalizing Data\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Normalize `X_train`, `X_val`, and `X_test` by *zero-centering* them:\n",
    "    1. calculate the `mean` of training data `X_train`\n",
    "    * subtract `X_train`, `X_val`, and `X_test` using mean of `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcpIfZc89Dm3"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "mean_image = ??\n",
    "X_train = ??\n",
    "X_val = ??\n",
    "X_test = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOMpLNHT9Dm6"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWC1p_HN9Dm7"
   },
   "outputs": [],
   "source": [
    "print('np.mean(X_train) =',np.mean(X_train))\n",
    "print('np.mean(X_val)   =',np.mean(X_val))\n",
    "print('np.mean(X_test)  =',np.mean(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnveLwD19Dm-"
   },
   "source": [
    "**Expected Output**: \n",
    "<pre>np.mean(X_train) = -6.6769658e-06\n",
    "np.mean(X_val)   = 0.89910334\n",
    "np.mean(X_test)  = 0.83958524"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhc2VSMs9Dm_"
   },
   "source": [
    "---\n",
    "### e. Reshape Data\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Reshape each data in `X_train`, `X_val`, and `X_test` into 1-dimensional matrix \n",
    "\n",
    "*Hint: use `np.reshape()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-H-e8_c9Dm_"
   },
   "outputs": [],
   "source": [
    "X_train = ??\n",
    "X_val = ??\n",
    "X_test = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOje5Ipx9DnD"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7JNj6Iu9DnE"
   },
   "outputs": [],
   "source": [
    "print('X_train.shape =',X_train.shape)\n",
    "print('X_val.shape   =',X_val.shape)\n",
    "print('X_test.shape  =',X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hh7U-aDf9DnJ"
   },
   "source": [
    "**Expected Output**: \n",
    "<pre>X_train.shape = (49000, 3072)\n",
    "X_val.shape   = (1000, 3072)\n",
    "X_test.shape  = (10000, 3072)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qj004Z-R9DnJ"
   },
   "source": [
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Reshape `y_train`, `y_val`, and `y_test` into a vector \n",
    "\n",
    "*Hint: use `np.ravel()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8OeUi9Z9DnK"
   },
   "outputs": [],
   "source": [
    "y_train = ??\n",
    "y_val = ??\n",
    "y_test = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmCy39OH9DnM"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtLgvhjN9DnO"
   },
   "outputs": [],
   "source": [
    "print('y_train.shape =',y_train.shape)\n",
    "print('y_val.shape   =',y_val.shape)\n",
    "print('y_test.shape  =',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jiVm8QKG9DnR"
   },
   "source": [
    "**Expected Output**: \n",
    "<pre>y_train.shape = (49000,)\n",
    "y_val.shape   = (1000,)\n",
    "y_test.shape  = (10000,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWdxD4yY9DnS"
   },
   "source": [
    "---\n",
    "## 2 - Linear Function\n",
    "* Complete the forward and backward function of basic linear regression function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCWkIzxS9DnU"
   },
   "source": [
    "---\n",
    "### a. Forward Function\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Implement `forward function` for Linear Classifier as follow:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x, W, b) = x.W + b\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5NBU3R09DnU"
   },
   "outputs": [],
   "source": [
    "def forward(x, W, b):  \n",
    "    v = ?? # x dot W + b    \n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RdksEU79DnY"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpCXyrSs9DnY"
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "\n",
    "np.random.seed(2019)\n",
    "W = np.random.random((3, 2)) \n",
    "b = np.zeros((1, 2))\n",
    "\n",
    "v = forward(X, W, b)\n",
    "\n",
    "print('v.shape =', v.shape, '\\n')\n",
    "print('v =')\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6grD-Rsj9Dnc"
   },
   "source": [
    "**Expected Output**: \n",
    "<pre>v.shape = (4, 2) \n",
    "\n",
    "v =\n",
    "[[0.88049907 0.29917202]\n",
    " [1.50446903 0.93704942]\n",
    " [1.78398128 0.69225253]\n",
    " [2.40795124 1.33012993]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVopw_gk9Dnd"
   },
   "source": [
    "---\n",
    "### b. Backward Function\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Implement `backward function` for Linear Classifier as follow:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\partial W & = x^T.\\partial out \\\\\n",
    "\\partial b & = \\sum \\partial out \\\\\n",
    "\\partial x & = \\partial out.W^T \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "*Hint: use `axis=0` and `keepdims=True` to calculate $\\partial b$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbgHRRHX9Dne"
   },
   "outputs": [],
   "source": [
    "def backward(dout, x, W, b):\n",
    "    dW = ?? # x.T dot dout\n",
    "    db = ?? # sum dout, axis=0, keepdims=True\n",
    "    dx = ?? # dout dot W.T\n",
    "    \n",
    "    return dW, db, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8fq8hUI9Dng"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwnaN5QY9Dng"
   },
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "dout = np.random.random((4, 2)) \n",
    "dW, db, dX = backward(dout, X, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kr_7dy609Dnj"
   },
   "outputs": [],
   "source": [
    "print('dW.shape =', dW.shape)\n",
    "print('dW =')\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZmBC6KsR9Dnl"
   },
   "source": [
    "**Expected Output**: \n",
    "<pre>\n",
    "dW.shape = (3, 2) \n",
    "dW =\n",
    "[[1.58269734 1.20237818]\n",
    " [1.32616823 1.54108356]\n",
    " [3.11014951 2.23333609]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YViZwMF79Dnl"
   },
   "outputs": [],
   "source": [
    "print('db.shape =', db.shape)\n",
    "print('db =', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xr0Er_ix9Dnr"
   },
   "source": [
    "**Expected Output**: \n",
    "<pre>db.shape = (1, 2)\n",
    "db = [[3.11014951 2.23333609]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8o5k8LUv9Dnr"
   },
   "outputs": [],
   "source": [
    "print('dX.shape =', dX.shape)\n",
    "print('dX =')\n",
    "print(dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNebgGmG9Dnt"
   },
   "source": [
    "**Expected Output**: \n",
    "<pre>dX.shape = (4, 3)\n",
    "dX =\n",
    "[[0.9707924  0.81448293 0.91311394]\n",
    " [0.81448293 0.79622609 0.74024004]\n",
    " [0.91311394 0.74024004 0.86478251]\n",
    " [0.98945638 1.01428543 0.88849893]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsBraIv69Dnu"
   },
   "source": [
    "---\n",
    "## 3 - Softmax Function\n",
    "\n",
    "* A very important concept to understand in numpy is `\"broadcasting\"`. \n",
    "* It is very useful for performing mathematical operations between arrays of different shapes. \n",
    "* For the full details on broadcasting, you can read the official [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEb-4C9C9Dnw"
   },
   "source": [
    "---\n",
    "### a. Softmax Score\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Implement a `softmax score function` using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWVsslj19Dnw"
   },
   "source": [
    "\n",
    "$\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n}$\n",
    "\n",
    "- $ n : \\text{number of data}$\n",
    "- $ m : \\text{number of label}$\n",
    "\n",
    "$\\text{$x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$}$ \n",
    "\n",
    "thus we have\n",
    "$$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "it is equal to\n",
    "$$softmax(x)  = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGx1tnkt9Dny"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    score -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "          -- normalized log probabilities score\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. COMPUTE THE CLASS PROBABILITIES    \n",
    "    \n",
    "    # shift x by subtracting with its maximum value . Use np.max(...)\n",
    "    x = ??\n",
    "    \n",
    "    # Apply exp() element-wise to x. Use np.exp(...).    \n",
    "    x_exp = ??\n",
    "\n",
    "    # Create a vector X_sum that sums each row of X_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = ??\n",
    "    \n",
    "    # Compute softmax(x) score by dividing X_exp by X_sum. It should automatically use numpy broadcasting.\n",
    "    score = ??\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rLwS9af9Dn1"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scUi41em9Dn2"
   },
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [1, 3, 0, 1 ,1],\n",
    "    [0, 2, 0, 1, 9],\n",
    "    [0, 1, 0, 1, 0]])\n",
    "\n",
    "np.random.seed(2019)\n",
    "W = np.random.random((5, 2)) \n",
    "b = np.zeros((1, 2))\n",
    "\n",
    "score = forward(x, W, b)\n",
    "\n",
    "print(\"score = \" )\n",
    "print(score,'\\n')\n",
    "print(\"softmax(score) = \" )\n",
    "print(softmax(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "coz3paZb9Dn4"
   },
   "source": [
    "**Expected Output**:\n",
    "<pre>score = \n",
    "[[4.35897229 3.61566867]\n",
    " [9.88257553 5.83070915]\n",
    " [1.32616823 1.54108356]] \n",
    "\n",
    "softmax(score) = \n",
    "[[0.67771785 0.32228215]\n",
    " [0.98290735 0.01709265]\n",
    " [0.44647702 0.55352298]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlkgZgQp9Dn4"
   },
   "source": [
    "---\n",
    "### b. Softmax Loss\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Implement a `softmax loss` function using numpy. \n",
    " \n",
    "- Loss of the `i-th data` is the normalized log probability of the score at the class should be<br>\n",
    "  $$L_i =  -\\log(\\frac{e^{X_{y_i}}}{\\sum_{j}e^{X_{j}}})$$<br><br>\n",
    " \n",
    "- The Softmax Loss is the average of all data loss<br>\n",
    "  $$L = \\frac{1}{N}\\sum^N_{i=1}L_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mi1S0uZp9Dn5"
   },
   "outputs": [],
   "source": [
    "def softmax_loss(score, y):\n",
    "    \"\"\"Calculates the softmax loss for each row of the input x.\n",
    "    \n",
    "    Argument:\n",
    "    score -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "          -- normalized log probabilities score\n",
    "          \n",
    "    y     -- A numpy vector of shape(n,)\n",
    "          -- containing training labels;\n",
    "          -- y[i] = c means that X[i] has label c, where 0 <= c < C.\n",
    "\n",
    "    Returns:\n",
    "    dscore -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "           -- gradient score of softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    num_examples = score.shape[0]\n",
    "        \n",
    "    # 2. COMPUTE THE LOSS : average cross-entropy loss\n",
    "    \n",
    "    #make a number list containing [1 2 3 ... n]\n",
    "    number_list = ??\n",
    "    \n",
    "    # calculate the correct log probability of score[number_list,y] by applycing -np.log(...)\n",
    "    corect_logprobs = ??\n",
    "    \n",
    "    # average the correct log probability, use np.sum then divide it by num_examples\n",
    "    loss = ??\n",
    "    \n",
    "    \n",
    "    # 3. COMPUTE THE GRADIENT ON SCORES\n",
    "    dscores = score\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    \n",
    "    return loss, dscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkV7BrMh9Dn7"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_6zil3N9Dn8"
   },
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [1, 3, 0, 1 ,1],\n",
    "    [0, 2, 0, 1, 9],\n",
    "    [0, 1, 0, 1, 0]])\n",
    "\n",
    "y = np.array([\n",
    "    0, 1, 0\n",
    "])\n",
    "\n",
    "np.random.seed(2019)\n",
    "W = np.random.random((5, 2)) \n",
    "b = np.zeros((1, 2))\n",
    "\n",
    "score = forward(x, W, b)\n",
    "\n",
    "print(\"score = \" )\n",
    "print(score, '\\n')\n",
    "\n",
    "score = softmax(score)\n",
    "print(\"softmax(score) = \" )\n",
    "print(score,'\\n')\n",
    "\n",
    "loss, dscore = softmax_loss(score, y)\n",
    "print('loss   =',loss, '\\n')\n",
    "print('dscore =')\n",
    "print(dscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBvXw_F89DoD"
   },
   "source": [
    "**Expected Output**:\n",
    "<pre>score = \n",
    "[[4.35897229 3.61566867]\n",
    " [9.88257553 5.83070915]\n",
    " [1.32616823 1.54108356]] \n",
    "\n",
    "softmax(score) = \n",
    "[[0.67771785 0.32228215]\n",
    " [0.98290735 0.01709265]\n",
    " [0.44647702 0.55352298]] \n",
    "\n",
    "loss   = 1.7548327929166458 \n",
    "\n",
    "dscore =\n",
    "[[-0.10742738  0.10742738]\n",
    " [ 0.32763578 -0.32763578]\n",
    " [-0.18450766  0.18450766]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vl2_OZBK9DoE"
   },
   "source": [
    "---\n",
    "## 4 - Train a Softmax Classifier\n",
    "* Combine linear function and softmax function that you have completed to train a multiclass softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KO3lGVv9DoF"
   },
   "source": [
    "---\n",
    "### a. Weight Initialization\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Implement `Weight initialization function`\n",
    "    - receive input size `n`, number of neuron `d`, and standard deviation `std`\n",
    "    - generate normal random matrix `W` of size `(n x d)` and multiply it by `std`\n",
    "    - generate zeros matrix `b` of size `(1 x d)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HsDrps-i9DoF"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n, d, std):\n",
    "    W = ?? # random.random * std\n",
    "    b = ?\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITTqUp4c9DoJ"
   },
   "source": [
    "---\n",
    "### b. Training Function\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font> \n",
    "* Implement **Training Function**\n",
    "    * call `forward function`\n",
    "    * call `softmax function`\n",
    "    * call `softmax_loss function`\n",
    "    * call `backward function`\n",
    "    * implement `weight update`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISpmp2Oj9DoJ"
   },
   "outputs": [],
   "source": [
    "def train(X, y, W=None, b=None, learning_rate=1e-6, reg=1e4, num_iters=100, batch_size=200, verbose=False):\n",
    "    \n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "    \n",
    "    # get number of training data and its dimension from X\n",
    "    num_train, dim = ??\n",
    "    \n",
    "    # initialize new weights if W matrix is not provided\n",
    "    # call initialize_weights function using dim, num_classes, and std=0.001\n",
    "    if W is None:\n",
    "        W, b = ??\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "                     \n",
    "    for it in range(num_iters):\n",
    "        X_batch = None\n",
    "        y_batch = None\n",
    "\n",
    "        # Randomly select indices from training examples\n",
    "        train_rows = np.arange(num_train)\n",
    "        idxs = np.random.choice(train_rows, batch_size, replace=False)\n",
    "  \n",
    "        X_batch = X[idxs]\n",
    "        y_batch = y[idxs]\n",
    "\n",
    "\n",
    "        # calculate class score by calling forward function using X_batch, W, and b\n",
    "        scores = ??\n",
    "        \n",
    "        # calculate softmax score by calling softmax function using scores\n",
    "        softmaX_score = ??\n",
    "        \n",
    "        # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n",
    "        loss, dout = ??\n",
    "    \n",
    "        # append the loss history\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # calculate weights gradient by calling backward function using dout, X_batch, W, and b\n",
    "        dW, db, _ = ??\n",
    "        \n",
    "        # perform regulatization gradient\n",
    "        dW += reg*W\n",
    "        \n",
    "        # perform parameter update by subtracting W and b with a fraction of dW and db\n",
    "        # according to the learning rate\n",
    "        W -= ??\n",
    "        b -= ??\n",
    "        \n",
    "        if verbose and it % 100 == 0:\n",
    "            print ('iteration', it,'/',num_iters, ': loss =', loss)\n",
    "    return loss_history, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbGhYnV49DoL"
   },
   "source": [
    "---\n",
    "### c. Train the Softmax Classifier\n",
    "\n",
    "Try the training Function using the initial parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsvCPm849DoM"
   },
   "outputs": [],
   "source": [
    "loss, W, b = train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLHo9U5O9DoQ"
   },
   "source": [
    "Visualize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0lzRGK49DoR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_dspMjV9DoT"
   },
   "source": [
    "**Expected Output**:\n",
    "The graph should show that the loss is generally decreased as the training goes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6iAfJWB9DoU"
   },
   "source": [
    "---\n",
    "### d. Predict Function\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Implement **Predict Function**\n",
    "    * call `forward function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqxCYfon9DoU"
   },
   "outputs": [],
   "source": [
    "def predict(X, W, b):    \n",
    "    y_pred = np.zeros(X.shape[1])\n",
    "\n",
    "    \n",
    "    # calculate class score by calling forward function using X, W, and b\n",
    "    y_pred = ??\n",
    "    \n",
    "    # take the maximum prediction and use that column to get the class     \n",
    "    y_pred = y_pred.argmax(axis=-1)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCpnbDZO9DoY"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6JAjNgRU9DoY"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = predict(X_train, W, b)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n",
    "\n",
    "print('Training Accuracy =', accuracy*100,'%')\n",
    "\n",
    "print('Training label  =', y_test[:15])\n",
    "print('Predicted label =', y_pred[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OplASJoa9Dob"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "You should get about **`27-29%`** accuracy on training set using the initial run\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMeU07P59Doc"
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X_val, W, b)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n",
    "print('Validation Accuracy =', accuracy*100,'%')\n",
    "\n",
    "print('Validation label =',y_test[:15])\n",
    "print('Predicted label  =',y_pred[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XTMNd3Ud9Dof"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "You should also get about **`27-29%`** accuracy on validation set\n",
    "\n",
    "You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSC5w5Bw9Dof"
   },
   "source": [
    "---\n",
    "## 5 - Hyperparameter Tuning\n",
    "* After you find that the classifier is working properly, next\n",
    "* Find the best Learning Rate and Regularization Strength\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Use the `validation set` to tune hyperparameters (regularization strength and learning rate). \n",
    "     * Greedily loop over `learning_rates` and `regulatization` to get all combination\n",
    "     \n",
    "     \n",
    "* You **should** experiment with different ranges for the learning rates and regularization strengths; \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZSdjo7V9Dof"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_W = None\n",
    "best_b = None\n",
    "learning_rates = [1e-6, 1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "iterations = 2000\n",
    "\n",
    "# Greedily loop over learning_rates and regularization_strengths\n",
    "for ??:\n",
    "    for ??:\n",
    "        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, rate, reg))\n",
    "        # call train function using the learning rate and regularization selected\n",
    "        loss, W, b = train(X_train, y_train, \n",
    "                              learning_rate=??, reg=??,\n",
    "                              num_iters=??, verbose=False)\n",
    "        \n",
    "        # call predict function using pretrained W and b on X_train and X_val to evaluate\n",
    "        y_train_pred = predict(??, ??, ??)\n",
    "        y_val_pred = predict(??, ??, ??)\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        val_accuracy = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        print ('rate = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n",
    "            rate, reg, train_accuracy, val_accuracy))\n",
    "        \n",
    "        # store the result accuracy combination\n",
    "        results[(rate, reg)] = (train_accuracy, val_accuracy)\n",
    "        \n",
    "        # store the best Weight and Bias\n",
    "        if val_accuracy > best_val:\n",
    "            best_W = W\n",
    "            best_b = b\n",
    "            best_val = val_accuracy\n",
    "\n",
    "\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr',lr,',reg',reg,', train accuracy: ',train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n",
    "    \n",
    "print()\n",
    "print('best validation accuracy achieved during cross-validation:',best_val*100,'%')\n",
    "print('best Weights and bias are stored in `best_W` and `best_b`')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvoX3R9b9Doi"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "if you are careful you should be able to get a classification accuracy of **over 35%** on the validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyocgmtR9Doi"
   },
   "source": [
    "---\n",
    "## 6 - Try Another Hyperparameter Combination\n",
    "\n",
    "Try to add another combination of learning rates and regularization strength hyperparameter, and repeat the `step 5`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mAms-8F9Doi"
   },
   "source": [
    "---\n",
    "## 7 - Test the Trained Weights\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Test the **Best Weights and Bias** to the `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rxRWuP59Doj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(??, ??, ??)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Testing Accuracy =', accuracy*100,'%')\n",
    "print('Test label      =',y_test[:15])\n",
    "print('Predicted label =',y_pred[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM85_AmY9Dol"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "You should reach at least  **32%** accuracy on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEOTrqQw9Dom"
   },
   "source": [
    "---\n",
    "## 8 - Vizualize the Trained Weights \n",
    "\n",
    "Visualize the learned weights for each class for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPsnDOay9Dom"
   },
   "source": [
    "---\n",
    "### a. Retrieve one class weights\n",
    "Try to visualize one trained weights.<br><br>\n",
    "**Note:** You can change the `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ga_gct3-9Dom"
   },
   "outputs": [],
   "source": [
    "id = 0\n",
    "\n",
    "# get the id-th column from best weight\n",
    "w = ??\n",
    "\n",
    "print('w.shape =',w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mroPU69R9Doo"
   },
   "source": [
    "**Expected Output**:\n",
    "<pre>\n",
    "w.shape = (3072,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WMK5MRI9Dop"
   },
   "source": [
    "---\n",
    "### b. Normalize the weight for visualization\n",
    "Image plot can only receive input range [0..1] for float type, or [0..255] for integer type, and the weights are not in those range.\n",
    "\n",
    "So we need to normalize. Here we use MinMaxScaler from sklearn toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCoMNGas9Dop"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler((0.05,.95))\n",
    "w = scaler.fit_transform(w.reshape(-1, 1))\n",
    "\n",
    "print('w.shape =',w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6hJ4PJU79Dor"
   },
   "source": [
    "**Expected Output**:\n",
    "<pre>\n",
    "w.shape = (3072, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpUlVHil9Dor"
   },
   "source": [
    "---\n",
    "### c. Visualize the weight \n",
    "Now reshape the weight back to (32,32,3), and plot the image. Use `np.reshape()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "op2LEL-x9Dos"
   },
   "outputs": [],
   "source": [
    "w = ??\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(w)\n",
    "plt.title(classes[id])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KHoUj9Tg9Dot"
   },
   "source": [
    "---\n",
    "### d. Do it to the other weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-9SIJDu9Dot",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,10,figsize=(18,5))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "for i in range(0, 10):\n",
    "    # get the i-th weight from best_W\n",
    "    w = ??\n",
    "    \n",
    "    # normalize the weight (see previous example)\n",
    "    w = ??\n",
    "    \n",
    "    # reshape the weight\n",
    "    w = ??\n",
    "    \n",
    "    ax[i].imshow(w)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(classes[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ucAtH6r9Dox"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "You should see that the weights visualization should  match the ilustration below\n",
    "\n",
    "![cifar10](http://cs231n.github.io/assets/templates.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpOR1Uww9Doz"
   },
   "source": [
    "---\n",
    "## 9 - Linear Classifier: Multiclass SVM\n",
    "Similar to before, let's build a Linear classifier to classify cifar-10 dataset, but now using Multiclass SVM loss, or Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0NwIDkX09Doz"
   },
   "source": [
    "---\n",
    "### a. SVM Loss\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Implement `Multiclass SVM Loss` or `Hinge Loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mp767_w69Do0"
   },
   "outputs": [],
   "source": [
    "def svm_loss(score, y):\n",
    "    \"\"\"Calculates the smb loss for each row of the input x.\n",
    "    \n",
    "    Argument:\n",
    "    score -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "          -- normalized log probabilities score\n",
    "          \n",
    "    y     -- A numpy vector of shape(n,)\n",
    "          -- containing training labels;\n",
    "          -- y[i] = c means that X[i] has label c, where 0 <= c < C.\n",
    "\n",
    "    Returns:\n",
    "    dscore -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "           -- gradient score of softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    num_examples = score.shape[0]\n",
    "    \n",
    "    \n",
    "    # 1. COMPUTE THE SVM LOSS\n",
    "    \n",
    "    # get the correct class score. Use np.choose( , ) using y and score.T\n",
    "    correct_scores = ??\n",
    "    \n",
    "    # calculate margin: score.T subtracted by correct_score, then add by 1\n",
    "    margin = ??\n",
    "    \n",
    "    # remove all margin below 0. Use np.maximum(0, margin)\n",
    "    margin = ??\n",
    "        \n",
    "    # COMPUTE THE LOSS \n",
    "    # calculate SVM loss of each data: sum of margin of non-target class\n",
    "    # simply total all the margin (use np.sum()), and subtract it by 1\n",
    "    loss_i = ??\n",
    "    \n",
    "    \n",
    "    \n",
    "    # average loss_i by num_examples\n",
    "    loss = ??\n",
    "    \n",
    "    \n",
    "    # 2. COMPUTE THE GRADIENT ON SCORES\n",
    "    # margins for the correct class have already been set to 0\n",
    "    dscores = (margin.T > 0).astype(float)\n",
    "    marginsSum = dscores.sum(1) - 1\n",
    "    dscores[range(dscores.shape[0]), y] = -marginsSum\n",
    "        \n",
    "    return loss, dscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukym7aAK9Do1"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxevLrVy9Do1"
   },
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0],\n",
    "    [1, 0, 3, 5 ,0]])\n",
    "\n",
    "y = np.array([\n",
    "    0, 0, 1\n",
    "])\n",
    "\n",
    "np.random.seed(20)\n",
    "W = np.random.random((5, 2)) \n",
    "b = np.zeros((1, 2))\n",
    "\n",
    "score = forward(x, W, b)\n",
    "\n",
    "print(\"score = \" )\n",
    "print(score)\n",
    "print()\n",
    "\n",
    "loss, dscore = svm_loss(score, y)\n",
    "print('loss   =',loss)\n",
    "print('dscore =')\n",
    "print(dscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKjCMyCY9Do3"
   },
   "source": [
    "**Expected Output**:\n",
    "<pre>\n",
    "score = \n",
    "[[ 7.2556866  13.16988641]\n",
    " [ 8.57456925 10.36318348]\n",
    " [ 2.58920427  5.5655412 ]]\n",
    "\n",
    "loss   = 3.234271348285095\n",
    "dscore =\n",
    "[[-1.  1.]\n",
    " [-1.  1.]\n",
    " [ 0. -0.]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGGXv4Zn9Do3"
   },
   "source": [
    "---\n",
    "\n",
    "Notice the loss gradients resulted from SVM Loss. \n",
    "\n",
    "What's the difference between those gradients and Softmax Gradients?\n",
    "\n",
    "What are the effects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AR0Ptu929Do3"
   },
   "source": [
    "---\n",
    "### b. Train Multiclass SVM Classifier\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* The same as the training function using Softmax before, complete the train function below, \n",
    "* but rather than using `softmax_loss`, use `svm_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYNpfn4g9Do4"
   },
   "outputs": [],
   "source": [
    "def train_svm(X, y, W=None, b=None, learning_rate=1e-6, reg=1e4, num_iters=100, batch_size=200, verbose=False):\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "    \n",
    "    if W is None:\n",
    "        W = 0.001 * np.random.randn(dim, num_classes)\n",
    "    if b is None:\n",
    "        b = np.zeros((1,num_classes))\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "                     \n",
    "    for it in range(num_iters):\n",
    "        X_batch = None\n",
    "        y_batch = None\n",
    "\n",
    "        # Randomly select indices from training examples\n",
    "        train_rows = np.arange(num_train)\n",
    "        idxs = np.random.choice(train_rows, batch_size, replace=False)\n",
    "  \n",
    "        X_batch = X[idxs]\n",
    "        y_batch = y[idxs]\n",
    "\n",
    "\n",
    "        # calculate class score by calling forward function using X_batch, W, and b\n",
    "        scores = ??\n",
    "                \n",
    "        # evaluate loss and gradient by calling svm_loss function using score and y_batch\n",
    "        loss, dout = ??\n",
    "    \n",
    "        # append the loss history\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # calculate weights gradient by calling backward function using dout, X_batch, W, and b\n",
    "        dW, db, _ = ??\n",
    "        \n",
    "        # perform regulatization gradient\n",
    "        dW += reg*W\n",
    "        \n",
    "        # perform parameter update by subtracting W and b with a fraction of dW and db\n",
    "        # according to the learning rate\n",
    "        W -= ??\n",
    "        b -= ??\n",
    "        \n",
    "        if verbose and it % 100 == 0:\n",
    "            print ('iteration', it,'/',num_iters, ': loss =', loss)\n",
    "    return loss_history, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfplBGaT9Do5"
   },
   "source": [
    "---\n",
    "### c. Hyperparameter Tuning for Multiclass SVM Classifier\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* Again, complete the codes below similar to the Hyperparameter tuning for `Softmax Classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_kPgMF79Do6"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_W = None\n",
    "best_b = None\n",
    "learning_rates = [1e-6, 1e-7, 5e-7]\n",
    "regularization_strengths = [3e4, 5e4]\n",
    "\n",
    "iterations = 2000\n",
    "\n",
    "# Greedily loop over learning_rates and regularization_strengths\n",
    "for ??:\n",
    "    for ??:\n",
    "        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, rate, reg))\n",
    "        # call train function using the learning rate and regularization selected\n",
    "        loss, W, b = train(X_train, y_train, \n",
    "                              learning_rate=??, reg=??,\n",
    "                              num_iters=??, verbose=False)\n",
    "        \n",
    "        # call predict function using pretrained W and b on X_train and X_val to evaluate\n",
    "        y_train_pred = predict(??, ??, ??)\n",
    "        y_val_pred = predict(??, ??, ??)\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        val_accuracy = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        print ('rate = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n",
    "            rate, reg, train_accuracy, val_accuracy))\n",
    "        \n",
    "        # store the result accuracy combination\n",
    "        results[(rate, reg)] = (train_accuracy, val_accuracy)\n",
    "        \n",
    "        # store the best Weight and Bias\n",
    "        if val_accuracy > best_val:\n",
    "            best_W = W\n",
    "            best_b = b\n",
    "            best_val = val_accuracy\n",
    "\n",
    "            \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr',lr,',reg',reg,', train accuracy: ', train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n",
    "    \n",
    "print()\n",
    "print('best validation accuracy achieved during cross-validation:', best_val*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HACKUtEn9Do7"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "You should reach `at least`  **`30%`** accuracy on cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFFir43e9Do8"
   },
   "source": [
    "---\n",
    "### d. Visualize the Weights Trained\n",
    "<br>\n",
    "\n",
    "<font color='red'>**EXERCISE**: </font>\n",
    "* complete the implementation to show the trained weight visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyG_2yWO9Do8"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,10,figsize=(18,5))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "for i in range(0, 10):\n",
    "    # get the i-th weight\n",
    "    w = ??\n",
    "    \n",
    "    # normalize the weight\n",
    "    w = ??\n",
    "    \n",
    "    # reshape the weight\n",
    "    w = ??\n",
    "    \n",
    "    ax[i].imshow(w)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(classes[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGwFLO299Do9"
   },
   "source": [
    "### Notice the difference in weight visualization\n",
    "Describe what your visualized SVM weights look like. Think on why they look they way that they do.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quMA17JKDnpI"
   },
   "source": [
    "---\n",
    "\n",
    "# Congratulation, You've Completed Exercise 1\n",
    "\n",
    "<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_Hkr9kg9Do-"
   },
   "source": [
    "![footer](https://image.ibb.co/hAHDYK/footer2018.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN2019 - 01 - Linear Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
