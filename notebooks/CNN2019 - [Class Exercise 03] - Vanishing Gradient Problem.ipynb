{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN2019 - [Class Exercise 03] - Vanishing Gradient Problem.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AwZB6XzTCkOF"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","# [Class Exercise] Vanishing Gradient Problems\n","\n","In this exercise you will investigate the problem of classic Neural Network, including:\n","\n","    * Vanishing Gradient Problem\n","    * Weight Initialization"]},{"cell_type":"code","metadata":{"id":"rmb3w0Si0G5b","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.set_printoptions(precision=7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wi9J9cgblNvj","colab_type":"text"},"source":["---\n","# Basic API"]},{"cell_type":"markdown","metadata":{"id":"bUZrJO7kphMs","colab_type":"text"},"source":["As explained earlier, the standards of Artificial Neural Networks experience several problems that cause the Network can not be deepened. One of them is the **Vanishing Gradient Problem** problem,\n","\n","To explore this issue, first let's re-create the Neural Net API implementation as before"]},{"cell_type":"code","metadata":{"id":"ipCVw1Anc31h","colab_type":"code","colab":{}},"source":["def affine_forward(x, W, b):   \n","  \n","    v = ??\n","    \n","    cache = (x, W, b)\n","    \n","    return v, cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5-rnF-qc3y3","colab_type":"code","colab":{}},"source":["def affine_backward(dout, cache):\n","    \n","    x, W, b = cache\n","    \n","    dW = ??\n","    \n","    db = ??\n","    \n","    dx = ??\n","    \n","    return dW, db, dx"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0GrKHRJc3v9","colab_type":"code","colab":{}},"source":["def sigmoid_forward(x):  \n","  \n","    out = ??\n","    \n","    return out  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vv6sCMLRbzrd","colab_type":"code","colab":{}},"source":["def sigmoid_backward(dout, ds):\n","  \n","    ds_ = ??\n","    \n","    dout = ??\n","    \n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQtqNhBDlQ-j","colab_type":"code","colab":{}},"source":["def tanh_forward(x):     \n","  \n","    out = ??\n","    \n","    cache = 1-out**2\n","    \n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dTmbMPfplWix","colab_type":"code","colab":{}},"source":["def relu_forward(x):\n","  \n","    out = ??\n","    \n","    cache = x\n","    \n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9MSUizgpeyK","colab_type":"text"},"source":["---\n","# Vanishing Gradient Problem"]},{"cell_type":"markdown","metadata":{"id":"lPVCdwdZqmCg","colab_type":"text"},"source":["---\n","## Vanishing Gradient in Sigmoid"]},{"cell_type":"markdown","metadata":{"id":"agWfzSMIqKgI","colab_type":"text"},"source":["The Sigmoid and Tanh Activation Function is an activation function that is almost always used at the beginning of the development of the Neural Network because it has a proper interpretation and intusion of the **firing rate** of neurons. Sigmoid suppresses output into the range $[0..1]$ (and the range $[- 1..1]$ for tanh).\n","\n","*Sigmoid function* | *Tanh function*\n","-- | --\n","![sigmoid](http://cs231n.github.io/assets/nn1/sigmoid.jpeg) | ![tanh](http://cs231n.github.io/assets/nn1/tanh.jpeg)\n","\n","This process causes the higher sigmoid input to get closer to the value of 1 output with very high decimal precision. And this will cause the value of the gradient returned to be smaller.\n","\n","Consider the example below"]},{"cell_type":"code","metadata":{"id":"4jEdzjUybzod","colab_type":"code","colab":{}},"source":["\n","print('sigmoid(.01) =',sigmoid_forward(.01))\n","print('sigmoid(.1)  =',sigmoid_forward(.1))\n","print('sigmoid(.5)  =',sigmoid_forward(.5))\n","print('sigmoid(1)   =',sigmoid_forward(1))\n","print('sigmoid(10)  =',sigmoid_forward(10))\n","print('sigmoid(50)  =',sigmoid_forward(50))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTtLvRbwbzif","colab_type":"code","colab":{}},"source":["\n","print('sigmoid backward(50)  =',sigmoid_backward(1,sigmoid_forward(50)))\n","print('sigmoid backward(10)  =',sigmoid_backward(1,sigmoid_forward(10)))\n","print('sigmoid backward(1)   =',sigmoid_backward(1,sigmoid_forward(1)))\n","print('sigmoid backward(0.5) =',sigmoid_backward(1,sigmoid_forward(.5)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L1Zdhpt8qWnX","colab_type":"text"},"source":["\n","Note that the smaller the input, the smaller the gradient\n","\n","And this gets worse if we have multiple layers. \n","\n","Because if we have a gradient matrix that contains decimal values, and when back propagation we multiply the gradient with the gradient layer that also contains decimal values, then this multiplication will certainly produce even smaller decimal values.\n","\n","<img src=\"https://image.ibb.co/jSc4ve/gradien.jpg\" alt=\"gradien\"/>\n","\n","If we bring this value up to the first layer, then the remaining gradient value at the end of the layer will limit to the value 0, or even because of the limited storage bits on the computer causing the gradient to 0. \n","\n","And you know that the gradient is a value that must be added to the weight matrix so that the network improves"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hu6qEGDuqq7n"},"source":["---\n","## Vanishing Gradient in Neural Network"]},{"cell_type":"markdown","metadata":{"id":"3KdlfXtIqt4b","colab_type":"text"},"source":["Now, let's try to see what happens to the gradient if we do training on a 10-layer Artificial Neural Network\n","\n","First we generate 1000 data with 500 attributes"]},{"cell_type":"code","metadata":{"id":"dqRUez_adNTn","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_classification\n","from sklearn.preprocessing import minmax_scale\n","\n","x,y = make_classification(n_samples=1000, n_features=500)\n","x = minmax_scale(x, feature_range=(-1, 1))\n","\n","y = np.expand_dims(y, 1)\n","nfitur = x.shape[1]\n","nlabel = y.shape[1]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ub56GZAOq6jM","colab_type":"text"},"source":["initializing 10 layers of artificial neural networks with 512 neurons in each layer"]},{"cell_type":"code","metadata":{"id":"YgM8t-zUdNQz","colab_type":"code","colab":{}},"source":["nhidden = 512\n","\n","w0 = np.random.randn(nfitur , nhidden)*.01 \n","w1 = np.random.randn(nhidden, nhidden)*.01\n","w2 = np.random.randn(nhidden, nhidden)*.01\n","w3 = np.random.randn(nhidden, nhidden)*.01\n","w4 = np.random.randn(nhidden, nhidden)*.01\n","w5 = np.random.randn(nhidden, nhidden)*.01\n","w6 = np.random.randn(nhidden, nhidden)*.01\n","w7 = np.random.randn(nhidden, nhidden)*.01\n","w8 = np.random.randn(nhidden, nlabel )*.01\n","b0 = np.zeros((1, nhidden))\n","b1 = np.zeros((1, nhidden))\n","b2 = np.zeros((1, nhidden))\n","b3 = np.zeros((1, nhidden))\n","b4 = np.zeros((1, nhidden))\n","b5 = np.zeros((1, nhidden))\n","b6 = np.zeros((1, nhidden))\n","b7 = np.zeros((1, nhidden))\n","b8 = np.zeros((1, nlabel))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKWn8nmirAJ5","colab_type":"text"},"source":["***\n","### Forward Pass\n","Let's try forward propagation on the network and see the average activation in each layer"]},{"cell_type":"code","metadata":{"id":"w4g744u7dNNs","colab_type":"code","colab":{}},"source":["# proses maju\n","layer1, cache1 = affine_forward(x, w0, b0)\n","aktivasi1 = sigmoid_forward(layer1)\n","\n","layer2, cache2 = affine_forward(aktivasi1, w1, b1)\n","aktivasi2 = sigmoid_forward(layer2)\n","\n","layer3, cache3 = affine_forward(aktivasi2, w2, b2)\n","aktivasi3 = sigmoid_forward(layer3)\n","\n","layer4, cache4 = affine_forward(aktivasi3, w3, b3)\n","aktivasi4 = sigmoid_forward(layer4)\n","\n","layer5, cache5 = affine_forward(aktivasi4, w4, b4)\n","aktivasi5 = sigmoid_forward(layer5)\n","\n","layer6, cache6 = affine_forward(aktivasi5, w5, b5)\n","aktivasi6 = sigmoid_forward(layer6)\n","\n","layer7, cache7 = affine_forward(aktivasi6, w6, b6)\n","aktivasi7 = sigmoid_forward(layer7)\n","\n","layer8, cache8 = affine_forward(aktivasi7, w7, b7)\n","aktivasi8 = sigmoid_forward(layer8)\n","\n","layer9, cache9 = affine_forward(aktivasi8, w8, b8)\n","aktivasi9 = sigmoid_forward(layer9)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8Ecy6bDd2xP","colab_type":"code","colab":{}},"source":["print('mean activation of layer 1 = ',np.mean(np.mean(aktivasi1)))\n","print('mean activation of layer 3 = ',np.mean(np.mean(aktivasi3)))\n","print('mean activation of layer 5 = ',np.mean(np.mean(aktivasi5)))\n","print('mean activation of layer 7 = ',np.mean(np.mean(aktivasi7)))\n","print('mean activation of layer 9 = ',np.mean(np.mean(aktivasi9)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pu1q_ioQrIpR","colab_type":"text"},"source":["it might seem that the activation rates are not much different. \n","\n","This is because the new network just starts, so that the contents of the weight of each layer are still equal"]},{"cell_type":"markdown","metadata":{"id":"kVhOuNeLsmRs","colab_type":"text"},"source":["***\n","### Backward Pass\n","Now, let's look at the backward process to calculate the gradient"]},{"cell_type":"code","metadata":{"id":"HiKVXLybdNK9","colab_type":"code","colab":{}},"source":["\n","# hitung error\n","error = y - aktivasi9\n","print(\"mse = %0.7f\" % (np.mean(error ** 2)))\n","\n","\n","#proses mundur\n","g_layer9 = sigmoid_backward(error, aktivasi9)\n","dw8, db8, g_aktivasi8 = affine_backward(g_layer9, cache9)\n","\n","g_layer8 = sigmoid_backward(g_aktivasi8, aktivasi8)\n","dw7, db7, g_aktivasi7 = affine_backward(g_layer8, cache8)\n","\n","g_layer7 = sigmoid_backward(g_aktivasi7, aktivasi7)\n","dw6, db6, g_aktivasi6 = affine_backward(g_layer7, cache7)\n","\n","g_layer6 = sigmoid_backward(g_aktivasi6, aktivasi6)\n","dw5, db5, g_aktivasi5 = affine_backward(g_layer6, cache6)\n","\n","g_layer5 = sigmoid_backward(g_aktivasi5, aktivasi5)\n","dw4, db4, g_aktivasi4 = affine_backward(g_layer5, cache5)\n","\n","g_layer4 = sigmoid_backward(g_aktivasi4, aktivasi4)\n","dw3, db3, g_aktivasi3 = affine_backward(g_layer4, cache4)\n","\n","g_layer3 = sigmoid_backward(g_aktivasi3, aktivasi3)\n","dw2, db2, g_aktivasi2 = affine_backward(g_layer3, cache3)\n","\n","g_layer2 = sigmoid_backward(g_aktivasi2, aktivasi2)\n","dw1, db1, g_aktivasi1 = affine_backward(g_layer2, cache2)\n","\n","g_layer1 = sigmoid_backward(g_aktivasi1, aktivasi1)\n","dw0, db0, dx = affine_backward(g_layer1, cache1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICLIYxU6ez-s","colab_type":"code","colab":{}},"source":["\n","print('mean gradient of layer 9 = ',np.mean(np.mean(g_aktivasi8)))\n","print('mean gradient of layer 7 = ',np.mean(np.mean(g_aktivasi6)))\n","print('mean gradient of layer 5 = ',np.mean(np.mean(g_aktivasi4)))\n","print('mean gradient of layer 3 = ',np.mean(np.mean(g_aktivasi2)))\n","print('mean gradient of layer 1 = ',np.mean(np.mean(dx)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6YUNRKkPsssh","colab_type":"text"},"source":["It can be seen that the more forward, the smaller the gradient.\n","\n","And the gradient is not directly added to the weight, but only a small portion of it because it will be multiplied by the learning rate first"]},{"cell_type":"markdown","metadata":{"id":"ysTkQ9p5In8L","colab_type":"text"},"source":["---\n","---\n","# Weight Initialization Technique\n","\n","Generally, people initialize weight parameter at random, practically using gaussian with zero mean and 1e-2 std\n","`0.01 * randn(dim, hid)` scheme, it works ~okay for small networks, but mostly it will be a problem and deeper networks\n","\n","let's say we build a layer neural net for an input of 500 parameters with 10 hidden layers, each having 500 neurons as follows\n"]},{"cell_type":"code","metadata":{"id":"xfg10Y1WIn8L","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# generate random input\n","D = np.random.randn(1000, 500)\n","\n","# 10 hidden layers, each 500 neurons\n","num_hid = [500] * 10\n","\n","# container for hidden layer output (as input to its intermediate layers)\n","Hs={}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDU26v5cIn8P","colab_type":"text"},"source":["---\n","## Initialize Weight with small random number (std=0.01)\n","\n","Let's try to initialize with the common technique,\n","\n","Then we iterate through the network, where each layers we generate random `W` parameter, and propagate the score through all 10 hidden layer"]},{"cell_type":"code","metadata":{"id":"NlsUtyRhIn8Q","colab_type":"code","colab":{}},"source":["X = D\n","for i in range(len(num_hid)):\n","    # if its not input layer, input is the output from previous layer\n","    if i>0:\n","        X = Hs[i-1]\n","    d_in = X.shape[1]\n","    d_out = num_hid[i]\n","    W = 0.01 * np.random.randn(d_in,d_out)\n","    b = np.zeros((1, d_out))\n","\n","    H, _ = affine_forward(X, W, b)\n","    H, _ = tanh_forward(H)\n","    Hs[i] = H\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5X070vpWIn8Z","colab_type":"text"},"source":["Visualize the weight and output distribution (mean and standard deviasion) over the layer"]},{"cell_type":"code","metadata":{"id":"Gi6EqeKxIn8l","colab_type":"code","colab":{}},"source":["print('input layer had meann %f and std %f' % (np.mean(D), np.std(D)))\n","layer_means = [np.mean(H) for i, H in Hs.items()]\n","layer_stds = [np.std(H) for i, H in Hs.items()]\n","for i, H in Hs.items():\n","    print ('hidden layer %d had mean %f and std %f' % (i + 1, layer_means[i], layer_stds[i]))\n","\n","plt.figure(figsize=(15,4))\n","plt.subplot(121)\n","plt.plot(list(Hs.keys()), layer_means, 'ob-')\n","plt.title('layer mean')\n","plt.subplot(122)\n","plt.plot(list(Hs.keys()), layer_stds, 'or-')\n","plt.title('layer std')\n","\n","plt.figure(figsize=(15,4))\n","for i, H in Hs.items():\n","    plt.subplot(1, len(Hs), i + 1)\n","    plt.hist(H.ravel(), 30, range=(-1, 1))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yoOuvr0sIn8p","colab_type":"text"},"source":["it can be seen that over the layer, all activations is super saturated and become zeros, and this is not good, as the gradient will vanish as it propagate back through the layers, the gradient will be all zero in earlier layer, thus those layers cannot learn and update its weights"]},{"cell_type":"markdown","metadata":{"id":"zOgeZQsuIn8s","colab_type":"text"},"source":["---\n","## Initialize Weight with JUST random number (std=1)\n","\n","Now let's see if we just initialize with random number"]},{"cell_type":"code","metadata":{"id":"b34vX4UvIn8v","colab_type":"code","colab":{}},"source":["X = D\n","for i in range(len(num_hid)):\n","    # if its not input layer, input is the output from previous layer\n","    if i>0:\n","        X = Hs[i-1]\n","    d_in = X.shape[1]\n","    d_out = num_hid[i]\n","    W = np.random.randn(d_in,d_out)\n","    b = np.zeros((1, d_out))\n","\n","    H, _ = affine_forward(X, W, b)\n","    H, _ = tanh_forward(H)\n","    Hs[i] = H\n","\n","    \n","print('input layer had meann %f and std %f' % (np.mean(D), np.std(D)))\n","layer_means = [np.mean(H) for i, H in Hs.items()]\n","layer_stds = [np.std(H) for i, H in Hs.items()]\n","for i, H in Hs.items():\n","    print ('hidden layer %d had mean %f and std %f' % (i + 1, layer_means[i], layer_stds[i]))\n","\n","plt.figure(figsize=(15,4))\n","plt.subplot(121)\n","plt.plot(list(Hs.keys()), layer_means, 'ob-')\n","plt.title('layer mean')\n","plt.subplot(122)\n","plt.plot(list(Hs.keys()), layer_stds, 'or-')\n","plt.title('layer std')\n","\n","plt.figure(figsize=(15,4))\n","for i, H in Hs.items():\n","    plt.subplot(1, len(Hs), i + 1)\n","    plt.hist(H.ravel(), 30, range=(-1, 1))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ohqWy14PIn8y","colab_type":"text"},"source":["---\n","## Xavier Weight Initialization\n","\n","on 2010, Glorot et al proposed a different and more reasonable sceme of weight initialization which is then called the Xavier Initialization \n","\n","`W = randn(nim, hid) / sqrt(dim)`"]},{"cell_type":"code","metadata":{"id":"8H8SK7CPIn8z","colab_type":"code","colab":{}},"source":["X = D\n","for i in range(len(num_hid)):\n","    # if its not input layer, input is the output from previous layer\n","    if i>0:\n","        X = Hs[i-1]\n","    d_in = X.shape[1]\n","    d_out = num_hid[i]\n","    W = np.random.randn(d_in, d_out) / np.sqrt(d_in)\n","    b = np.zeros((1, d_out))\n","\n","    H, _ = affine_forward(X, W, b)\n","    H, _ = tanh_forward(H)\n","    Hs[i] = H\n","    \n","print('input layer had meann %f and std %f' % (np.mean(D), np.std(D)))\n","layer_means = [np.mean(H) for i, H in Hs.items()]\n","layer_stds = [np.std(H) for i, H in Hs.items()]\n","for i, H in Hs.items():\n","    print ('hidden layer %d had mean %f and std %f' % (i + 1, layer_means[i], layer_stds[i]))\n","\n","plt.figure(figsize=(15,4))\n","plt.subplot(121)\n","plt.plot(list(Hs.keys()), layer_means, 'ob-')\n","plt.title('layer mean')\n","plt.subplot(122)\n","plt.plot(list(Hs.keys()), layer_stds, 'or-')\n","plt.title('layer std')\n","\n","plt.figure(figsize=(15,4))\n","for i, H in Hs.items():\n","    plt.subplot(1, len(Hs), i + 1)\n","    plt.hist(H.ravel(), 30, range=(-1, 1))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b1kebxNvIn84","colab_type":"text"},"source":["it can be seen that the distribution became more sensible, and nothing is super saturated"]},{"cell_type":"markdown","metadata":{"id":"574uKplmIn85","colab_type":"text"},"source":["---\n","## Xavier Weight Initialization in ReLU\n","\n","however, later when the initialization is then applied to ReLU activation unit, it breaks"]},{"cell_type":"code","metadata":{"id":"n4seaKSeIn88","colab_type":"code","colab":{}},"source":["X = D\n","for i in range(len(num_hid)):\n","    # if its not input layer, input is the output from previous layer\n","    if i>0:\n","        X = Hs[i-1]\n","    d_in = X.shape[1]\n","    d_out = num_hid[i]\n","    W = np.random.randn(d_in, d_out) / np.sqrt(d_in)\n","    b = np.zeros((1, d_out))\n","\n","    H, _ = affine_forward(X, W, b)\n","    H, _ = relu_forward(H)\n","    Hs[i] = H\n","    \n","print('input layer had meann %f and std %f' % (np.mean(D), np.std(D)))\n","layer_means = [np.mean(H) for i, H in Hs.items()]\n","layer_stds = [np.std(H) for i, H in Hs.items()]\n","for i, H in Hs.items():\n","    print ('hidden layer %d had mean %f and std %f' % (i + 1, layer_means[i], layer_stds[i]))\n","\n","plt.figure(figsize=(15,4))\n","plt.subplot(121)\n","plt.plot(list(Hs.keys()), layer_means, 'ob-')\n","plt.title('layer mean')\n","plt.subplot(122)\n","plt.plot(list(Hs.keys()), layer_stds, 'or-')\n","plt.title('layer std')\n","\n","plt.figure(figsize=(15,4))\n","for i, H in Hs.items():\n","    plt.subplot(1, len(Hs), i + 1)\n","    plt.hist(H.ravel(), 30, range=(-1, 1))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HU1mr7zRIn8-","colab_type":"text"},"source":["---\n","## Kaiming He Weight Initialization\n","\n","But as it turns out, the solution is much more simple, as it was pointed out by Kaiming He et al that to solve this problem, the initialization is simply need to be divided not ny `sqrt(dim)` but by `sqrt(dim/2)`\n","\n","As the ReLU “half” the distribution (variance), so to initiate the weight we have to account the factor of 2\n"]},{"cell_type":"code","metadata":{"id":"P6SUATfNIn9A","colab_type":"code","colab":{}},"source":["X = D\n","for i in range(len(num_hid)):\n","    # if its not input layer, input is the output from previous layer\n","    if i>0:\n","        X = Hs[i-1]\n","    d_in = X.shape[1]\n","    d_out = num_hid[i]\n","    W = np.random.randn(d_in, d_out) / np.sqrt(d_in/2)\n","    b = np.zeros((1, d_out))\n","\n","    H, _ = affine_forward(X, W, b)\n","    H, _ = relu_forward(H)\n","    Hs[i] = H\n","    \n","print('input layer had meann %f and std %f' % (np.mean(D), np.std(D)))\n","layer_means = [np.mean(H) for i, H in Hs.items()]\n","layer_stds = [np.std(H) for i, H in Hs.items()]\n","for i, H in Hs.items():\n","    print ('hidden layer %d had mean %f and std %f' % (i + 1, layer_means[i], layer_stds[i]))\n","\n","plt.figure(figsize=(15,4))\n","plt.subplot(121)\n","plt.plot(list(Hs.keys()), layer_means, 'ob-')\n","plt.title('layer mean')\n","plt.subplot(122)\n","plt.plot(list(Hs.keys()), layer_stds, 'or-')\n","plt.title('layer std')\n","\n","plt.figure(figsize=(15,4))\n","for i, H in Hs.items():\n","    plt.subplot(1, len(Hs), i + 1)\n","    plt.hist(H.ravel(), 30, range=(-1, 1))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TgOzT5-vIn9D","colab_type":"text"},"source":["---\n","<font size=6><b>Xavier Init and Kaiming He Init is the current default Initialization</b></font>"]},{"cell_type":"markdown","metadata":{"id":"pYqI89dWTmEX","colab_type":"text"},"source":["\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"q__eGSJ8CkeK"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}