{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNN2019 - 04 - Layerwise Pretraining.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"hPwFEN34mNUP","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sjlcHwermNUS","colab_type":"text"},"source":["\n","# Task 4 - Layerwise Pretrained Neural Network\n","\n","\n","In this assignment you will practice putting together a simple image classification pipeline, based on the Multi-layer Neural Network classifier which is trained using Layerwise Pretraining Scheme. \n","\n","The goals of this assignment are as follows:\n","\n","    * implement Deep Neural Network API\n","    * implement Layerwise Pretraining using Auto Encoder\n","    * compare a 4 layered tanh network trained from scratch and trained layerwise \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pMlC1Kwa9Dl0"},"source":["---\n","# [Part 0] Import libraries"]},{"cell_type":"code","metadata":{"id":"LUazLJ-dmNUW","colab_type":"code","colab":{}},"source":["import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.set_printoptions(precision=7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cWDWPVtzmNUd","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"JUJ1GFimmNUe","colab_type":"code","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = 123456\n","Nama = \"\"\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VX4n9KNXmNVY","colab_type":"text"},"source":["---\n","\n","---\n","# [Part 1] Deep Neural Net API\n","\n","For this exercise, we'll create a Deep Neural Net API which will automatically create deep layers for a given list of hidden layer\n","\n","<font color='blue'>This part is exacly the same as **Task 3**</font>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cqPr5kaVuEd6"},"source":["---\n","## 1 - Basic Layer Functions\n","\n","For this part, we've laready provide you the implementation of some basic layers"]},{"cell_type":"markdown","metadata":{"id":"oLrwVZt2mNUk","colab_type":"text"},"source":["---\n","### a. Affine API\n"]},{"cell_type":"code","metadata":{"id":"BnVusViAmNUl","colab_type":"code","colab":{}},"source":["def affine_forward(x, W, b ):   \n","\n","    v = np.dot(x, W) + b    \n","    cache = (x, W, b)\n","    \n","    return v, cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"spWWmF1HmNUq","colab_type":"code","colab":{}},"source":["def affine_backward(dout, cache):\n","    \n","    x, W, b = cache\n","    dW = np.dot(x.T,dout)\n","    db = np.sum(dout, axis=0, keepdims=True)\n","    dx = dout.dot(W.T)\n","    \n","    return dW, db, dx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0-3RefQmNUv","colab_type":"text"},"source":["### b. Tanh API"]},{"cell_type":"code","metadata":{"id":"oik7PQ_3mNUx","colab_type":"code","colab":{}},"source":["def tanh_forward(x):     \n","  \n","    out = np.tanh(x)\n","    cache = 1-out**2\n","    \n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2koHn3lmNU3","colab_type":"code","colab":{}},"source":["def tanh_backward(dout, cache):   \n","  \n","    dout = dout*cache\n","    \n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-k8-FNOmNU7","colab_type":"text"},"source":["### c. Softmax API"]},{"cell_type":"code","metadata":{"id":"x_9sC3jFmNU9","colab_type":"code","colab":{}},"source":["def softmax(x):  \n","  \n","    x -= np.max(x)\n","    x_exp = np.exp(x)\n","    x_sum = np.sum(x_exp, axis = 1, keepdims = True)  \n","    score = x_exp / x_sum\n","    \n","    return score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWes0OWamNVD","colab_type":"code","colab":{}},"source":["def softmax_loss(score, y):\n","   \n","    num_examples = score.shape[0]\n","    number_list = range(num_examples)\n","    corect_logprobs = -np.log(score[number_list,y])\n","    loss = np.sum(corect_logprobs)/num_examples\n","    \n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    \n","    return loss, dscores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCloT9NVmNVI","colab_type":"text"},"source":["---\n","## 2 - Weight Init\n","Below is a function to repeatedly initialize weights and bias for each layer"]},{"cell_type":"code","metadata":{"id":"yQe8zlZfmNVK","colab_type":"code","colab":{}},"source":["def init_weights(d_in, hidden, d_out, std=1e-2, seed=None):\n","    \"\"\"\n","    Inputs:\n","    - d_in  : int, number of input dimension\n","    - hidden: list of number hidden neuron in each hiidden layer\n","    - d_out : int, number of output dimension\n","    - std   : standar deviation for generating weights\n","    - seed  : random seed\n","    \n","    Outputs:\n","    - W: list of Weights\n","    - b: list of biases\n","    \"\"\"\n","    \n","    W = []\n","    b = []\n","    np.random.seed(seed)\n","    dims = [d_in] + hidden + [d_out] \n","    \n","    for i in range(len(dims)-1):\n","        W.append(std * np.random.randn(dims[i],dims[i+1]))\n","        b.append(np.zeros((1, dims[i+1])))\n","    return W, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTzrXg8nmNVZ","colab_type":"text"},"source":["---\n","## 3 - Predict Function\n","\n","Implement the predict function first, because we are going to use **predict** function inside the **training** function to track the **validation** accuracy \n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input - N * [FC Layer - activation] - FC Layer - argmax</b></pre>\n","\n","<br>\n","\n","The **N** is the number of hidden layer, which can be calculated from **len(W)-1**\n","\n","---\n","\n","<font color='blue'>This part is exacly the same as **Task 3**\n","\n","so if you have finished working on **Task 3**, just paste your answers here</font>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"XSugv9vVmNVb","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement Predict Function**\n","\n","    * loop call forward function for each hidden layer weights\n","    * check and use the requested activation funtion\n","    * call forward function for the last layer"]},{"cell_type":"code","metadata":{"id":"e2SM8EN5OOpA","colab_type":"code","colab":{}},"source":["def predict_multi_layer(X, W, b, act_f ='tanh'):    \n","    \"\"\"\n","    Inputs:\n","    - X    : Input data, of shape(N, D)\n","    - W    : list of Weight\n","    - b    : list of biases\n","    - act_f: activation function ('tanh' or 'relu')\n","    \n","    Output:\n","    - y_pred : list of class prediction\n","    \"\"\"\n","    \n","    \n","    y_pred = np.zeros(X.shape[1])\n","    n_layer = len(W)\n","    \n","    \n","    \n","    # first activation is X\n","    act = X\n","    \n","    ## ------------------------- start your code here -------------------------\n","    \n","    # loop i over n_layer-1\n","    for i in range(n_layer-1):\n","    \n","        # calculate layer score by calling affine forward function using act, W[i], and b[i]\n","        layer, _ = ??\n","  \n","        if ( act_f == 'tanh'):\n","            # calculate activation score by calling tanh forward function using layer score\n","            act, _ = ??\n","          \n","        else:\n","            # calculate activation score by calling relu forward function using layer score\n","            act, _ = ??\n","          \n","\n","    # calculate last layer score by calling affine forward function using act, W[-1], and b[-1]\n","    last_layer, _ = ??\n","    \n","    \n","    # take the maximum prediction from the last layer and use that column to get the class       \n","    # use np.argmax with axis=-1 \n","    y_pred = ??\n","\n","    ## ------------------------- end your code here -------------------------\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNthPdhwmNVi","colab_type":"text"},"source":["Check your implementation"]},{"cell_type":"markdown","metadata":{"id":"ZLv7H7WXmNVo","colab_type":"text"},"source":["---\n","## 4 - Training Function\n","\n","Now let's complete the training function\n","\n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input -N * [FC Layer - activation] - FC Layer - Softmax</b></pre>\n","\n","<br>\n","\n","The **N** is the number of hidden layer, which can be calculated from **len(W)-1**\n","\n","---\n","\n","<font color='blue'>This part is exacly the same as **Task 3**\n","\n","so if you have finished working on **Task 3**, just paste your answers here</font>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"56ExupMlmNVp","colab_type":"text"},"source":["<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","**Implement Training Function**\n","\n","there are **four steps** in this training function\n","\n","---\n","\n","**1. Forward Pass**\n","\n","    * loop over hidden layer [for len(W)-1]\n","        * call affine forward function\n","        * call activation forward function\n","    * call affine forward function for the last layer\n","    * call softmax score function\n","\n","**2. Calculate Loss**\n","\n","    * call softmax_loss function\n","    * loop over weights [for W]\n","        * calculate loss with regularization\n","\n","\n","**3. Backward Pass**\n","\n","    * call affine backward function for the last layer\n","    * loop over hidden layer [from len(W)-2 to 0]\n","        * call activation backward function\n","        * call affine backward function\n","\n","**4. Weight Update**\n","\n","    * loop over weights [for W]\n","        * implement weight update\n","    * calculate the training and validation accuracy"]},{"cell_type":"code","metadata":{"id":"vBKeIE-IOnqF","colab_type":"code","colab":{}},"source":["def train_multi_layer(X, y, X_val, y_val, hidden_size, act_f='tanh',\n","                      W=None, b=None, std=1e-4, seed=None,\n","                      lr=1e-4, lr_decay=0.95, reg=0.25, \n","                      epochs=100, batch_size=200, verbose=True):\n","    \"\"\"\n","    Inputs:\n","    - X          : array of train data, of shape (N, D)\n","    - y          : array of train labels, of shape (N,)\n","    - X_val      : array of validation data, of shape (Nv, D)\n","    - y_val      : array of validation labels, of shape (Nv,)\n","    - hidden_size: list of hidden neuron for each hidden layer\n","    - act_f      : activation function ('tanh' or 'relu')\n","    - W          : list of Weight, if W is None, it will be initialized\n","    - b          : list of biases, if W is None, bias will be initialized\n","    - std        : float, standar deviation for generating weights\n","    - seed       : int, initial random seed\n","    - lr         : float, initial learning rate\n","    - lr_decay   : float, 0-1, decay rate to reduce learning rate each epoch\n","    - reg        : float, regularization rate\n","    - epochs     : int, number of training epoch\n","    - batch_size : int, number of batch used each step\n","    - verbose    : boolean, verbosity\n","    \n","    Outputs:\n","    - W          : list of trained Weights\n","    - b          : list of trained biases\n","    - history    : list of training history [loss, train_acc, val_acc]\n","    \n","    \"\"\"\n","    \n","    num_train, dim = X.shape\n","    \n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","    \n","    # assume y takes values 0...K-1 where K is number of classes\n","    num_classes = np.max(y) + 1  \n","        \n","    # initialize Weights\n","    if W is None:\n","        W, b = init_weights(dim, hidden_size, num_classes, std, seed) \n","        \n","    # number of layer (including output layer)\n","    n_layer = len(W)\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","    \n","    \n","    ## ------------------------- start your code here --------------------------\n","\n","    print('start training using', act_f, 'activation function')\n","    for ep in range(epochs):\n","        # Shuffle data train index\n","        train_rows = np.arange(num_train)\n","        np.random.shuffle(train_rows)\n","        \n","        # split index into mini batches\n","        id_batch = np.split(train_rows, num_iter)\n","  \n","        for batch in id_batch:\n","      \n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","\n","            # store all cache in dictionary\n","            cache = {}\n","\n","            # first layer activation input is X_batch\n","            act = X_batch\n","\n","            # ------------------------------------------------------------------\n","            # 1. Forward Pass\n","            # ------------------------------------------------------------------\n","\n","            # loop i over hidden layer (n_layer-1)\n","            # see predict function implementation\n","            for i in ??:\n","\n","                # calculate layer score by calling affine forward function using activation act, W[i], and b[i]\n","                layer, cache_affine = ??\n","\n","\n","                if ( act_f == 'tanh'):\n","                    # calculate activation score by calling tanh forward function using layer score\n","                    act, _ = ??\n","                    \n","                else:\n","                    # calculate activation score by calling relu forward function using layer score\n","                    act, _ = ??\n","\n","                    \n","                # combine cache from affine and activation layer into cache for this layer\n","                cache[i] = (cache_affine, cache_act)\n","\n","            # calculate last layer score by calling affine forward function using activation act, W[i+1], and b[i+1]\n","            last_layer, cache[i+1] = ??\n","\n","            # calculate softmax score by calling softmax function using last_layer output score\n","            softmax_score = ??\n","\n","            # ------------------------------------------------------------------\n","            # 2. Calculate Loss\n","            # ------------------------------------------------------------------\n","\n","            # evaluate loss and gradient by calling softmax_loss function using input softmax_score and y_batch\n","            loss, dout = ??\n","\n","            # add regularization to the loss:\n","            #    for each weights, calculate the sum square, multiply regularization strength\n","            #    then add it to the loss\n","            # see the implementation in the previous Task\n","            for w in W:\n","                # loss = loss + reg * sum(w*w)\n","                loss += ??  \n","\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","\n","            # ------------------------------------------------------------------\n","            # 3. Backward Pass\n","            # ------------------------------------------------------------------    \n","\n","            # dictionary to contain all gradients\n","            dW = {}\n","            db = {}\n","\n","            # calculate last weights gradient by calling affine backward function using dout and cache[n_layer-1]\n","            dW[n_layer-1], db[n_layer-1], dact = ??\n","\n","            #loop i from n_layer-2 down to 0\n","            for i in range(n_layer-2,-1,-1):\n","\n","                # extract affine cache and activation cache from layer cache\n","                cache_affine, cache_act = cache[i]\n","\n","                if ( act_f == 'tanh'):\n","                    # calculate tanh gradient by calling tanh backward function using dact and cache_act\n","                    dlayer = ??\n","                    \n","                else:\n","                    # calculate relu gradient by calling relu backward function using dact and cache_act\n","                    dlayer = ??\n","\n","                    \n","                # calculate layer weights gradient by calling affine backward function using dlayer and cache_affine\n","                dW[i], db[i], dact = ??\n","\n","                # add regularization to gradient\n","                dW[i] += 2 * reg * W[i]\n","\n","            # ------------------------------------------------------------------\n","            # 4. Weight Update\n","            # ------------------------------------------------------------------    \n","\n","            # perform parameter update by subtracting W[i] and b[i] for each layer with a fraction of dW[i] and db[i]\n","            # according to the learning rate\n","            # loop over W\n","            for i in range(len(W)):    \n","                # w_i = w_i - lr * dw_i\n","                W[i] -= ??\n","    \n","                # b_i = b_i - lr * db_i\n","                b[i] -= ??\n","\n","\n","            # iteration count\n","            it +=1\n","\n","            if verbose and it % 100 == 0:\n","                print ('iteration',it,'(epoch', ep+1,'/',epochs, '): loss =', loss)\n","              \n","            \n","        # At the end of one epoch\n","        # 1. Check accuracy\n","        #    calculate the training accuracy by calling predict_multi_layer function on X_batch\n","        #    and compare it to y_batch. Then calculate the mean correct (accuracy in range 0-1)\n","        train_acc = (predict_multi_layer(X_batch, W, b, act_f) == y_batch).mean()\n","        train_acc_history.append(train_acc)\n","\n","        # 2. Calculate the training accuracy by calling predict_multi_layer function on X_val\n","        #    and compare it tu y_val. Then calculate the mean correct (accuracy in range 0-1)\n","        val_acc = (predict_multi_layer(X_val, W, b, act_f) == y_val).mean()\n","        val_acc_history.append(val_acc)\n","\n","        # 3. Decay learning rate\n","        #    multiply learning rate with decay\n","        #    see sigmoid train function\n","        lr *= lr_decay\n","            \n","            \n","    ## ------------------------- end your code here ----------------------------\n","    \n","    history = [loss_history, train_acc_history, val_acc_history]\n","    \n","    if verbose:\n","      print('Training done')\n","    \n","    return W, b, history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOXrvo7nmNVu","colab_type":"text"},"source":["---\n","---\n","# [Part 2] CIFAR-10 Dataset\n","\n","Again, we'll use the CIFAR-10 dataset\n"]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv","colab_type":"text"},"source":["## 1 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz","colab_type":"text"},"source":["## 2 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","colab_type":"code","colab":{}},"source":["X_val_ori = X_train_ori[-1000:,:]\n","y_val     = y_train[-1000:]\n","\n","X_train_ori = X_train_ori[:-1000, :]\n","y_train     = y_train[:-1000]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4","colab_type":"text"},"source":["## 3 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","colab_type":"code","colab":{}},"source":["X_train = X_train_ori.astype('float32')\n","X_val = X_val_ori.astype('float32')\n","X_test = X_test_ori.astype('float32')\n","\n","mean_image = np.mean(X_train, axis = 0)\n","X_train -= mean_image\n","X_val -= mean_image\n","X_test -= mean_image\n","\n","X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]*X_train.shape[3]))\n","X_val = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]*X_val.shape[3]))\n","X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]*X_test.shape[3]))\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n","\n","y_train = y_train.ravel()\n","y_val = y_val.ravel()\n","y_test = y_test.ravel()\n","\n","print('\\ny_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5t2TWycmNWU","colab_type":"text"},"source":["---\n","***\n","#[Part 3] Four Layer Tanh Network\n","* In this part, we'll train a **4-layered tanh network** and train it from scratch\n","* we'll use **200, 100, and 50 hidden neurons** for each hidden layers"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vc7LETzCUYMQ"},"source":["---\n","## 1 - Train Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xc2RyhhXUYMU"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **three-hidden layer neural network** with **500, 100, and 50 hidden neurons** for each hidden layers using **tanh** activation function\n","\n"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"BBAJlDlAUYMY","colab":{}},"source":["hidden_size=[200, 100, 50]\n","\n","t1 = time.time()\n","\n","W_tanh, b_tanh, history_tanh = train_multi_layer(\n","    X_train, y_train, X_val, y_val, \n","    hidden_size=hidden_size, \n","    act_f = 'tanh',\n","    std=1e-2, lr=1e-2,\n","    lr_decay=0.95, reg=0.01, \n","    epochs=30)\n","\n","t2 = time.time()\n","\n","time_scratch = (t2-t1)/60\n","print('time training from scratch: %0.2f minutes' % time_scratch)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7ZIL-ShMUYMg"},"source":["**Expected Output**:\n","<pre>\n","loss should start around 2.9 and end around 2.15 in about 5 minutes"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NWrj-pbiUYMh"},"source":["---\n","## 2 - Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"9C6Su2WQUYMj","colab":{}},"source":["loss, train_acc, val_acc = history_tanh\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"waoBKzJqUYMr"},"source":["---\n","## 3 - Training Accuracy\n","Calculate the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nSQ4AgzqUYMs","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_multi_layer(X_train, W_tanh, b_tanh, act_f='tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","scratch_train_acc = accuracy*100\n","print('Training Accuracy = %0.2f%%' % scratch_train_acc)\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2CRdVyk5UYMw"},"source":["**Expected Output**:\n","\n","<pre>You should be able to get about <b>~22%</b> accuracy on training set using the initial run\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XsmjABg9UYMz","colab":{}},"source":["y_pred = predict_multi_layer(X_val, W_tanh, b_tanh, act_f = 'tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","\n","\n","scratch_val_acc = accuracy*100\n","print('Validation Accuracy = %0.2f%%' % scratch_val_acc)\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YGKyv4msUYM4"},"source":["**Expected Output**:\n","\n","<pre>You should also be able to get about <b>~22%</b> accuracy on validation set</pre>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HihrFmP3mNV9","colab_type":"text"},"source":["---\n","***\n","# [Part 4] AutoEncoder API\n","\n","**Autoencoder** is a neural network that is trained with the aim of copying input into output. \n","\n","The Autoencoder network consists of two parts: **Encoder** and **Decoder**. \n","\n","Initially, Autoencoder was used to reduce dimension or feature learning which in some cases succeeded in reducing dimensions better than PCA.\n","\n","![img](https://blog.keras.io/img/ae/autoencoder_schema.jpg)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fGS9-L7t7o8q"},"source":["---\n","## 1 - Training Function\n","\n","Now let's complete the training function\n","\n","For this exercise, we'll only use one layer Neural net for each Encoder and Decoder.\n","\n","<br>\n","\n","Thus, the network architecture should be: \n","<pre><b>Input - <font color='blue'>[FC Layer - Tanh]</font> - <font color='red'>[FC Layer - Tanh]</font> - Reconstructed</b></pre>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BBAgHRm-7o8x"},"source":["<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","**Implement Training Function**\n","\n","there are **four steps** in this training function\n","\n","---\n","\n","**1. Forward Pass**\n","\n","    * call affine forward function for encoder\n","    * call tanh forward function for encoder\n","    * call affine forward function for decoder\n","    * call tanh forward function for decoder\n","\n","**2. Calculate Loss**\n","\n","    * calculate Mean Squared Error Loss\n","\n","\n","**3. Backward Pass**\n","\n","    * call tanh backward function for decoder\n","    * call affine backward function for decoder\n","    * call tanh backward function for encoder\n","    * call affine backward function for encoder\n","\n","**4. Weight Update**\n","\n","    * implement weight update"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ydd8OvFq7o82","colab":{}},"source":["def train_autoencoder(X, reduction_size, W=None, b=None,\n","                      std=1e-4, seed=None, lr=1e-4, lr_decay=0.95,\n","                      epochs=100, batch_size=200, verbose=True):\n","    \"\"\"\n","    Inputs:\n","    - X             : array of train data, of shape (N, D)\n","    - reduction_size: int, size of dimension reduction\n","    - W             : list of Weight, if W is None, it will be initialized\n","    - b             : list of biases, if W is None, bias will be initialized\n","    - std           : float, standar deviation for generating weights\n","    - seed          : int, initial random seed\n","    - lr            : float, initial learning rate\n","    - lr_decay      : float, 0-1, decay rate to reduce learning rate each epoch\n","    - reg           : float, regularization rate\n","    - epochs        : int, number of training epoch\n","    - batch_size    : int, number of batch used each step\n","    - verbose       : boolean, verbosity\n","    \n","    Outputs:\n","    - W             : list of trained Weights\n","    - b             : list of trained biases\n","    - history       : list of training history [loss, train_acc, val_acc]\n","    \n","    \"\"\"\n","    \n","    num_train, dim = X.shape\n","    \n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","        \n","    # initialize Weights\n","    if W is None:\n","        W, b = init_weights(dim, [reduction_size], dim, std, seed) \n","        \n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    \n","    \n","    ## ------------------------- start your code here -------------------------\n","\n","    print('start training autoencoder, reducing to', reduction_size)\n","    \n","    for ep in range(epochs):\n","        # Shuffle data train index\n","        train_rows = np.arange(num_train)\n","        np.random.shuffle(train_rows)\n","        \n","        # split index into mini batches\n","        id_batch = np.split(train_rows, num_iter)\n","  \n","        for batch in id_batch:\n","      \n","            X_batch = X[batch]\n","        \n","\n","            # ------------------------------------------------\n","            # 1. Forward Pass\n","            # ------------------------------------------------\n","\n","            # calculate encoder layer score by calling affine forward function using input X_batch, W[0], and b[0]\n","            layer_encoder, cache_affine_e = ??\n","\n","            # calculate encoder activation score by calling tanh forward function using encoder layer score\n","            encoded, cache_act_e = ??\n","            \n","            # calculate decoder layer score by calling affine forward function using encoded activation, W[1], and b[1]\n","            layer_decoder, cache_affine_d = ??\n","\n","            # calculate decoded output by calling tanh forward function using decoder layer score\n","            decoded, cache_act_d = ??\n","  \n","\n","\n","            # ------------------------------------------------\n","            # 2. Calculate Loss\n","            # ------------------------------------------------\n","\n","            # evaluate loss and gradient using Mean Squared Error\n","            dout = decoded-X_batch\n","            loss = np.mean(dout**2)\n","            dout = dout/batch_size\n","\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","\n","            # ------------------------------------------------\n","            # 3. Backward Pass\n","            # ------------------------------------------------    \n","\n","            # dictionary to contain all gradients\n","            dW = {}\n","            db = {}\n","\n","            # calculate tanh gradient by calling tanh backward function using dout and cache_act_d\n","            dlayer_decoder = ??\n","\n","            # calculate layer weights gradient by calling affine backward function using dlayer_decoder and cache_affine_d\n","            dW[1], db[1], d_encoded = ??\n","            \n","            # calculate tanh gradient by calling tanh backward function using d_encoded and cache_act_e\n","            dlayer_encoder = ??\n","\n","            # calculate layer weights gradient by calling affine backward function using dlayer_encoder and cache_affine_e\n","            dW[0], db[0], dx = ??\n","\n","\n","            # ------------------------------------------------\n","            # 4. Weight Update\n","            # ------------------------------------------------    \n","\n","            # perform parameter update by subtracting W[i] and b[i] for each layer with a fraction of dW[i] and db[i]\n","            # according to the learning rate\n","            W[0] -= lr * dW[0]\n","            b[0] -= lr * db[0]\n","            W[1] -= lr * dW[1]\n","            b[1] -= lr * db[1]\n","\n","\n","            # iteration count\n","            it +=1\n","\n","            if verbose and it % 100 == 0:\n","                print ('iteration',it,'(epoch', ep+1,'/',epochs, '): loss =', loss)\n","                \n","        # Decay learning rate\n","        #    multiply learning rate with decay\n","        lr *= lr_decay\n","            \n","            \n","    ## ------------------------- end your code here -------------------------\n","    \n","    if verbose:\n","      print('Training done')\n","    \n","    return W, b, loss_history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W72Q_BQLzgwM","colab_type":"text"},"source":["---\n","## 2 - Predict Function\n","\n","The predict function is just a simple Single Layer Neural Network\n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input - [FC Layer - Tanh]</b></pre>\n","\n","And it will perform either **Encoder** or **Decoder** depending on the weights that passed\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y3RoKgG5NfPx"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement Predict Function**\n","\n","    * call forward function \n","    * call tanh forward funtion\n"]},{"cell_type":"code","metadata":{"id":"KF6jLPkmK0G8","colab_type":"code","colab":{}},"source":["def predict_autoencoder(X, W, b):\n","    \n","    # calculate layer score by calling affine forward function using input X, W, and b  \n","    layer, _ = ??\n","\n","    # calculate activation score by calling tanh forward function using layer score\n","    act, _ = ??\n","    \n","    return act"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PV3bHM4A0uSb"},"source":["---\n","***\n","#[Part 5] Layerwise Pretraining\n","\n","\n","In 2006, Geoffrey Hinton and Ruth Salakhutdinov successfully trained the Network with more than 10 layer architectures by training it layer-by-layer, one by one, using the **Restricted Boltzmann Machine** \n","\n","The RBMs are trained as feature extraction to understand the data represntation, and then merging the trained layers back into one network. \n","\n","Once stacked, the Network can be trained with Back-Propagation without worrying about experiencing **Vanishing Gradient Problems**\n","\n","![img](https://image.ibb.co/eHq6Ae/mlenewimage14.png)\n","\n","For this exercise, we'll use **AutoEncoder** to train the network layers since the implementations are easier than RBMs\n","\n","\n","To compare the network trained before, we'll train a 4-layered network,but now we train it using layerwise pretrain scheme.\n","\n","We'll use the same **200, 100, and 50 hidden neurons** for each hidden layers\n","\n","Each layer only trained for **5 epochs**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HDgmslJw0uSg"},"source":["---\n","## 1 - Train First Layer \n","\n","First we train an **AutoEncoder** network to reduce image **[32,32,3]** into **200 dimensional feature space**"]},{"cell_type":"markdown","metadata":{"id":"HNiI9HsSPMIb","colab_type":"text"},"source":["### Prepare Data\n","\n","For this part, we need to prepare the data and normalize it into **[-1..1]** range"]},{"cell_type":"code","metadata":{"id":"gj9ov3jrPWO5","colab_type":"code","colab":{}},"source":["X_train_ae1 = X_train/127.5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"31rr054x0uSj"},"source":["### Train Layer\n","\n","Use the AutoEncoder Training Function and train the first layer of **200 hidden neurons**  using **tanh** activation function\n","\n"]},{"cell_type":"code","metadata":{"id":"fwZKba-xLx_J","colab_type":"code","colab":{}},"source":["t1 = time.time()\n","\n","W1, b1, hist1 = train_autoencoder(X_train_ae1, reduction_size=200,\n","                                  std=1e-3, lr=1e-3, \n","                                  lr_decay=0.95,epochs=5)\n","\n","t2 = time.time()\n","\n","time_ae_1 = (t2-t1)/60\n","print('time training layer 1: %0.2f minutes' % time_ae_1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pZSYEH8IYGCr"},"source":["**Expected Output**:\n","<pre>\n","loss should start around 0.15 and end around 0.08 in about 2 minutes"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bS9l1ANUtYfc"},"source":["### Visualize Loss\n","Visualize the trining loss"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"R33dkPDftYfr","colab":{}},"source":["plt.rcParams['figure.figsize'] = [7, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.plot(hist1)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MJLQ2Eli1JLe"},"source":["---\n","## 2 - Train Second Layer \n","\n","\n","Next we train the second **AutoEncoder** network to reduce input **200 dimensional feature space** into **100 dimensional feature space**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rjuCeueFQBpY"},"source":["### Prepare Data\n","\n","First we prepare the data by reducing all training data into 200 dimensional space using **Encoder** from the first network"]},{"cell_type":"code","metadata":{"id":"lpJ6zRnKuwvY","colab_type":"code","colab":{}},"source":["X_train_ae2 = predict_autoencoder(X_train_ae1, W1[0], b1[0])\n","\n","print('Original shape:', X_train_ae1.shape)\n","print('Reducted shape:', X_train_ae2.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LS-kMWHJmNVn","colab_type":"text"},"source":["**Expected Output**:\n","<pre>\n"," Original shape: (49000, 3072)\n"," Reducted shape: (49000, 200)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jVaqx1Q0QBpm"},"source":["### Train Layer\n","\n","Use the AutoEncoder Training Function and train the second layer of **100 hidden neurons**  using **tanh** activation function\n","\n"]},{"cell_type":"code","metadata":{"id":"gnl1YL98ubeJ","colab_type":"code","colab":{}},"source":["t1 = time.time()\n","\n","W2, b2, hist2 = train_autoencoder(X_train_ae2, reduction_size=100,\n","                                  std=1e-3, lr=1e-3, \n","                                  lr_decay=0.95,epochs=5)\n","\n","t2 = time.time()\n","\n","time_ae_2 = (t2-t1)/60\n","print('time training layer 2: %0.2f minutes' % time_ae_2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZkVvLiGdYLSq"},"source":["**Expected Output**:\n","<pre>\n","loss should start around 0.4 and end around 0.18 in about 7 seconds"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qKSuB3BA1peP"},"source":["### Visualize Loss\n","Visualize the trining loss"]},{"cell_type":"code","metadata":{"id":"L1ulplzWu4MO","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [7, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.plot(hist2)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BSY9G-S3RDsJ"},"source":["---\n","## 3 - Train Third Layer\n","\n","\n","Next we train the third **AutoEncoder** network to reduce input even further from **100 dimensional feature space** into **50 dimensional feature space**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lF6sEMoMRDsP"},"source":["### Prepare Data\n","\n","First we prepare the data by reducing all training data into 100 dimensional space using **Encoder** from the second network"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xJbp8pUGRDsU","colab":{}},"source":["X_train_ae3 = predict_autoencoder(X_train_ae2, W2[0], b2[0])\n","\n","print('Original shape:', X_train_ae2.shape)\n","print('Reducted shape:', X_train_ae3.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qeStBOIURDsc"},"source":["**Expected Output**:\n","<pre>\n"," Original shape: (49000, 200)\n"," Reducted shape: (49000, 100)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O2vJ8GfrRDse"},"source":["### Train Layer\n","\n","Use the AutoEncoder Training Function and train the third layer of **50 hidden neurons**  using **tanh** activation function\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_AtT1bcjvJl_","colab":{}},"source":["t1 = time.time()\n","\n","W3, b3, hist3 = train_autoencoder(X_train_ae3, reduction_size=50,\n","                                  std=1e-3, lr=1e-3, \n","                                  lr_decay=0.95,epochs=5)\n","\n","t2 = time.time()\n","\n","time_ae_3 = (t2-t1)/60\n","print('time training layer 3: %0.2f minutes' % time_ae_3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1p4G5bofYVs7"},"source":["**Expected Output**:\n","<pre>\n","loss should start around 0.25 and end around 0.14 in about 3 seconds"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"60uMhS0D1p4I"},"source":["### Visualize Loss\n","Visualize the trining loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mFjOOfFyvJmF","colab":{}},"source":["plt.rcParams['figure.figsize'] = [7, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.plot(hist3)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cL7L_bSfR-KH"},"source":["---\n","## 4 - Train Last Layer\n","\n","\n","Lastly we train the last **AutoEncoder** network to reduce input from **50 dimensional feature space** into **10 dimensional class**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YiLsBPb0R-KN"},"source":["### Prepare Data\n","\n","First we prepare the data by reducing all training data into 50 dimensional space using **Encoder** from the third network"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WEpnIiizR-KR","colab":{}},"source":["X_train_ae4 = predict_autoencoder(X_train_ae3, W3[0], b3[0])\n","\n","print('Original shape:', X_train_ae3.shape)\n","print('Reducted shape:', X_train_ae4.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rZldzna2R-KY"},"source":["**Expected Output**:\n","<pre>\n"," Original shape: (49000, 100)\n"," Reducted shape: (49000, 60)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9VGyjw9TR-Kb"},"source":["### Train Layer\n","\n","Use the AutoEncoder Training Function and train the last layer of **10 output neuron**  using **tanh** activation function\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QKgs2WfdvU1U","colab":{}},"source":["t1 = time.time()\n","\n","W4, b4, hist4 = train_autoencoder(X_train_ae4, reduction_size=50,\n","                                  std=1e-3, lr=1e-3, \n","                                  lr_decay=0.95,epochs=5)\n","\n","t2 = time.time()\n","\n","time_ae_4 = (t2-t1)/60\n","print('time training layer 4: %0.2f minutes' % time_ae_4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZr1PMf-YcSw"},"source":["**Expected Output**:\n","<pre>\n","loss should start around 0.17 and end around 0.07 in about 2 seconds"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4usEF0IU1qb3"},"source":["### Visualize Loss\n","Visualize the trining loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MgDSfBmJvU1c","colab":{}},"source":["plt.rcParams['figure.figsize'] = [7, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.plot(hist4)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Lw8v6GwT2NeT"},"source":["---\n","***\n","#[Part 6] Four Layer Pretrained Tanh Network\n","\n","After all layers are trained, now we retrieve all **encoder** weights from each layer and combine it into one complete **4-Layer Tanh Network**, then we train it once more using Supervised Backpropagation"]},{"cell_type":"markdown","metadata":{"id":"WLOTreSRS9Xw","colab_type":"text"},"source":["---\n","### 1 - Get Weights\n","\n","Get first Weights and biases (**Encoder Weights and biases**) form each layer, and combine them"]},{"cell_type":"code","metadata":{"id":"mofDtVuJvhNk","colab_type":"code","colab":{}},"source":["W_ae = [W1[0], W2[0], W3[0], W4[0]]\n","b_ae = [b1[0], b2[0], b3[0], b4[0]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KZEg0k4a2Nea"},"source":["## 2 - Train Network\n","\n","Train the 4-Layer Pretrained Tanh Network for just **10 epochs**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e_qxaJEE2Nef"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **three-hidden layer neural network** with **500, 100, and 50 hidden neurons** for each hidden layers using **tanh** activation function\n","\n"]},{"cell_type":"code","metadata":{"id":"1EdKl6XUvhLb","colab_type":"code","colab":{}},"source":["t1 = time.time()\n","\n","W_ae, b_ae, history_ae = train_multi_layer(\n","    X_train, y_train, X_val, y_val, \n","    hidden_size=[], \n","    W = W_ae, b = b_ae,\n","    act_f = 'tanh',\n","    std=1e-2, lr=1e-2,\n","    lr_decay=0.95, reg=0.01, \n","    epochs=10)\n","\n","t2 = time.time()\n","\n","time_merged = (t2-t1)/60\n","print('time training merged: %0.2f minutes' % time_merged)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XSO_vFQw2Tc2"},"source":["**Expected Output**:\n","<pre>\n","loss should start around 3.8 and end around 2.6 in less than 2 minutes"]},{"cell_type":"markdown","metadata":{"id":"pDFX4fOjUzbC","colab_type":"text"},"source":["You can see that the total time spent in the pre-training scheme is almost the same as the training process from the scratch"]},{"cell_type":"code","metadata":{"id":"ZJBl05H-UhNG","colab_type":"code","colab":{}},"source":["print('time training from scratch: %0.2f minutes' % time_scratch)\n","\n","total_time = time_ae_1 + time_ae_2 + time_ae_3 + time_ae_4 + time_merged\n","print('total time pretrain scheme: %0.2f minutes' % total_time)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aXyQQUWN2Tc4"},"source":["---\n","## 3 - Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"LNFWRmo3vhII","colab_type":"code","colab":{}},"source":["loss, train_acc, val_acc = history_ae\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tGMM2Sm2wBsK"},"source":["---\n","## 4 - Training Accuracy\n","Calculate the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dhJmdtjGwBsQ","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_multi_layer(X_train, W_ae, b_ae, act_f='tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","pre_train_acc = accuracy*100\n","print('Training Accuracy Pretraining = %0.2f%%' % pre_train_acc)\n","print('Training Accuracy From Scratch= %0.2f%%' % scratch_train_acc)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wF1p6T-swBsh"},"source":["**Expected Output**:\n","\n","<pre>You should be able to get about <b>~33%</b> accuracy on training set using the initial run. \n","10% higher than training from scratch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CMXdO1ZFwBsj","colab":{}},"source":["y_pred = predict_multi_layer(X_val, W_ae, b_ae, act_f = 'tanh')\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","\n","pre_val_acc = accuracy*100\n","\n","print('Validation Accuracy Pretraining = %0.2f%%' % pre_val_acc)\n","print('Validation Accuracy From Scratch= %0.2f%%' % scratch_val_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nXjFnrwFwBsp"},"source":["**Expected Output**:\n","\n","<pre>You should also be able to get about <b>~34%</b> accuracy on validation set.\n","10% higher than training from scratch\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 4\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bvBk-u7_92ub"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}