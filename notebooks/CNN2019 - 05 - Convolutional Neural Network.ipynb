{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN2019 - 05 - Convolutional Neural Network.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y4GgVIEOx63V"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"mhlMh-sVcZzW","colab_type":"text"},"source":["# Task 5 - Convolutional Neural Network\n","\n","<p align='center'>\n","<img src='https://cdn-images-1.medium.com/max/1600/1*NQQiyYqJJj4PSYAeWvxutg.png' width='700'>\n","\n","So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead.\n","\n","Here You will be implementing the building blocks of a convolutional neural network! Each function you will implement will have detailed instructions that will walk you through the steps needed:\n","\n","- **Convolution functions**, including:\n","    - Zero Padding\n","    - Convolve window \n","    - Convolution forward\n","    - Convolution backward\n","- **Pooling functions**, including:\n","    - Pooling forward\n","    - Create mask \n","    - Distribute value\n","    - Pooling backward\n","\n","  <br>\n","  \n","This notebook will ask you to first implement these functions from scratch in numpy. You will then use these layers to train a convolutional network on the CIFAR-10 dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1","colab_type":"text"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"id":"12SMnaunBw96","colab_type":"text"},"source":["---\n","## 1 - Import Libraries\n","\n","First, install tabulate library for visualization"]},{"cell_type":"code","metadata":{"id":"LFBXvDQ0cZzs","colab_type":"code","colab":{}},"source":["!pip install tabulate"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DvPSXMEIaFm1","colab_type":"text"},"source":["Import requiered libraries"]},{"cell_type":"code","metadata":{"id":"hsZYqgngcZzY","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tabulate import tabulate\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk","colab_type":"code","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4oLZzTjVcZ24","colab_type":"text"},"source":["---\n","## 2 - Layer API\n","\n","This part is exacly the same as Task 3\n","for that, we've already provide you the implementation of the basic layers"]},{"cell_type":"markdown","metadata":{"id":"tsiFt6eTokew","colab_type":"text"},"source":["### Affine Functions"]},{"cell_type":"code","metadata":{"id":"s214hq3vcZ24","colab_type":"code","colab":{}},"source":["def affine_forward(x, W, b ):\n","    \n","    N = x.shape[0]\n","    a1 = x.reshape(N, -1)\n","    v = np.dot(a1, W) + b    \n","    cache = (x, W, b)\n","    \n","    return v, cache\n","  \n","# --------------------------------\n","\n","def affine_backward(dout, cache):\n","    \n","    x, W, b = cache\n","    N = x.shape[0]\n","    dW = np.dot(x.reshape(N, -1).T,dout)\n","    db = np.sum(dout, axis=0, keepdims=True)\n","    dx = dout.dot(W.T).reshape(x.shape)\n","    \n","    return dx, dW, db\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"voiXjsH-onm_","colab_type":"text"},"source":["### ReLU Functions"]},{"cell_type":"code","metadata":{"id":"0bqdPxmWohxD","colab_type":"code","colab":{}},"source":["def relu_forward(x):\n","    out = x * (x > 0).astype(float)\n","    cache = x\n","    return out, cache\n","\n","# --------------------------------\n","\n","def relu_backward(dout, cache):    \n","    dx = dout * (cache >= 0)\n","    \n","    return dx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aA04_S3noppa","colab_type":"text"},"source":["### Softmax Functions"]},{"cell_type":"code","metadata":{"id":"TPNfcEGpojf-","colab_type":"code","colab":{}},"source":["def softmax(x):  \n","    x -= np.max(x)\n","    x_exp = np.exp(x)\n","    x_sum = np.sum(x_exp, axis = 1, keepdims = True)  \n","    score = x_exp / x_sum\n","    \n","    return score\n","\n","# --------------------------------\n","\n","def softmax_loss(score, y):\n","   \n","    num_examples = score.shape[0]\n","    number_list = range(num_examples)\n","    corect_logprobs = -np.log(score[number_list,y])\n","    loss = np.sum(corect_logprobs)/num_examples\n","    \n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    return loss, dscores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv","colab_type":"text"},"source":["---\n","## 3 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw","colab_type":"code","colab":{}},"source":["(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz","colab_type":"text"},"source":["---\n","## 4 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","colab_type":"code","colab":{}},"source":["X_val_ori = X_train_ori[-1000:,:]\n","y_val     = y_train[-1000:]\n","\n","X_train_ori = X_train_ori[:-1000, :]\n","y_train     = y_train[:-1000]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4","colab_type":"text"},"source":["---\n","## 5 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","colab_type":"code","colab":{}},"source":["X_train = X_train_ori.astype('float32')\n","X_val   = X_val_ori.astype('float32')\n","X_test  = X_test_ori.astype('float32')\n","\n","mean_image = np.mean(X_train, axis = 0)\n","X_train -= mean_image\n","X_val   -= mean_image\n","X_test  -= mean_image\n","\n","X_train = X_train.astype('float32')\n","X_val = X_val.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n","\n","y_train = y_train.ravel()\n","y_val   = y_val.ravel()\n","y_test  = y_test.ravel()\n","\n","print('\\ny_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xHzs5eancZ0K","colab_type":"text"},"source":["---\n","---\n","# [Part 1] Convolution API\n","<p align='center'>\n","<img src='https://image.ibb.co/j1juWK/model.png' width='700'></p>\n","\n","Although programming frameworks make convolutions easy to use, they remain one of the hardest concepts to understand in Deep Learning. \n","\n","\n","<br>\n","\n","---\n","A convolution layer transforms an input volume into an output volume of different size, as shown below.\n","\n","<p align='center'>\n","<img src='https://image.ibb.co/eAXsJz/conv_nn.png' width='400'>\n","\n","In this part, you will build every step of the convolution layer. \n","\n","To understand the steps of Convolutional Layer better, You will first implement **three helper functions**: \n","  \n","    * zero padding\n","    * slicing, and\n","    * and the convolution function itself. \n"]},{"cell_type":"markdown","metadata":{"id":"lNTbGmq_cZ0T","colab_type":"text"},"source":["---\n","## 1 - Zero-Padding\n","\n","Zero-padding adds zeros around the border of an image:\n","\n","<p align=center>\n","<img src=\"https://image.ibb.co/eYz6dz/PAD.png\" width='600'><br>\n","  <b>Zero-Padding</b> Image (3 channels, RGB) with a padding of 2.\n","</p>\n","\n","\n","<br>\n","The main benefits of padding are the following:\n","\n","1. It allows you to use a **CONV** layer without necessarily shrinking the height and width of the volumes. \n"," - This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. \n"," - An important special case is the \"same\" convolution, in which the height/width is exactly preserved after one layer. \n"," \n"," <br>\n","\n","1. It helps us keep more of the information at the border of an image. \n"," - Without padding, very few values at the next layer would be affected by pixels as the edges of an image.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xc2RyhhXUYMU"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement **Padding** function, which pads all the images of a batch of examples X with zeros. \n","\n","*Hint*: [Use np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). \n","\n","<br>\n","\n","---\n","<br>\n","\n","*Hint*: If you want to pad the array **\"`a`\"** of shape $(5,5,5,5,5)$ with:\n","* `pad = 1` for the `2nd` dimension, \n","* `pad = 3` for the `4th` dimension and \n","* `pad = 0` for the rest, \n","\n","<br>\n","\n","you would do:\n","\n","```python\n","a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))\n","```\n","    \n","<br>\n","\n","In this excercise, X is a 4-dimensional array, and we want to pad the **2nd** and **3rd** dimension\n","\n"]},{"cell_type":"code","metadata":{"id":"yACaMiQ5cZ0V","colab_type":"code","colab":{}},"source":["def zero_pad(X, pad):\n","    \"\"\"\n","    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n","    as illustrated in Figure 1.\n","    \n","    Argument:\n","    X -- python numpy array of shape (m, H_n, W_n, C_n) representing a batch of m images\n","    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n","    \n","    Returns:\n","    X_pad -- padded image of shape (m, H_n + 2*pad, W_n + 2*pad, C_n)\n","    \"\"\"\n","    \n","    X_pad = ??\n","    \n","    return X_pad"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RrkvR9h4Il8e","colab_type":"text"},"source":["Check your implementations\n","\n","First, create new image examples"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"pSTlkh3ycZ0c","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","\n","X_rand = np.random.randn(4, 3, 3, 2)\n","print (\"X_rand.shape =\", X_rand.shape)\n","\n","print('Original Images')\n","plt.rcParams['figure.figsize'] = (6, 10)\n","fig, axarr = plt.subplots(1, 2)\n","axarr[0].set_title('x[0,:,:,0]')\n","axarr[0].imshow(X_rand[0,:,:,0])\n","axarr[1].set_title('x[1,:,:,0]')\n","axarr[1].imshow(X_rand[1,:,:,0])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PBWvFyuMIeGs"},"source":["\n","Now give pad to images above with padding size=2"]},{"cell_type":"code","metadata":{"id":"hGxcueQhcZ0m","colab_type":"code","colab":{}},"source":["X_rand_pad = zero_pad(X_rand, 2)\n","\n","print (\"X_rand_pad.shape =\", X_rand_pad.shape)\n","print (\"X_rand_pad[1,1] =\\n\", X_rand_pad[1,1].T,'\\n')\n","\n","print('Padded Images')\n","fig, axarr = plt.subplots(1, 2)\n","axarr[0].set_title('x_pad[0,:,:,0]')\n","axarr[0].imshow(X_rand_pad[0,:,:,0])\n","axarr[1].set_title('x_pad[1,:,:,0]')\n","axarr[1].imshow(X_rand_pad[1,:,:,0])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ozGzjnABcZ0u","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","X_rand_pad.shape = (4, 7, 7, 2)\n","X_rand_pad[1,1]  =\n"," [[0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]]\n","</pre><BR>\n","  \n","**EXPECTED IMAGES**:<br>\n","  ![pad](https://image.ibb.co/kwZSgp/pad.png)"]},{"cell_type":"markdown","metadata":{"id":"4faAR7iVcZ0w","colab_type":"text"},"source":["---\n","## 2 - Single step of convolution \n","\n","In this part, implement **JUST A SINGLE STEP** of matrix multiplication in convolution process, in which you apply the filter to a single position of the input. \n","\n","<font color='red'>**WITHOUT THE CONVOLUTION LOOP**</font>\n","\n","<br>\n","\n","<center>\n","<img src=\"https://image.ibb.co/fv7f49/conv_step.png\" width='600'>\n","</center>\n","\n","Perform Element-wise product between $input\\_slice$ and $W$. \n","After that, Sum over all entries of volume $S$ and add bias $b$ to get the result\n","\n","Later in this notebook, you'll apply this function to multiple positions of the input to implement the full convolutional operation. \n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PKTnzE8cNdDJ"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Implement conv_single_step():\n","    * Perform Element-wise product between input_slice and W. \n","    * Sum over all entries of volume S \n","    * add bias to get the result\n","\n","*Hint* : use `np.multiply()` and `np.sum()`\n"]},{"cell_type":"code","metadata":{"id":"s79cQDK5cZ0x","colab_type":"code","colab":{}},"source":["def conv_single_step(input_slice, W, b):\n","    \"\"\"\n","    Apply one filter defined by parameters W on a single slice (input_slice) of the output activation \n","    of the previous layer.\n","    \n","    Arguments:\n","    input_slice -- slice of input data of shape (f, f, C_prev)\n","    W -- Weight parameters contained in a window - matrix of shape (f, f, C_prev)\n","    b -- Bias parameters contained in a window - matrix of shape (1,)\n","    \n","    Returns:\n","    z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n","    \"\"\"\n","    \n","    # Element-wise product between input_slice and W. You can use np.multiply() \n","    S = ??\n","    \n","    # Sum over all entries of the volume S\n","    # Then Add bias. \n","    z = ??   \n","    \n","    return z"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AVOEaaLsOQMP"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"tFtmoqd2cZ07","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","X_slice = np.random.randn(4, 4, 3)\n","W = np.random.randn(4, 4, 3)\n","b = np.random.randn()\n","\n","Z = conv_single_step(X_slice, W, b)\n","print(\"Z =\", Z)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHmOnYQ1cZ1A","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Z = -6.999089450680221\n"]},{"cell_type":"markdown","metadata":{"id":"JYp5Rab3cZ1B","colab_type":"text"},"source":["---\n","## 3 - Slicing Array\n","\n","The `conv_single_step` function above will be used to calculate the activation of each slice of image when we convolve the filter.\n","\n","<centeR>\n","<img src=\"https://stanford.edu/~shervine/images/convolution-layer-a.png\" width=\"700\">\n","</center>\n","\n","\n","To make things easier, we build  `get_slice()` function as follow\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cmgMJWDuP15b"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","  * implement **get_slice()** function to slice a 3D array \n","  * Slicing begin from vertical position **H_start** and horizontal position **W_start**\n","  * Resulting a **3D array** with height $H$ and width $W$ with the same depth as input\n"]},{"cell_type":"code","metadata":{"id":"tA7z6ZyOcZ1G","colab_type":"code","colab":{}},"source":["def get_slice(X, H_start, W_start, H, W):\n","    \"\"\"\n","    Slicing X input in shape of f_size from postition H_start and W_start\n","    Input:\n","    - X       : Input data of shape (H, W, C)\n","    - H_start : height (vertical) position to begin the slicing\n","    - W_start : width (horizontal) position to begin the slicing\n","    - H       : height slice\n","    - W       : width slice\n","    \n","    Returns:\n","    - slices : a slice of data of shape (H, W, C)\n","    \"\"\"\n","    slices = ??\n","    \n","    return slices"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P2B62w62iC4t","colab_type":"text"},"source":["Check your implementations\n","\n","* with an input of size $5\\times5$ and a filter of size $3\\times3$, \n","* there will be $9$ slices across the space, \n","* with each slice should be a $3\\times3$ matrix (same size as filter)\n","\n","<br>\n","\n","Here we'll get **the first three slices**"]},{"cell_type":"code","metadata":{"id":"igLFGALZcZ1L","colab_type":"code","colab":{}},"source":["np.random.seed(15)\n","X_int = np.random.randint(0,2,(5,5,1))\n","print('X_int =')\n","print('\\n'.join(' '.join(str(cell) for cell in row) for row in X_int[:,:,0]))\n","print()\n","\n","\n","f_size = 3\n","\n","slices00 = get_slice(X_int, 0, 0, f_size, f_size)\n","\n","slices01 = get_slice(X_int, 0, 1, f_size, f_size)\n","\n","slices10 = get_slice(X_int, 1, 0, f_size, f_size)\n","\n","s = []\n","s1 = []\n","s00 = 'h = 0 ,w = 0\\nshape ='+str(slices00.shape)+'\\n'\n","s00 += '\\n'.join(' '.join(str(cell) for cell in row) for row in slices00[:,:,0])\n","s00 += '\\n-----------------\\n'\n","s1.append(s00)\n","\n","s01 = 'h = 0 ,w = 1\\nshape ='+str(slices01.shape)+'\\n'\n","s01 += '\\n'.join(' '.join(str(cell) for cell in row) for row in slices01[:,:,0])\n","s01 += '\\n-----------------\\n'\n","s1.append(s01)\n","s.append(s1)\n","\n","s2 = []\n","s10 = 'h = 1 ,w = 0\\nshape ='+str(slices10.shape)+'\\n'\n","s10 += '\\n'.join(' '.join(str(cell) for cell in row) for row in slices10[:,:,0])\n","s10 += '\\n-----------------\\n'\n","s2.append(s10)\n","s.append(s2)\n","\n","print(tabulate(s,tablefmt=\"pipe\"))\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nsvhgX30cZ1Q","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","X_int =\n","0 1 0 1 1\n","0 0 1 1 1\n","1 0 1 1 1\n","1 1 1 0 0\n","0 1 0 1 1\n","\n","|:------------------|:------------------|\n","| h = 0 ,w = 0      | h = 0 ,w = 1      |\n","| shape =(3, 3, 1)  | shape =(3, 3, 1)  |\n","| 0 1 0             | 1 0 1             |\n","| 0 0 1             | 0 1 1             |\n","| 1 0 1             | 0 1 1             |\n","| ----------------- | ----------------- |\n","| h = 1 ,w = 0      |                   |\n","| shape =(3, 3, 1)  |                   |\n","| 0 0 1             |                   |\n","| 1 0 1             |                   |\n","| 1 1 1             |                   |\n","| ----------------- |                   |"]},{"cell_type":"markdown","metadata":{"id":"7y4guiD8jdxy","colab_type":"text"},"source":["---\n","## 4 - Stride\n","\n","For a convolutional or a pooling operation, the stride $S$ denotes the number of pixels by which the window moves after each operation. \n","\n","Below is the example of slicing with **stride=**$2$\n","<center>\n","  <img src='https://stanford.edu/~shervine/images/stride.png' width=600><br>\n","  <img src='https://stanford.edu/~shervine/images/padding-valid-a.png' width=300></center>"]},{"cell_type":"markdown","metadata":{"id":"R0E6oQgEcZ1R","colab_type":"text"},"source":["---\n","\n","### a. Slicing Stride=1\n","* with an input of size $5\\times5$ and a filter of size $3\\times3$, \n","* if we slice with **stride=**$1$\n","* there will be $9$ slices across the space, \n","* with each slice should be a $3\\times3$ matrix (same size as filter)\n","\n","<br>\n","\n","Now, to do it automatically to slice all possible location, we can use for loop as follow:"]},{"cell_type":"code","metadata":{"id":"JSfqtViicZ1S","colab_type":"code","colab":{}},"source":["f_size = 3\n","output_h = 3\n","output_w = 3\n","\n","output = []\n","\n","for h in range(output_h):\n","    \n","    w_out = []\n","    for w in range(output_w):\n","        \n","        slices = get_slice(X_int, h, w, f_size, f_size)\n","        \n","        s = 'h='+str(h)+', w='+str(w)+'\\nshape='+str(slices.shape)+'\\n'\n","        s += '\\n'.join(' '.join(str(cell) for cell in row) for row in slices[:,:,0])\n","        s += '\\n----------------'\n","        w_out.append(s)\n","        \n","    output.append(w_out)\n","\n","\n","    \n","s = '\\n'.join(' '.join(str(cell) for cell in row) for row in X_int[:,:,0])\n","print('X_int =')\n","print(tabulate([[s]],tablefmt=\"pipe\"))\n","print('\\nslices =')\n","print(tabulate(output,tablefmt=\"pipe\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3YHOxZCEcZ1X","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","|:-----------------|:-----------------|:-----------------|\n","| h=0, w=0         | h=0, w=1         | h=0, w=2         |\n","| shape=(3, 3, 1)  | shape=(3, 3, 1)  | shape=(3, 3, 1)  |\n","| 0 1 0            | 1 0 1            | 0 1 1            |\n","| 0 0 1            | 0 1 1            | 1 1 1            |\n","| 1 0 1            | 0 1 1            | 1 1 1            |\n","| ---------------- | ---------------- | ---------------- |\n","| h=1, w=0         | h=1, w=1         | h=1, w=2         |\n","| shape=(3, 3, 1)  | shape=(3, 3, 1)  | shape=(3, 3, 1)  |\n","| 0 0 1            | 0 1 1            | 1 1 1            |\n","| 1 0 1            | 0 1 1            | 1 1 1            |\n","| 1 1 1            | 1 1 0            | 1 0 0            |\n","| ---------------- | ---------------- | ---------------- |\n","| h=2, w=0         | h=2, w=1         | h=2, w=2         |\n","| shape=(3, 3, 1)  | shape=(3, 3, 1)  | shape=(3, 3, 1)  |\n","| 1 0 1            | 0 1 1            | 1 1 1            |\n","| 1 1 1            | 1 1 0            | 1 0 0            |\n","| 0 1 0            | 1 0 1            | 0 1 1            |\n","| ---------------- | ---------------- | ---------------- |"]},{"cell_type":"markdown","metadata":{"id":"Zr9ed78rcZ1Y","colab_type":"text"},"source":["---\n","### b. Slicing Stride=2\n","* with an input of size $5\\times5$ and a filter of size $3\\times3$, \n","* if we slice with **stride=**$2$\n","* there will be $4$ slices across the space, \n","* with each slice should be a $3\\times3$ matrix (same size as filter)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fxZCjW7ti8dK","colab_type":"text"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","  * Look closely at the code to slice input with  **stride=**$1$\n","  * Modify the code so that the slice can implement  **stride=**$2$\n"]},{"cell_type":"code","metadata":{"id":"ZocXCnsJcZ1Z","colab_type":"code","colab":{}},"source":["f_size = 3\n","stride = 2\n","\n","output_h = 2\n","output_w = 2\n","\n","output = []\n","\n","for h in range(output_h):\n","    \n","    w_out = []\n","    for w in range(output_w):\n","        \n","        slices = get_slice(X_int, ??, ??, f_size, f_size)\n","        \n","        s = 'h='+str(h)+', w='+str(w)+'\\nshape='+str(slices.shape)+'\\n'\n","        s += '\\n'.join(' '.join(str(cell) for cell in row) for row in slices[:,:,0])\n","        s += '\\n----------------'\n","        w_out.append(s)\n","        \n","    output.append(w_out)\n","\n","\n","    \n","s = '\\n'.join(' '.join(str(cell) for cell in row) for row in X_int[:,:,0])\n","print('X_int =')\n","print(tabulate([[s]],tablefmt=\"pipe\"))\n","print('\\nslices =')\n","print(tabulate(output,tablefmt=\"pipe\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XF_xpEZNcZ1e","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","|:-----------------|:-----------------|\n","| h=0, w=0         | h=0, w=1         |\n","| shape=(3, 3, 1)  | shape=(3, 3, 1)  |\n","| 0 1 0            | 0 1 1            |\n","| 0 0 1            | 1 1 1            |\n","| 1 0 1            | 1 1 1            |\n","| ---------------- | ---------------- |\n","| h=1, w=0         | h=1, w=1         |\n","| shape=(3, 3, 1)  | shape=(3, 3, 1)  |\n","| 1 0 1            | 1 1 1            |\n","| 1 1 1            | 1 0 0            |\n","| 0 1 0            | 0 1 1            |\n","| ---------------- | ---------------- |"]},{"cell_type":"markdown","metadata":{"id":"B_7ez5MocZ1g","colab_type":"text"},"source":["---\n","## 5 - Convolution Naive Forward\n","\n","\n","The core of a convolutional network is the convolution operation. \n","\n","In the forward pass, you will \n","* take many filters and convolve them on the input. \n","* each '**convolution**' gives a 2D matrix output. \n","*  then stack these outputs to get a 3D volume.\n","\n","<center>\n","<img src=\"https://image.ibb.co/imSLj9/conv_kiank2.gif\" width=\"800\">\n","</center>\n","\n","\n","<br>\n","\n","And from previous exercises, you should already figure it out the formula to calculate the output volume size \n","\n","given the input volume size, filter size, padding size, and stride\n","\n","$$\n","\\begin{align}\n","H_{out} &= 1+\\frac{(H_{in}+2P-H_f)}{S}\\\\\\\\\n","W_{out} &= 1+\\frac{(W_{in}+2P-W_f)}{S}\\\\\\\\\n","C_{out} &= F\n","\\end{align}\n"," $$"]},{"cell_type":"markdown","metadata":{"id":"zq42FNsRlICi","colab_type":"text"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement the forward pass** for the convolution layer in the function `conv_forward_naive`. \n","\n","<br>\n","\n","Note: \n","<pre>You don't have to worry too much about efficiency at this point; \n","just write the code in whatever way you find most clear.\n","\n"]},{"cell_type":"code","metadata":{"id":"uQBySQwecZ1n","colab_type":"code","colab":{}},"source":["def conv_forward_naive(X, W, b, conv_param):\n","    \"\"\"\n","    A naive implementation of the forward pass for a convolutional layer.\n","\n","    The input consists of N data points, each with height H_in, width W_in and C channels. \n","    \n","    We convolve each input with F different filters, where each filter\n","    spans all C channels and has height H_f and width W_f.\n","\n","    Input:\n","    - X: Input data of shape (N, H_in, W_in, C)\n","    - W: Filter weights of shape (F, H_f, W_f, C)\n","    - b: Biases, of shape (F,)\n","    - conv_param: A dictionary with the following keys:\n","      - 'stride': The number of pixels between adjacent receptive fields in the\n","        horizontal and vertical directions.\n","      - 'pad': The number of pixels that will be used to zero-pad the input. \n","        \n","\n","    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n","    along the height and width axes of the input. Be careful not to modfiy the original\n","    input x directly.\n","\n","    Returns a tuple of:\n","    - out: Output data, of shape (N, H_out, W_out, C_out) where H_out and W_out are given by\n","      H_out = 1 + (H_in + 2 * pad - H_f) / stride\n","      W_out = 1 + (W_in + 2 * pad - W_f) / stride\n","      C_out = number of filter (F)\n","    - cache: (X, W, b, conv_param)\n","    \"\"\"\n","    out = None\n","    \n","    N, H_in, W_in, C = X.shape\n","    F, H_f, W_f, _ = W.shape\n","    \n","    P = conv_param[\"pad\"]\n","    S = conv_param[\"stride\"]\n","\n","    # output (activation) size after convolution\n","    H_out = ??\n","    W_out = ??\n","    C_out = ??\n","    \n","    # initialize output matrix\n","    out = np.zeros((N, H_out, W_out, C_out))\n","\n","    # add P zero padding to input X by calling zero_pad function\n","    X_pad = ??\n","\n","    # Convolution Forward Pass\n","    # loop for all data \n","    for i in range(N):\n","        \n","        # get the i-th example from padded X_pad\n","        X_i = X_pad[i,:,:,:]\n","\n","        # loop for all filter\n","        for f in range(F):            \n","\n","            # loop in output height\n","            for h in range(H_out):\n","\n","                #loop in output width\n","                for w in range(W_out):                \n","        \n","                    # get slice of X_i by calling get_slice function with the size of H_f and W_f\n","                    # Note: mind the stride!\n","                    X_slice = ??\n","                    \n","                    # calculate the convolution output by calling conv_single_step function\n","                    # with input X_slice, W[f], and b[f]\n","                    out[i, h, w, f] = ??\n","                                        \n","    # Making sure your output shape is correct\n","    assert(out.shape == (N, H_out, W_out, C_out))\n","    \n","    cache = (X, W, b, conv_param)\n","    \n","    return out, cache\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uECP4VMulcRd","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"Sllr7hCZcZ1s","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","X_rand = np.random.randn(10,4,4,3)\n","W = np.random.randn(8,2,2,3)\n","b = np.random.randn(8,)\n","\n","conv_param = {\"pad\" : 2, \"stride\": 1}\n","\n","\n","Z, cache_conv = conv_forward_naive(X_rand, W, b, conv_param)\n","print(\"Z's mean =\", np.mean(Z))\n","print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OfHVdp8FcZ1w","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Z's mean = 0.03749357578367075\n","cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]"]},{"cell_type":"code","metadata":{"id":"AERqUPpMqVls","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","X_rand = np.random.randn(10,4,4,3)\n","W = np.random.randn(8,2,2,3)\n","b = np.random.randn(8,)\n","\n","conv_param = {\"pad\" : 3, \"stride\": 2}\n","\n","\n","Z, cache_conv = conv_forward_naive(X_rand, W, b, conv_param)\n","print(\"Z's mean =\", np.mean(Z))\n","print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NUSDO65oqjZ0"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Z's mean = 0.003582574977755886\n","cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]"]},{"cell_type":"markdown","metadata":{"id":"pz4bN2KEcZ2L","colab_type":"text"},"source":["---\n","## 6 - Remove Pad\n","\n","Since normally we use pad to $X$ berfore we convolve it with filters, thus the backward propagate will result $dX$ with size of padded $X$. \n","\n","As a helper function, we need a function to remove the pad from $dX$ so it can be the same size as $X$\n"]},{"cell_type":"markdown","metadata":{"id":"53EJjGGQsuEk","colab_type":"text"},"source":["\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","**Remove padding from X**\n","\n","In this excercise, X is a 4-dimensional array, and we want to remove the pad in the 2nd and 3rd dimension\n","\n","*Hint*: the idea is the same as slicing"]},{"cell_type":"code","metadata":{"id":"nm76EdtocZ2L","colab_type":"code","colab":{}},"source":["def remove_pad(X_pad, pad):    \n","    \"\"\"\n","    Remove the padding that is applied to the height and width of X\n","    \n","    Argument:\n","    X_pad -- python numpy array of shape (m, H_n, W_n, C_n) representing a batch of m images\n","    pad -- integer, amount of padding to remove from around each image on vertical and horizontal dimensions\n","    \n","    Returns:\n","    X --  image of shape (m, H_n - 2*pad, W_n - 2*pad, C_n)\n","    \"\"\"\n","    X = ??\n","    \n","    return X"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YLgwfXdp9GiP","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"0W6_xH0AcZ2O","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","\n","x = np.random.randn(4, 3, 3, 2)\n","print (\"x.shape         =\", x.shape)\n","\n","x_pad = zero_pad(x, 2)\n","print (\"x_pad.shape     =\", x_pad.shape)\n","\n","x_removed = remove_pad(x_pad,2)\n","print (\"x_removed.shape =\", x_removed.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c2j1mkS4cZ2R","colab_type":"text"},"source":["**Expected Result**:\n","<pre>\n","x.shape         = (4, 3, 3, 2)\n","x_pad.shape     = (4, 7, 7, 2)\n","x_removed.shape = (4, 3, 3, 2)"]},{"cell_type":"code","metadata":{"id":"TCdDhykacZ2S","colab_type":"code","colab":{}},"source":["print('original images')\n","plt.rcParams['figure.figsize'] = (6.0, 6.0) # set default size of plots\n","fig, axarr = plt.subplots(1, 2)\n","axarr[0].set_title('x_pad[0,:,:,0]')\n","axarr[0].imshow(x_pad[0,:,:,0])\n","axarr[1].set_title('x_pad[1,:,:,0]')\n","axarr[1].imshow(x_pad[1,:,:,0])\n","plt.show()\n","\n","print('\\nremoved pad images')\n","fig, axarr = plt.subplots(1, 2)\n","axarr[0].set_title('x_removed[0,:,:,0]')\n","axarr[0].imshow(x_removed[0,:,:,0])\n","axarr[1].set_title('x_removed[1,:,:,0]')\n","axarr[1].imshow(x_removed[1,:,:,0])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xis7H_vRcZ2U","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","    original images\n","![pad](https://image.ibb.co/kwZSgp/pad.png)\n","\n","    removed pad images\n","![no_pad](https://image.ibb.co/by0dnU/no_pad.png) "]},{"cell_type":"markdown","metadata":{"id":"GfsiJlQScZ2J","colab_type":"text"},"source":["---\n","## 7 - Convolution Naive Backward\n","\n","In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers don't need to bother with the details of the backward pass. \n","\n","The backward pass for convolutional networks is complicated, but we'll try to implement the backward pass step by step\n"]},{"cell_type":"markdown","metadata":{"id":"qqU0NOPlcZ2K","colab_type":"text"},"source":["---\n","### a. Convolution Backward Math\n","\n","Now remember that in convolutional layer, the backward pass is also a convolution process. \n","\n","<center>\n","  <img src='https://image.ibb.co/mkvjCU/backward.png' width=400>\n","</center>\n","\n","First, let's take a look at the math\n","\n","\n","As the normal Neural Net layer, there are three (3) steps to make in Backward pass:\n","* Computing $\\partial X$\n","* Computing $\\partial W$, and \n","* Computing $\\partial b$\n","\n","Now let's take a look on how to implement those"]},{"cell_type":"markdown","metadata":{"id":"SPlK0Ez561P4","colab_type":"text"},"source":["---\n","#### Computing $\\partial X$:\n","This is the formula for computing $\\partial A$ with respect to the cost for a certain filter $W_c$ and a given training example:\n","\n","$$ dX += \\sum _{h=0} ^{n\\_H} \\sum_{w=0} ^{n\\_W} W_f \\times dout_{hw} \\tag{1}$$\n","\n","* Where $W_f$ is a filter and $dout_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv current layer at the $h$-th row and $w$-th column \n","* (corresponding to the dot product taken at the $i$-th stride left and $j$-th stride down). \n","\n","<br>\n","\n","Note that at each time, we multiply the the same filter $W_f$ by a different $dout$ when updating $dX$. We do so mainly because when computing the forward propagation, each filter is dotted and summed by a different $X_{slices}$. \n","\n","Therefore when computing the backprop for $dX$, we are just adding the gradients of all the $X_{slices}$. \n","\n","<br>\n","\n","In code, inside the appropriate for-loops, this formula translates into:\n","\n","```python\n","dX[h_start:h_end, w_start:w_end, :] += W[f,:,:,:] * dout[i, h, w, f]\n","```"]},{"cell_type":"markdown","metadata":{"id":"0RIlJekV8Tw-","colab_type":"text"},"source":["---\n","\n","#### Computing $\\partial W$:\n","This is the formula for computing $dW_f$ with respect to the loss:\n","\n","$$ dW_f  += \\sum _{h=0} ^{n\\_H} \\sum_{w=0} ^ {n\\_W} X_{slice} \\times dout_{hw}  \\tag{2}$$\n","\n","* ($dW_f$ is the derivative of one filter)\n","\n","<br>\n","\n","Where $X_{slice}$ corresponds to the slice which was used to generate the acitivation $out_{ij}$. \n","\n","Hence, this end up giving us the gradient for $W$ with respect to that slice. \n","\n","Since it is the same $W$, we will just add up all such gradients to get $dW$. \n","\n","<br>\n","\n","In code, inside the appropriate for-loops, this formula translates into:\n","```python\n","dW[f,:,:,:] += X_slice * dout[i, h, w, f]\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"_-FDQJQ_830k","colab_type":"text"},"source":["---\n","#### Computing $\\partial b$:\n","\n","This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:\n","\n","$$ db = \\sum_h \\sum_w dout_{hw} \\tag{3}$$\n","\n","As you have previously seen in basic neural networks, db is computed by summing $dout$. In this case, you are just summing over all the gradients of the current conv output with respect to the cost. \n","\n","<br>\n","\n","In code, inside the appropriate for-loops, this formula translates into:\n","```python\n","db[f,:,:,:] += dout[i, h, w, f]\n","```"]},{"cell_type":"markdown","metadata":{"id":"aiu-sZXGcZ2V","colab_type":"text"},"source":["---\n","### b - Convolution Backward Naive Function\n","\n","After all helper functions are made, we can implement the naive backward pass for convolution layer in the function conv_backward_naive\n"]},{"cell_type":"markdown","metadata":{"id":"AnPScDv3uVut","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","**Implement the backward pass**\n","\n","    Again, you don't need to worry too much about computational efficiency."]},{"cell_type":"code","metadata":{"id":"doc6TvjDcZ2X","colab_type":"code","colab":{}},"source":["def conv_backward_naive(dout, cache):\n","    \"\"\"\n","    A naive implementation of the backward pass for a convolutional layer.\n","\n","    Inputs:\n","    - dout: Upstream derivatives.\n","    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n","\n","    Returns a tuple of:\n","    - dx: Gradient with respect to x\n","    - dw: Gradient with respect to w\n","    - db: Gradient with respect to b\n","    \"\"\"\n","    \n","    dX, dW, db = None, None, None\n","    \n","    X, W, b, conv_param = cache\n","\n","    N, H_out, W_out, F = dout.shape\n","    N, H_in,  W_in,  C = X.shape\n","    F, H_f,   W_f,   C = W.shape\n","    P = conv_param[\"pad\"]\n","    S = conv_param[\"stride\"]\n","\n","    \n","    # add P zero padding to input X by calling zero_pad function\n","    X_pad = zero_pad(X, P)\n","\n","    # initialize dX with zeros matrix of size = X_pad\n","    dX = np.zeros(X_pad.shape)  \n","    # initialize dW with zeros matrix of size = W\n","    dW = np.zeros(W.shape)   \n","    # initialize db with zeros matrix of size = b\n","    db = np.zeros(b.shape) \n","        \n","    ## ------------------------- start your code here -------------------------\n","    \n","    # Convolution Backward Pass\n","    # loop for all data \n","    for i in range(N):\n","        \n","        # select the i-th example from X_pad\n","        X_i = ??\n","        \n","        # select the i-th gradient from dX\n","        dX_i = ??\n","        \n","        # loop for all filter\n","        for f in range(F):            \n","            \n","            # loop in output height\n","            for h in range(H_out):\n","                \n","                #loop in output width\n","                for w in range(W_out):\n","                    \n","                    # get slice of X_i by calling get_slice function with the size of H_f and W_f\n","                    # Note: mind the stride!\n","                    X_slice = ??\n","                    \n","                    # get slice of dX_i by calling get_slice function with the size of H_f and W_f\n","                    # Note: mind the stride!\n","                    dX_slice = ??\n","                    \n","                    # update the value of gradient dX_slice by adding the result from W[f] * dout[i, h, w, f]\n","                    dX_slice += ??\n","                    \n","                    # update the value of gradient dW[f] by adding the result from X_slice * dout[i, h, w, f]\n","                    dW[f] += ??\n","                    \n","                    # update the value of gradient db[f] by adding dout[i, h, w, f] to it\n","                    db[f] += ??\n","    \n","    # remove the pad in dX by calling remove_pad with size of P\n","    dX = ??\n","    \n","    \n","    ## ------------------------- your code end here ---------------------------\n","    \n","    # Making sure your output shape is correct\n","    assert(dX.shape == (N, H_in, W_in, C))\n","    \n","    return dX, dW, db\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4B0pzyCPvFxv","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"G50V8eFTcZ2Y","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","A_prev = np.random.randn(10,4,4,3)\n","W = np.random.randn(8,2,2,3)\n","b = np.random.randn(8,)\n","conv_param = {\"pad\" : 2, \"stride\": 1}\n","\n","Z, cache_conv = conv_forward_naive(A_prev, W, b, conv_param)\n","print(\"Z's mean               =\", np.mean(Z))\n","print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])\n","print()\n","\n","dX, dW, db = conv_backward_naive(Z, cache_conv)\n","print(\"dX_mean =\", np.mean(dX))\n","print(\"dW_mean =\", np.mean(dW))\n","print(\"db_mean =\", np.mean(db))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3LBYg_wTcZ2b","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Z's mean               = 0.03749357578367075\n","cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n","\n","dX_mean = 7.60493719340998\n","dW_mean = 9.271277763672389\n","db_mean = 18.371852133998935"]},{"cell_type":"markdown","metadata":{"id":"6Z7kaYndcZ1w","colab_type":"text"},"source":["---\n","## 8 - Image processing via convolutions\n","\n","As fun way to both check your implementation and gain a better understanding of the type of operation that convolutional layers can perform, we will set up an input containing two images and manually set up filters that perform common image processing operations (**grayscale conversion** and **edge detection**).\n","\n","The convolution forward pass will apply these operations to each of the input images.\n","\n","We can then visualize the results as a sanity check."]},{"cell_type":"code","metadata":{"id":"sRhnPpsHcZ1z","colab_type":"code","colab":{}},"source":["from imageio import imread\n","from skimage.transform import resize\n","\n","\n","def imshow_noax(img, normalize=True):\n","    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n","    if normalize:\n","        img_max, img_min = np.max(img), np.min(img)\n","        img = 255.0 * (img - img_min) / (img_max - img_min)\n","    plt.imshow(img.astype('uint8'))\n","    plt.gca().axis('off')\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xpf2keubvhyQ","colab_type":"text"},"source":["---\n","### a. Load Images\n","\n","For this part, we've provided you with two example images\n","\n","you can change the image by changing the links"]},{"cell_type":"code","metadata":{"id":"5rx5ZUKivUH1","colab_type":"code","colab":{}},"source":["!wget -q 'https://raw.githubusercontent.com/CNN-ADF/Task2019/master/resources/kitten.jpg'\n","!wget -q 'https://raw.githubusercontent.com/CNN-ADF/Task2019/master/resources/puppy.jpg'\n","\n","\n","# load the image\n","kitten, puppy = imread('kitten.jpg'), imread('puppy.jpg')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MkKdnNW1vkE5","colab_type":"text"},"source":["---\n","### b. View Images\n","\n","Now show the two cute images"]},{"cell_type":"code","metadata":{"id":"Py4zIV5JcZ14","colab_type":"code","colab":{}},"source":["# kitten is wide, and puppy is already square\n","d = kitten.shape[1] - kitten.shape[0]\n","kitten_cropped = kitten[:, d//2:-d//2, :]\n","\n","\n","img_size = 200   # Make this smaller if it runs too slow\n","\n","\n","X_image = np.zeros((2, img_size, img_size, 3))\n","X_image[0, :, :, :] = resize(puppy, (img_size, img_size), mode='constant')\n","X_image[1, :, :, :] = resize(kitten_cropped, (img_size, img_size), mode='constant')\n","\n","plt.rcParams['figure.figsize'] = (6.0, 6.0) # set default size of plots\n","imshow_noax(X_image[0, :, :, :])\n","plt.show()\n","imshow_noax(X_image[1, :, :, :])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPBcAG9svtSB","colab_type":"text"},"source":["---\n","### c. Grayscale Filter\n","\n","Set up a convolutional weights holding 2 filters, each 3x3\n","\n","The first filter converts the image to grayscale. Set up the red, green, and blue channels of the filter"]},{"cell_type":"code","metadata":{"id":"Dt-eQIVDcZ19","colab_type":"code","colab":{}},"source":["W_image = np.zeros((2, 3, 3, 3))\n","\n","W_image[0, :, :, 0] = [[0, 0, 0], [0, 0.3, 0], [0, 0, 0]]\n","W_image[0, :, :, 1] = [[0, 0, 0], [0, 0.6, 0], [0, 0, 0]]\n","W_image[0, :, :, 2] = [[0, 0, 0], [0, 0.1, 0], [0, 0, 0]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJeYCSwWv7W7","colab_type":"text"},"source":["---\n","### d. Edge Detection Filter\n","\n","Second filter detects horizontal edges in the blue channel.\n","\n","For Edge Detection filter, we need to add 128 to each output so that nothing is negative. So we create a Vector of biases\n"]},{"cell_type":"code","metadata":{"id":"PYocC6PCwGAW","colab_type":"code","colab":{}},"source":["W_image[1, :, :, 2] = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]\n","\n","b_image = np.array([0, 128])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jzgIPuo3wYF1","colab_type":"text"},"source":["---\n","### e. Process Convolution\n","\n","Compute the result of convolving each input in **X_image** with each filter in **W_image**, offsetting by **b_image**, and storing the results in $out$."]},{"cell_type":"code","metadata":{"id":"dXhEwWabcZ1_","colab_type":"code","colab":{}},"source":["out, _ = conv_forward_naive(X_image, W_image, b_image, {'stride': 1, 'pad': 1})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74QbAHJwwtly","colab_type":"text"},"source":["---\n","### f. View Results\n","Let's take a look at the greyscale and edge detection results"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"H8js-9xncZ2C","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = (15.0, 8.0) # set default size of plots\n","# Show the original images and the results of the conv operation\n","plt.subplot(2, 3, 1)\n","imshow_noax(puppy, normalize=False)\n","plt.title('Original image')\n","plt.subplot(2, 3, 2)\n","imshow_noax(out[0, :, :, 0])\n","plt.title('Grayscale')\n","plt.subplot(2, 3, 3)\n","imshow_noax(out[0, :, :, 1])\n","plt.title('Edges')\n","plt.subplot(2, 3, 4)\n","imshow_noax(kitten_cropped, normalize=False)\n","plt.subplot(2, 3, 5)\n","imshow_noax(out[1, :, :, 0])\n","plt.subplot(2, 3, 6)\n","imshow_noax(out[1, :, :, 1])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8NAJcirpcZ2H","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","\n","![conv_try](https://image.ibb.co/iUWdnU/conv_try_resize.png)\n"]},{"cell_type":"markdown","metadata":{"id":"kWN3beG3cZ2b","colab_type":"text"},"source":["---\n","---\n","# [Part 2] Pooling Layer\n","\n","The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input. The two types of pooling layers are: \n","\n","- Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.\n","\n","- Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.\n","\n","<p align='center'>\n","<table>\n","<td>\n","<img src=\"https://image.ibb.co/dfLq5e/max_pool1.png\" width=500>\n","<td>\n","\n","<td>\n","<img src=\"https://image.ibb.co/hegmdz/a_pool.png\" width=500>\n","<td>\n","</table>\n","</p>\n","\n","<br>\n","\n","These pooling layers have no parameters for backpropagation to train. However, they have hyperparameters such as the window size $f$. \n","This specifies the height and width of the $[f\\times f]$ window you would compute a **max** or **average** over. \n","\n","Now, you are going to implement **MAX-POOL** and **AVG-POOL**, in the same function. "]},{"cell_type":"markdown","metadata":{"id":"XUZz9Np3cZ2b","colab_type":"text"},"source":["---\n","## 1 - Pooling Naive Forward\n","\n","As there's no padding, the formulas binding the output shape of the pooling to the input shape is:\n","\n","\n","$$\n","\\begin{align}\n","H_{out} &= \\lfloor \\frac{H_{in} - H_f}{S} \\rfloor +1\\\\\\\\\n","W_{out} &= \\lfloor \\frac{W_{in} - W_f}{S} \\rfloor +1\\\\\\\\\n","C_{out} &= C_{in}\n","\\end{align}\n"," $$\n"," \n"]},{"cell_type":"markdown","metadata":{"id":"6malzPLVyTAQ","colab_type":"text"},"source":["\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement the **forward pass for the max-pooling** operation in the function `max_pool_forward_naive`\n","\n","Follow the hints in the comments below. Again, don't worry too much about computational efficiency.\n"]},{"cell_type":"code","metadata":{"id":"bMnmKo7EcZ2c","colab_type":"code","colab":{}},"source":["def pool_forward_naive(X, pool_param, mode='max'):\n","    \"\"\"\n","    A naive implementation of the forward pass for a max-pooling layer.\n","\n","    Inputs:\n","    - X: Input data, of shape (N, H_in, W_in, C)\n","    - pool_param: dictionary with the following keys:\n","      - 'pool_height': The height of each pooling region\n","      - 'pool_width': The width of each pooling region\n","      - 'stride': The distance between adjacent pooling regions\n","    - mode: the pooling mode you would like to use, defined as a string (\"max\" or \"avg\")\n","\n","    No padding is necessary here. Output size is given by \n","\n","    Returns a tuple of:\n","    - out: Output data, of shape (N, H_out, W_out, C) where H_out and W_out are given by\n","      H_out = 1 + (H_in - pool_height) / stride\n","      W_out = 1 + (W_in - pool_width) / stride\n","    - cache: (X, pool_param)\n","    \"\"\"\n","    \n","    out = None\n","    \n","    N, H_in, W_in, C = X.shape\n","    \n","    pool_height = pool_param['pool_height']\n","    pool_width = pool_param['pool_width']\n","    stride = pool_param['stride']\n","    \n","    \n","    ## ------------------------- start your code here -------------------------\n","    \n","    # Define the dimensions of the output\n","    H_out = ??\n","    W_out = ??\n","\n","    # initialize zeros matrix for output with the specified dimension\n","    out = np.zeros([N, H_out, W_out, C]) \n","    \n","    # Pooling Forward Pass\n","    # loop for all data \n","    for i in range(N):         \n","            \n","        # loop in output height\n","        for h in range(H_out):\n","\n","            #loop in output width\n","            for w in range(W_out):\n","                \n","                # loop for all Channel\n","                for c in range(C):           \n","                    \n","                    # get slice of X[i] by calling get_slice function with the size of pool_height and pool_width\n","                    # Note: mind the stride!\n","                    X_slice = ??\n","                    \n","                    # Compute the pooling operation on the c-th slice. \n","                    # Use an if statment to differentiate the modes. \n","                    # Use np.max or np.mean to X_slice[:,:,c]\n","                    \n","                    if mode == \"max\":\n","                        out[i, h, w, c] = ??\n","                    elif mode == \"avg\":\n","                        out[i, h, w, c] = ??\n","                        \n","                        \n","    ## ------------------------- your code end here ---------------------------\n","\n","    \n","    # Making sure your output shape is correct\n","    assert(out.shape == (N, H_out, W_out, C))\n","    \n","    # Store the input and pool_param in \"cache\" for pool_backward()\n","    cache = (X, pool_param)\n","    \n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xNt-ha39hrn","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"TDzxfl7YcZ2f","colab_type":"code","colab":{}},"source":["np.random.seed(48)\n","A_prev = np.random.randint(0, 10,(2, 4, 4, 3))\n","pool_param = {'pool_width': 4, 'pool_height': 4, 'stride': 1}\n","\n","print('Input activation A_prev =')\n","print(tabulate(A_prev.transpose(0,1,3,2)[0],tablefmt=\"pipe\"))\n","print(tabulate(A_prev.transpose(0,1,3,2)[1],tablefmt=\"pipe\"))\n","print()\n","\n","A, cache = pool_forward_naive(A_prev, pool_param)\n","print('mode = max pool')\n","print('pool shape =',A.shape)\n","print(tabulate(A.reshape(2,3),tablefmt=\"pipe\"))\n","print()\n","\n","A, cache = pool_forward_naive(A_prev, pool_param, mode = 'avg')\n","print('mode = average pool')\n","print('pool shape =',A.shape)\n","print(tabulate(A.reshape(2,3),tablefmt=\"pipe\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VIwRnV0gcZ2h","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Input activation A_prev =\n","|:----------|:----------|:----------|\n","| [0 4 6 0] | [3 0 6 6] | [1 0 2 6] |\n","| [0 2 2 2] | [7 2 5 6] | [8 5 9 2] |\n","| [1 0 6 6] | [5 5 6 2] | [6 2 5 2] |\n","| [5 3 3 0] | [4 2 3 2] | [7 0 6 1] |\n","|:----------|:----------|:----------|\n","| [6 9 4 6] | [9 1 2 3] | [3 7 1 5] |\n","| [0 2 3 0] | [7 1 0 7] | [4 9 3 4] |\n","| [0 1 5 9] | [3 0 5 2] | [3 6 9 0] |\n","| [8 6 0 5] | [8 7 0 1] | [5 6 3 6] |\n","\n","mode = max pool\n","pool shape = (2, 1, 1, 3)\n","|--:|--:|--:|\n","| 6 | 7 | 9 |\n","| 9 | 9 | 9 |\n","\n","mode = average pool\n","pool shape = (2, 1, 1, 3)\n","|----:|----:|------:|\n","| 2.5 | 4   | 3.875 |\n","| 4   | 3.5 | 4.625 |\n"]},{"cell_type":"markdown","metadata":{"id":"QLyLs1ytcZ2h","colab_type":"text"},"source":["---\n","## 2 - Pooling Naive Backward\n","\n","\n","Next, let's implement the backward pass for the pooling layer, starting with the **MAX-POOL** layer. \n","\n","Even though a pooling layer has no parameters for backprop to update, you still need to propagate back the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. "]},{"cell_type":"markdown","metadata":{"id":"fPw6YNtQcZ2j","colab_type":"text"},"source":["---\n","### a. MaxPool: Mask Function\n","\n","Before jumping into the backpropagation of the pooling layer, you are going to build a helper function called `create_mask_from_window()` which does the following: \n","\n","$$ X = \\begin{bmatrix}\n","1 && 3 \\\\\n","4 && 2\n","\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n","0 && 0 \\\\\n","1 && 0\n","\\end{bmatrix}$$\n","\n","<br>\n","\n","As you can see, this function creates a **\"mask\"** matrix which keeps track of where the maximum of the matrix is. \n","\n","* True (1) indicates the position of the maximum in X, \n","* the other entries are False (0). \n","\n","<br>\n"]},{"cell_type":"markdown","metadata":{"id":"Pep2cUf5EkGY","colab_type":"text"},"source":["\n","Why do we keep track of the position of the max? \n","\n","It's because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. \n","\n","<p align='center'>\n","<img src='https://image.ibb.co/jYWpe9/maxpool.png'>\n","</p>\n","\n","So, backprop will \"propagate\" the gradient back to this particular input value that had influenced the cost. \n","\n","<br>\n","\n","\n","You'll see later that the backward pass for average pooling will be similar to this but using a different mask.  "]},{"cell_type":"markdown","metadata":{"id":"5dWwV52p38nv","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement `create_mask_from_window()`. \n","\n","\n","Hints:\n","- [np.max()]() may be helpful. It computes the maximum of an array.\n","- If you have a matrix X and a scalar x: `A = (X == x)` will return a matrix A of the same size as X such that:\n","```\n","A[i,j] = True if X[i,j]   = x\n","A[i,j] = False if X[i,j] != x\n","```\n","- Here, you don't need to consider cases where there are several maxima in a matrix."]},{"cell_type":"code","metadata":{"id":"x6nfXYHMcZ2j","colab_type":"code","colab":{}},"source":["def create_mask_from_window(x):\n","    \"\"\"\n","    Creates a mask from an input matrix x, to identify the max entry of x.\n","    \n","    Arguments:\n","    x -- Array of shape (f, f)\n","    \n","    Returns:\n","    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n","    \"\"\"\n","    \n","    mask = ??\n","    \n","    return mask"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cECPAkcP4bMF","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7M4LaVtPcZ2l","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","x = np.random.randint(0,10,(2,3))\n","\n","mask = create_mask_from_window(x)\n","print('x =')\n","print(x)\n","\n","print('\\nmax x = ', np.max(x),'\\n')\n","\n","print('mask =')\n","print(mask)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEXaf-rGcZ2n","colab_type":"text"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","x =\n","[[5 8 9]\n"," [5 0 0]]\n","\n","max x =  9 \n","\n","mask =\n","[[False False  True]\n"," [False False False]]"]},{"cell_type":"markdown","metadata":{"id":"ll7v_udacZ2q","colab_type":"text"},"source":["---\n","### b. AvgPool: Distribute Function\n","\n","In max pooling, for each input window, all the \"influence\" on the output came from a single input value--the max. \n","\n","In average pooling, every element of the input window has equal influence on the output. So to implement backprop, you will now implement a helper function that reflects this.\n","\n","For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you'll use for the backward pass will look like: \n","$$ dX = 1 \\quad \\rightarrow  \\quad dX =\\begin{bmatrix}\n","1/4 && 1/4 \\\\\n","1/4 && 1/4\n","\\end{bmatrix}\\tag{5}$$\n","\n","This implies that each position in the $dX$ matrix contributes equally to output because in the forward pass, we took an average. \n","\n","<p align='center'><img src='https://image.ibb.co/mYj6sU/avgpool.png'></p>\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TEg5A-V7FGQF"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement `distribute_value()`. \n","\n","Implement the function to equally distribute a value dz through a matrix of dimension shape. \n"]},{"cell_type":"code","metadata":{"id":"Y2bH9oqKcZ2r","colab_type":"code","colab":{}},"source":["def distribute_value(dout, shape):\n","    \"\"\"\n","    Distributes the input value in the matrix of dimension shape\n","    \n","    Arguments:\n","    dout -- input scalar, Upstream derivatives\n","    shape -- the shape (H_out, W_out) of the output matrix for which we want to distribute the value of dout\n","    \n","    Returns:\n","    a -- Array of size (H_out, W_out) for which we distributed the value of dout\n","    \"\"\"\n","    \n","    # Retrieve dimensions from shape (≈1 line)\n","    (H_out, W_out) = shape\n","    \n","    # Compute the value to distribute on the matrix (≈1 line)\n","    average = ??\n","    \n","    # Create a matrix where every entry is the \"average\" value (≈1 line)\n","    # use np.ones\n","    a = ??\n","    \n","    return a"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSyKMk9xFgx4","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"dL7D5ii4cZ2t","colab_type":"code","colab":{}},"source":["dout = 2\n","print('dout =', dout)\n","\n","shape = (2,2)\n","a = distribute_value(dout, shape)\n","print('distributed value =')\n","print(a)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N5_lFRmacZ2v","colab_type":"text"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","dout = 2\n","distributed value =\n","[[0.5 0.5]\n"," [0.5 0.5]]"]},{"cell_type":"markdown","metadata":{"id":"2txzPqkZcZ2w","colab_type":"text"},"source":["---\n","### c. Pooling Backward Function\n","\n","You now have everything you need to compute backward propagation on a pooling layer.\n"]},{"cell_type":"markdown","metadata":{"id":"0QIyVSOUFfNy","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement the **pool_backward()** function in both modes (`'max'` and `'avg'`). \n","\n","* You will once again use **4 for-loops** (iterating over training examples, height, width, and channels). \n","\n","* You should use an `if/elif` statement to see if the mode is equal to `'max'` or `'avg'`. \n","* If it is equal to `'average'` you should use the `distribute_value()` function you implemented above to create a matrix of the same shape as `a_slice`. \n","* Otherwise, the mode is equal to `'max'`, and you will create a mask with `create_mask_from_window()` and multiply it by the corresponding value of dout."]},{"cell_type":"code","metadata":{"id":"f07gf2zhcZ2w","colab_type":"code","colab":{}},"source":["def pool_backward_naive(dout, cache, mode = 'max'):\n","    \"\"\"\n","    Implements the backward pass of the pooling layer\n","    \n","    Arguments:\n","    dout -- gradient of cost with respect to the output of the pooling layer, same shape as X\n","    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n","    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"avg\")\n","    \n","    Returns:\n","    dX -- gradient of cost with respect to the input of the pooling layer, same shape as X\n","    \"\"\"\n","    \n","    ### START CODE HERE ###\n","    \n","    # Retrieve information from cache (≈1 line)\n","    (X, pool_param) = cache\n","    \n","    # Retrieve hyperparameters from \"pool_param\" (≈2 lines)\n","    pool_height = pool_param['pool_height']\n","    pool_width = pool_param['pool_width']\n","    stride = pool_param['stride']\n","    \n","    # Retrieve dimensions from X's shape and dout's shape (≈2 lines)\n","    N, H_in, W_in, C = X.shape\n","    N, H_out, W_out, C = dout.shape\n","    \n","    # Initialize dA_prev with zeros with the same size as X\n","    dX = np.zeros_like(X)\n","    \n","    \n","    ## ------------------------- start your code here -------------------------\n","    \n","    # loop over the training examples\n","    for i in range(N):                       \n","        \n","        # select i-th training example from X\n","        X_i = ??\n","        \n","        # select the i-th gradient from dX\n","        dX_i = ??        \n","        \n","        # loop in out height\n","        for h in range(H_out):\n","            \n","            # loop in out width\n","            for w in range(W_out):\n","                \n","                # loop for all Channel\n","                for c in range(C):                    \n","                    \n","                    # get slice of X[i] by calling get_slice function with the size of pool_height and pool_width\n","                    # Note: mind the stride!\n","                    X_slice = ??\n","\n","                    # get slice of dX_i by calling get_slice function with the size of pool_height and pool_width\n","                    # REMEMBER: mind the stride!\n","                    dX_slice = ??\n","                    \n","                    \n","                    # Compute the backward propagation in both modes.\n","                    if mode == 'max':\n","                        \n","                        # Create the mask from X_slice[:,:,c] window by calling create_mask_from_window\n","                        mask = ??\n","                                                \n","                        # update dX_slice[:,:,c] by adding the result of mask * dout[i,h,w,c]\n","                        dX_slice[:,:,c] += ??\n","\n","                    elif mode == 'avg':                  \n","                        \n","                        # Get the value dx from dout[i, h, w, c]\n","                        dx = ??\n","                        \n","                        # Define the shape of the filter as [pool_height x pool_width] (≈1 line)\n","                        shape = (pool_height, pool_width)\n","                                                \n","                        # update dX_slice[:,:,c] by adding the result of the distributed value of dx \n","                        # by calling distribute_value winth input dX and shape\n","                        dX_slice[:,:,c] += ??\n","                        \n","                        \n","    ## ------------------------- your code end here ----------------------------\n","\n","    # Making sure your output shape is correct\n","    assert(dX.shape == X.shape)\n","    \n","    return dX"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"65nTq94sF8-n","colab_type":"text"},"source":["Check your implementations"]},{"cell_type":"code","metadata":{"id":"v6tO4jH7cZ2z","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","A_prev = np.random.randn(5, 5, 3, 2)\n","pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 1}\n","A, cache = pool_forward_naive(A_prev, pool_param)\n","dA = np.random.randn(5, 4, 2, 2)\n","\n","dA_prev = pool_backward_naive(dA, cache, mode = 'max')\n","print(\"mode = max\")\n","print('mean of dA = ', np.mean(dA))\n","print('dA_prev[1,1] =') \n","print(dA_prev[1,1]) \n","print()\n","\n","dA_prev = pool_backward_naive(dA, cache, mode ='avg')\n","print(\"mode = average\")\n","print('mean of dA = ', np.mean(dA))\n","print('dA_prev[1,1] =') \n","print(dA_prev[1,1]) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UPoxXFuqcZ22","colab_type":"text"},"source":["**EXPECTED OUTPUT**: \n","<pre>\n","mode = max\n","mean of dA =  0.14571390272918056\n","dA_prev[1,1] =\n","[[ 0.          0.        ]\n"," [ 5.05844394 -1.68282702]\n"," [ 0.          0.        ]]\n","\n","mode = average\n","mean of dA =  0.14571390272918056\n","dA_prev[1,1] =\n","[[ 0.08485462  0.2787552 ]\n"," [ 1.26461098 -0.25749373]\n"," [ 1.17975636 -0.53624893]]\n"]},{"cell_type":"markdown","metadata":{"id":"MMKfg974cZ26","colab_type":"text"},"source":["---\n","---\n","# [Part 3] Fast Layer API\n","\n","Making convolution and pooling layers fast can be challenging. \n","\n","To spare you the pain, we've provided **fast implementations** of the forward and backward passes for convolution and pooling layers in the file `fast_layers.py`"]},{"cell_type":"code","metadata":{"id":"g-NOnruaGRas","colab_type":"code","colab":{}},"source":["!wget -q 'https://raw.githubusercontent.com/CNN-ADF/Task2019/master/resources/fast_layers.py'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmtME4JWcZ27","colab_type":"code","colab":{}},"source":["from fast_layers import *\n","from time import time\n","\n","def rel_error(x, y):\n","  \"\"\" returns relative error \"\"\"\n","  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n","\n","np.random.seed(1)\n","X_big = np.random.randn(100, 31, 31, 3)\n","W_big = np.random.randn(25, 3, 3, 3)\n","b_big = np.random.randn(25,)\n","dout_big = np.random.randn(100, 16, 16, 25)\n","conv_param = {'stride': 2, 'pad': 1}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tb6T0T2lcZ29","colab_type":"text"},"source":["---\n","## 1 - Fast Convolution Layer\n","\n","Now let's compare the naive version of Convolution layer with the fast implementation"]},{"cell_type":"code","metadata":{"id":"PGdP-c93cZ2_","colab_type":"code","colab":{}},"source":["t0 = time()\n","Z_naive, cache_conv_naive = conv_forward_naive(X_big, W_big, b_big, conv_param)\n","t1 = time()\n","Z_fast, cache_conv_fast = conv_forward_fast(X_big, W_big, b_big, conv_param)\n","t2 = time()\n","\n","print('Testing conv_forward_fast:')\n","print('Naive     : %f seconds' % (t1 - t0))\n","print('Fast      : %f seconds' % (t2 - t1))\n","print('Speedup   : %fx faster' % ((t1 - t0) / (t2 - t1)))\n","print('Difference:', rel_error(Z_naive, Z_fast))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nJroePaDcZ3C","colab_type":"text"},"source":["**Expected Result**:\n","\n","    Difference should be close to 0. (smaller than 1e-10)\n","    Fast Layer should speed up the naive layer by hudreds folds"]},{"cell_type":"code","metadata":{"id":"Y6gCRvsHcZ3D","colab_type":"code","colab":{}},"source":["t0 = time()\n","dX_naive, dW_naive, db_naive = conv_backward_naive(dout_big, cache_conv_naive)\n","t1 = time()\n","dX_fast, dW_fast, db_fast = conv_backward_fast(dout_big, cache_conv_fast)\n","t2 = time()\n","\n","print('\\nTesting conv_backward_fast:')\n","print('Naive        : %f seconds' % (t1 - t0))\n","print('Fast         : %f seconds' % (t2 - t1))\n","print('Speedup      : %fx faster' % ((t1 - t0) / (t2 - t1)))\n","print('dx difference:', rel_error(dX_naive, dX_fast))\n","print('dw difference:', rel_error(dW_naive, dW_fast))\n","print('db difference:', rel_error(db_naive, db_fast))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qL5TqvHcZ3F","colab_type":"text"},"source":["**Expected Result**:\n","\n","    Difference in dx, dw, db should be close to 0. (all smaller than 1e-10)\n","    \n","    Fast Layer should speed up the naive layer by more than 40 folds"]},{"cell_type":"markdown","metadata":{"id":"BxZKybcbcZ3L","colab_type":"text"},"source":["---\n","## 2 - Fast Pooling Layer"]},{"cell_type":"code","metadata":{"id":"GPcY7M_IcZ3M","colab_type":"code","colab":{}},"source":["# Relative errors should be close to 0.0\n","\n","np.random.seed(231)\n","X_big2 = np.random.randn(100, 32, 32, 3)\n","dout_big2 = np.random.randn(100, 16, 16, 3)\n","pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n","\n","t0 = time()\n","out_naive, cache_naive = pool_forward_naive(X_big2, pool_param)\n","t1 = time()\n","out_fast, cache_fast = pool_forward_fast(X_big2, pool_param)\n","t2 = time()\n","\n","print('Testing max pool_forward_fast:')\n","print('Naive     : %f seconds' % (t1 - t0))\n","print('fast      : %f seconds' % (t2 - t1))\n","print('speedup   : %fx faster' % ((t1 - t0) / (t2 - t1)))\n","print('difference:', rel_error(out_naive, out_fast))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"385MiL-xcZ3O","colab_type":"text"},"source":["**Expected Result**:\n","\n","    Difference should be close to 0."]},{"cell_type":"code","metadata":{"id":"xcLpPjdscZ3O","colab_type":"code","colab":{}},"source":["\n","t0 = time()\n","dX_naive = pool_backward_naive(dout_big2, cache_naive)\n","t1 = time()\n","dX_fast = pool_backward_fast(dout_big2, cache_fast)\n","t2 = time()\n","\n","print('\\nTesting max pool_backward_fast:')\n","print('Naive        : %f seconds' % (t1 - t0))\n","print('Fast         : %f seconds' % (t2 - t1))\n","print('Speedup      : %fx faster' % ((t1 - t0) / (t2 - t1)))\n","print('dx difference:', rel_error(dX_naive, dX_fast))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BsH1z1OwcZ3T","colab_type":"text"},"source":["**Expected Result**:\n","\n","    Difference should be close to 0."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"GbYYbFWAcZ3T","colab_type":"code","colab":{}},"source":["t0 = time()\n","out_naive, cache_naive = pool_forward_naive(X_big2, pool_param, mode='avg')\n","t1 = time()\n","out_fast, cache_fast = pool_forward_fast(X_big2, pool_param, mode='avg')\n","t2 = time()\n","\n","print('Testing avg pool_forward_fast:')\n","print('Naive     : %f seconds' % (t1 - t0))\n","print('fast      : %f seconds' % (t2 - t1))\n","print('speedup   : %fx faster' % ((t1 - t0) / (t2 - t1)))\n","print('difference:', rel_error(out_naive, out_fast))\n","\n","\n","t0 = time()\n","dX_naive = pool_backward_naive(dout_big2, cache_naive, mode='avg')\n","t1 = time()\n","dX_fast = pool_backward_fast(dout_big2, cache_fast, mode='avg')\n","t2 = time()\n","\n","print('\\nTesting avg pool_backward_fast:')\n","print('Naive        : %f seconds' % (t1 - t0))\n","print('Fast         : %f seconds' % (t2 - t1))\n","print('Speedup      : %fx faster' % ((t1 - t0) / (t2 - t1)))\n","print('dx difference:', rel_error(dX_naive, dX_fast))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2-i1PMucZ3V","colab_type":"text"},"source":["**Expected Result**:\n","\n","    Difference should be close to 0."]},{"cell_type":"markdown","metadata":{"id":"3pa9zX5vcZ3G","colab_type":"text"},"source":["---\n","## 3 - Image processing via convolutions"]},{"cell_type":"code","metadata":{"id":"VtM55WIEcZ3G","colab_type":"code","colab":{}},"source":["t0 = time()\n","out_naive, _ = conv_forward_naive(X_image, W_image, b_image, {'stride': 1, 'pad': 1})\n","t1 = time()\n","out_fast, _  = conv_forward_fast(X_image, W_image, b_image, {'stride': 1, 'pad': 1})\n","t2 = time()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqkP6FHacZ3J","colab_type":"code","colab":{}},"source":["print('\\nTesting:')\n","print('Naive  : %f seconds' % (t1 - t0))\n","print('Fast   : %f seconds' % (t2 - t1))\n","print('Speedup: %fx faster' % ((t1 - t0) / (t2 - t1)))\n","\n","plt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\n"," \n","# Show the original images and the results of the conv operation\n","plt.subplot(4, 3, 1)\n","imshow_noax(puppy, normalize=False)\n","plt.title('Original image')\n","plt.subplot(4, 3, 2)\n","imshow_noax(out_naive[0, :, :, 0])\n","plt.title('Grayscale Naive')\n","plt.subplot(4, 3, 3)\n","imshow_noax(out_naive[0, :, :, 1])\n","plt.title('Edges Naive')\n","plt.subplot(4, 3, 5)\n","imshow_noax(out_fast[0, :, :, 0])\n","plt.title('Grayscale Fast Layer')\n","plt.subplot(4, 3, 6)\n","imshow_noax(out_fast[0, :, :, 1])\n","plt.title('Edges Fast Layer')\n","\n","plt.subplot(4, 3, 7)\n","imshow_noax(kitten_cropped, normalize=False)\n","plt.subplot(4, 3, 8)\n","imshow_noax(out_naive[1, :, :, 0])\n","plt.title('Grayscale Naive')\n","plt.subplot(4, 3, 9)\n","imshow_noax(out_naive[1, :, :, 1])\n","plt.title('Edges Naive')\n","plt.subplot(4, 3, 11)\n","imshow_noax(out_fast[1, :, :, 0])\n","plt.title('Grayscale Fast Layer')\n","plt.subplot(4, 3, 12)\n","imshow_noax(out_fast[1, :, :, 1])\n","plt.title('Edges Fast Layer')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2YqJ5FPcZ3W","colab_type":"text"},"source":["Next we implement the \"sandwich\" layer blocks to make it easier to use in deeper network"]},{"cell_type":"markdown","metadata":{"id":"TgYk-mpGcZ3X","colab_type":"text"},"source":["---\n","---\n","# [Part 4] \"Sandwich\" Layer API\n","\n","You've build a working Deep Neural Network before using Deep Neural Net API. And now we're adding a couple more layers to the mix.\n","\n","Now you can just stack those in order like before, but you must've noticed that there's some kind of fixed order in stacking layers\n","\n","Like in Deep Neural Net, in the hidden layers, after each affine layer, we always add an activation layer. \n","\n","And now in Covolutional Neural Network, after each conv layer, you either add an activation layer or add an activation folling by a pool layer.\n","\n","Therefore, we can pre-stack those layer combinations into a concept of \"sandwich\" layers that combine multiple operations into commonly used patterns. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"frfjzcyncZ3X","colab_type":"text"},"source":["---\n","## 1 - Affine-ReLU Block\n","Let's combine Affine layer with ReLU layer into Affine-ReLU Block"]},{"cell_type":"markdown","metadata":{"id":"GgLUVeIxGlxl","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","* Implement `affine_relu_forward` functions by stacking the necessary layer"]},{"cell_type":"code","metadata":{"id":"eOP7UnCUcZ3X","colab_type":"code","colab":{}},"source":["def affine_relu_forward(X, W, b):\n","    \"\"\"\n","    Convenience layer that perorms an affine transform followed by a ReLU\n","\n","    Inputs:\n","    - X: Input to the affine layer\n","    - W, b: Weights for the affine layer\n","\n","    Returns a tuple of:\n","    - out: Output from the ReLU\n","    - cache: Object to give to the backward pass\n","    \"\"\"\n","    \n","    # calculate layer score by calling affine forward function using X, W, and b\n","    act, fc_cache = ??\n","    \n","    # calculate activation score by calling relu forward function using layer act\n","    out, relu_cache = ??\n","    \n","    cache = (fc_cache, relu_cache)\n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JaUIfbwbOcKL","colab_type":"text"},"source":["* Implement `affine_relu_backward` functions by stacking the necessary layer"]},{"cell_type":"code","metadata":{"id":"DIiqtfIIcZ3a","colab_type":"code","colab":{}},"source":["def affine_relu_backward(dout, cache):\n","    \"\"\"\n","    Backward pass for the affine-relu convenience layer\n","    \"\"\"    \n","    fc_cache, relu_cache = cache\n","    \n","    # calculate gradient by calling relu backward function using dout and relu_cache\n","    dact = ??\n","    \n","     # calculate layer weights gradient by calling affine backward function using dact and fc_cache\n","    dX, dW, db = ??\n","    \n","    return dX, dW, db\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKhMyWy9cZ3b","colab_type":"text"},"source":["---\n","## 2 - Conv-ReLU Block\n","\n","Next is Conv-ReLU Block"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lF5Zr1lpHEjo"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","* Implement `conv_relu_forward` functions by stacking the necessary layer\n","\n","* **Use the Fast Layer**"]},{"cell_type":"code","metadata":{"id":"O42kYvbecZ3d","colab_type":"code","colab":{}},"source":["def conv_relu_forward(X, W, b, conv_param):\n","    \"\"\"\n","    A convenience layer that performs a convolution followed by a ReLU.\n","\n","    Inputs:\n","    - X: Input to the convolutional layer\n","    - W, b, conv_param: Weights and parameters for the convolutional layer\n","\n","    Returns a tuple of:\n","    - out: Output from the ReLU\n","    - cache: Object to give to the backward pass\n","    \"\"\"\n","    \n","    # calculate layer score by calling conv forward fast function using X, W, b, and conv_param\n","    # NOTE: USE FAST FUNCTION\n","    act, conv_cache = ??\n","    \n","    # calculate activation score by calling relu forward function\n","    out, relu_cache = ??\n","    \n","    cache = (conv_cache, relu_cache)\n","    return out, cache"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ci7TPyU0OqF9","colab_type":"text"},"source":["\n","* Implement `conv_relu_backward` functions by stacking the necessary layer\n","\n","* **Use the Fast Layer**"]},{"cell_type":"code","metadata":{"id":"a7rxFpLxcZ3f","colab_type":"code","colab":{}},"source":["def conv_relu_backward(dout, cache):\n","    \"\"\"\n","    Backward pass for the conv-relu convenience layer.\n","    \"\"\"\n","    conv_cache, relu_cache = cache\n","    \n","    # calculate gradient by calling relu backward function\n","    dact = ??\n","    \n","    # calculate layer weights gradient by calling conv backward fast function using dact and conv_cache\n","    # NOTE: USE FAST FUNCTION\n","    dX, dW, db = ??\n","    \n","    return dX, dW, db"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tyNZZsBxcZ3g","colab_type":"text"},"source":["---\n","## 3 - Conv-ReLU-MaxPool Block\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RSzK_sf2Hfzi"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","* Implement `conv_relu_pool_forward` \n","\n","* **Use the Fast Layer**\n","\n","* **Use max pool** "]},{"cell_type":"code","metadata":{"id":"7g-OyucTcZ3h","colab_type":"code","colab":{}},"source":["def conv_relu_pool_forward(X, W, b, conv_param, pool_param):\n","    \"\"\"\n","    Convenience layer that performs a convolution, a ReLU, and a pool.\n","\n","    Inputs:\n","    - X: Input to the convolutional layer\n","    - W, b, conv_param: Weights and parameters for the convolutional layer\n","    - pool_param: Parameters for the pooling layer\n","\n","    Returns a tuple of:\n","    - out: Output from the pooling layer\n","    - cache: Object to give to the backward pass\n","    \"\"\"\n","    \n","    # calculate layer score by calling conv forward fast function using X, W, b, and conv_param\n","    # NOTE: USE FAST FUNCTION\n","    act, conv_cache = ??\n","    \n","    # calculate activation score by calling relu forward function\n","    s, relu_cache = ??\n","    \n","    # calculate max pool score by calling pool forward fast function using s and pool_param\n","    # NOTE: USE FAST FUNCTION\n","    out, pool_cache = ??\n","    \n","    cache = (conv_cache, relu_cache, pool_cache)    \n","    return out, cache\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TXXAGdIbO5KF","colab_type":"text"},"source":["* Implement `conv_relu_pool_backward`\n","\n","* **Use the Fast Layer**\n","\n","* **Use max pool** "]},{"cell_type":"code","metadata":{"id":"LSPD5L5LcZ3i","colab_type":"code","colab":{}},"source":["def conv_relu_pool_backward(dout, cache):\n","    \"\"\"\n","    Backward pass for the conv-relu-pool convenience layer\n","    \"\"\"\n","    conv_cache, relu_cache, pool_cache = cache\n","    \n","    # propagate gradient to pooling layer by calling pool backward fast function\n","    # NOTE: USE FAST FUNCTION\n","    ds = ??\n","    \n","    # calculate gradient by calling relu backward function\n","    dact = ??\n","    \n","    # calculate layer weights gradient by calling conv backward fast function using dact and conv_cache\n","    # NOTE: USE FAST FUNCTION\n","    dX, dW, db = ??\n","    \n","    return dX, dW, db"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHR4eI8icZ3k","colab_type":"text"},"source":["---\n","---\n","# [Part 5] Three-layer ConvNet\n","Now that you have implemented all the necessary layers, we can put them together into a simple convolutional network.\n","\n","Complete the implementation of the `three_layer_convnet` function below.\n","\n","<br>\n","\n","A three-layer convolutional network with the following architecture:\n","\n","<pre>\n","|                                |               |                  |\n","| <font color='red'>7x7 conv</font> - <font color=''>relu</font> - <font color='blue'>2x2 max pool</font> | <font color='brown'>affine</font> - <font color=''>relu</font> | <font color='brown'>affine</font> - <font color=''>softmax</font> |\n","|                                |               |                  |\n","|              1                 |       2       |        3         |\n","</pre>\n","<br>\n","\n","The network operates on minibatches of data that have shape $(N, H, W, C)$ <br>\n","consisting of $N$ images, each with height $H$, width $W$, and $C$ input channels."]},{"cell_type":"markdown","metadata":{"id":"-37zeD0vcZ3m","colab_type":"text"},"source":["---\n","## 1 - Predict Function\n","\n","Implement the predict function first, because we are going to use **predict** function inside the **training** function to track the **validation** accuracy "]},{"cell_type":"markdown","metadata":{"id":"9MAPCkEycZ3m","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement Predict Function**\n","\n","    * call forward conv-relu-pool function\n","    * call forward affine-relu function\n","    * call forward affine function"]},{"cell_type":"code","metadata":{"id":"6wYbGY2ycZ3o","colab_type":"code","colab":{}},"source":["def predict_three_layer_convnet(X, W, b, params):    \n","    \"\"\"\n","    Inputs:\n","    - X         : Input data, of shape(N, D)\n","    - W         : list of Weight\n","    - b         : list of biases\n","    - params    : parameter for convolution and pooling layer\n","    \n","    Output:\n","    - y_pred : list of class prediction\n","    \"\"\"\n","        \n","    # get parameters\n","    conv_param, pool_param = params\n","        \n","    # Forward Pass Layer 1 :\n","    # calculate 1st layer score by calling conv-relu-pool forward function \n","    # using X, W[0], b[0], conv_param, and pool_param\n","    act1, cache1 = ??\n","\n","    # Forward Pass Layer 2 : \n","    # calculate 2nd layer score by calling affine-relu forward function \n","    # using act1, W[1], and b[1]\n","    act2, cache2 = ??\n","\n","    # Forward Pass Layer 3 :\n","    # calculate 2nd layer score by calling affine forward function \n","    # using act2, W[2], and b[2]\n","    act3, cache3 = ??\n","    \n","    # take the maximum prediction from act3 and use that column to get the class          \n","    # use np.argmax with axis=-1 \n","    y_pred = ??\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nAUbXcazcZ3p","colab_type":"text"},"source":["---\n","## 2 - Train Function\n","\n","Now let's complete the training function\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ed1JLEjzUFig"},"source":["<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","**Implement Training Function**\n","\n","there are **four steps** in this training function\n","\n","---\n","\n","**1. Forward Pass**\n","\n","    * call forward conv-relu-pool function\n","    * call forward affine-relu function\n","    * call forward affine function for the last layer\n","    * call softmax score function\n","\n","**2. Calculate Loss**\n","\n","    * call softmax_loss function\n","    * calculate loss with regularization\n","\n","\n","**3. Backward Pass**\n","\n","    * call backward affine function for the last layer\n","    * call backward affine-relu function\n","    * call backward conv-relu-pool function\n","\n","**4. Weight Update**\n","\n","    * implement weight update\n","    * calculate the training and validation accuracy"]},{"cell_type":"code","metadata":{"id":"uO6_8_-OcZ3p","colab_type":"code","colab":{}},"source":["def train_three_layer_convnet(X, y, X_val, y_val,\n","                              num_filters=32, filter_size=(7, 7),\n","                              hidden_dim=100, W=None, b=None,\n","                              lr=1e-4, lr_decay=0.95, reg=0.001, \n","                              epochs=100,  batch_size=50, \n","                              verbose=True, print_every=100):\n","    \"\"\"\n","    Inputs:\n","    - X          : Array, shape (N, C, H, W) of training images\n","    - y          : Array, shape (N,) of labels for training images\n","    - X_val      : Array, shape (N_val, C, H, W) of validation images\n","    - y_val      : Array, shape (N_val,) of labels for validation images   \n","    - num_filters: Number of filters to use in the convolutional layer\n","    - filter_size: Width/height of filters to use in the convolutional layer\n","    - hidden_dim: Number of units to use in the fully-connected hidden layer\n","    - W          : list of Weight, if W is None, it will be initialized\n","    - b          : list of biases, if W is None, bias will be initialized\n","    - lr         : float, initial learning rate\n","    - lr_decay   : float, 0-1, decay rate to reduce learning rate each epoch\n","    - reg        : float, regularization rate\n","    - epochs     : int, number of training epoch\n","    - batch_size : int, number of batch used each step\n","    - verbose    : boolean, verbosity\n","    \n","    Outputs:\n","    - W          : list of trained Weights\n","    - b          : list of trained biases\n","    - history    : list of training history [loss, train_acc, val_acc]\n","    - params     : tuple of convolution and pooling layer parameters\n","    \n","    \"\"\"\n","    \n","    \n","       \n","    # extract input size\n","    num_train, H_in, W_in, C_in = X.shape    \n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","    \n","    \n","    # assume y takes values 0...K-1 where K is number of classes\n","    num_classes = np.max(y) + 1 \n","    \n","    # extract filter size\n","    H_f, W_f = filter_size    \n","    \n","    # set stride and padding\n","    f_stride = 1\n","    pad = (H_f - 1)//2\n","\n","    \n","    ## ------------------------- start your code here -------------------------\n","\n","    \n","    # calculate output convolution size (1st layer activation)\n","    # output conv height = 1 + (input height + 2 * padding size - filter height)/ filter stride\n","    conv_H = ??\n","    \n","    # output conv width = 1 + (input width + 2 * padding size - filter width) / filter stride\n","    conv_W = ??\n","    \n","    \n","    \n","    # set pool size and stride\n","    pool_size   = 2\n","    pool_stride = 2\n","    \n","    # calculate output pool size (1st layer pool) based on conv layer output size\n","    # output pool height  = 1 + (conv height - pool size) / pool stride\n","    pool_H = ??\n","    \n","    # output pool width  =  1 + (conv width - pool size) / pool stride \n","    pool_W = ??\n","\n","    # initialize weights and biases\n","    if W is None:        \n","        W0 = 1e-3 * np.random.randn(num_filters, H_f, W_f, C_in).astype(np.float32)\n","        W1 = 1e-3 * np.random.randn(num_filters*pool_H*pool_W, hidden_dim).astype(np.float32)\n","        W2 = 1e-3 * np.random.randn(hidden_dim, num_classes).astype(np.float32)\n","        W = [W0, W1, W2]\n","        \n","        \n","    if b is None:\n","        b0 = np.zeros(num_filters).astype(np.float32)\n","        b1 = np.zeros((1,hidden_dim)).astype(np.float32)\n","        b2 = np.zeros((1,num_classes)).astype(np.float32)\n","        b = [b0, b1, b2]\n","        \n","        \n","    # set training parameters   \n","    conv_param = {'stride': 1, 'pad': pad}\n","    pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n","    \n","        \n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","    dW = {}\n","    db = {}\n","    dx = {}\n","    \n","                     \n","    print('start training')\n","    \n","    for ep in range(epochs):\n","      \n","       # Shuffle data train index\n","        train_rows = np.arange(num_train)\n","        np.random.shuffle(train_rows)\n","        \n","        # split index into mini batches\n","        id_batch = np.split(train_rows, num_iter)\n","  \n","        for batch in id_batch:\n","      \n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","        \n","\n","            # ------------------------------------------------\n","            # 1. Forward Pass\n","            # ------------------------------------------------\n","            \n","            \n","            # Forward Pass Layer 1 (Conv):\n","            # calculate 1st layer score by calling conv-relu-pool forward function \n","            # using X_batch, W[0], b[0], conv_param, and pool_param\n","            act1, cache1 = ??\n","\n","            # Forward Pass Layer 2 (Affine): \n","            # calculate 2nd layer score by calling affine-relu forward function \n","            # using act1, W[1], and b[1]\n","            act2, cache2 = ??\n","\n","            # Forward Pass Layer 3 (Affine):\n","            # calculate 2nd layer score by calling affine forward function \n","            # using act2, W[2], and b[2]\n","            act3, cache3 = ??\n","\n","            # calculate softmax score by calling softmax function using act3 \n","            softmax_score = ??\n","\n","            # ------------------------------------------------\n","            # 2. Calculate Loss\n","            # ------------------------------------------------\n","      \n","            # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n","            loss, dout = ??\n","\n","            # add regularization to the loss:\n","            #    for each weights, calculate the sum square, multiply regularization strength\n","            #    then add it to the loss\n","            for w in W:\n","                loss += reg * np.sum(w*w)\n","\n","            # append the loss history\n","            loss_history.append(loss)\n","        \n","        \n","\n","            # ------------------------------------------------\n","            # 3. Backward Pass\n","            # ------------------------------------------------    \n","            \n","            # Backward Pass Layer 3 (Affine):\n","            # calculate layer 3 gradients by calling affine backward function using dout and cache3\n","            dact3, dW[2], db[2] = ??\n","\n","            # Forward Pass Layer 2 (Affine): \n","            # calculate layer 2 gradients by calling affine-relu backward function using dact3 and cache2\n","            dact2, dW[1], db[1] = ??\n","\n","            # Forward Pass Layer 1 (Conv): \n","            # calculate layer 1 gradients by calling conv-relu-pool backward function using dact2 and cache1\n","            dact1, dW[0], db[0] = ??\n","\n","\n","\n","            # ------------------------------------------------\n","            # 4. Weight Update\n","            # ------------------------------------------------    \n","\n","            # add regularization to the gradient:\n","            #    for each gradient dW, add with twice of the weight multiplied by regularization strength\n","            #    dwi = dwi + 2 * reg * wi\n","            # loop over W\n","            for i in range(len(W)):   \n","                dW[i] += ??\n","\n","\n","            # perform parameter update by subtracting W[i] and b[i] for each layer with a fraction of dW[i] and db[i]\n","            # according to the learning rate\n","            # loop over W\n","            for i in range(len(W)):    \n","                # w_i = w_i - lr * dw_i\n","                W[i] -= ??\n","    \n","                # b_i = b_i - lr * db_i\n","                b[i] -= ??\n","\n","\n","            # iteration count\n","            it +=1\n","\n","            if verbose and it % print_every == 1:\n","                print ('iteration',it,'/',num_iter,'\\t(epoch', ep+1,'/',epochs, '): \\tloss =', loss)\n","                \n","                \n","        # At the end of one epoch\n","        # 1. Check accuracy\n","        #    calculate the training accuracy by calling predict_three_layer_convnet function on X_batch\n","        #    and compare it to y_batch. Then calculate the mean correct (accuracy in range 0-1)\n","        train_acc = (predict_three_layer_convnet(X_batch, W, b, (conv_param, pool_param)) == y_batch).mean()\n","        train_acc_history.append(train_acc)\n","\n","        # 2. Calculate the training accuracy by calling predict_three_layer_convnet function on X_val\n","        #    and compare it to y_val. Then calculate the mean correct (accuracy in range 0-1)\n","        val_acc = (predict_three_layer_convnet(X_val, W, b, (conv_param, pool_param)) == y_val).mean()\n","        val_acc_history.append(val_acc)\n","\n","        # 3. Decay learning rate\n","        #    multiply learning rate with decay\n","        lr *= ??\n","            \n","            \n","    ## ------------------------- your code end here ----------------------------\n","    \n","    history = [loss_history, train_acc_history, val_acc_history]\n","    \n","    if verbose:\n","      print('Training done')\n","      \n","    params = (conv_param, pool_param)\n","    \n","    return W, b, history, params"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Gcb4CFfcZ3t","colab_type":"text"},"source":["---\n","## 3 - Train a 3-Layer ConvNet\n","\n","By training the three-layer convolutional network for **one epoch**, you should achieve greater than **50% accuracy** on the training set\n","\n","<font color='red'>*nb: it took **several minutes** just to run 1 epoch*"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HbHQDxmeUYM7"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **three layer convolutional network** \n"]},{"cell_type":"code","metadata":{"id":"iMtpGbZHcZ3u","colab_type":"code","colab":{}},"source":["t1 = time()\n","W_conv, b_conv, history, params = train_three_layer_convnet(X_train, y_train, \n","                                                            X_val, y_val,\n","                                                            num_filters=36, filter_size=(7, 7),\n","                                                            hidden_dim=500, lr=1e-2, \n","                                                            lr_decay=0.95, reg=0.005, \n","                                                            epochs=1, batch_size=100, \n","                                                            verbose=True, print_every=20)\n","                                                     \n","t2 = time()\n","\n","training_time = (t2-t1)/60\n","print('\\ntraining time: %0.2f minutes' % training_time)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XPRTFnUFUYNB"},"source":["**Expected Output**:\n","<pre>\n","loss should start around 2.3 and end around 1.4\n","in around 25 minutes of training for just 1 epoch"]},{"cell_type":"code","metadata":{"id":"TouW65BMWDjQ","colab_type":"code","colab":{}},"source":["loss, train_acc, val_acc = history\n","\n","plt.rcParams['figure.figsize'] = (10, 6)\n","plt.plot(loss)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IQSVh0HOOgJT","colab_type":"text"},"source":["---\n","## 4 - Training Accuracy\n","Calculate the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"YfMbdyFmmNWK","colab_type":"code","colab":{}},"source":["print('Training Accuracy   = %.1f%%' % (train_acc[-1]*100))\n","\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_three_layer_convnet(X_val, W_conv, b_conv, params)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy = %.1f%%' % (accuracy*100))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PwxpQeb3mNWT","colab_type":"text"},"source":["**Expected Output**:\n","\n","<pre>\n","You should be able to get about <b>~70%</b> accuracy on training set using the initial run\n","You should also be able to get about <b>~45%</b> accuracy on validation set</pre>\n"]},{"cell_type":"markdown","metadata":{"id":"BlyXk9wncZ32","colab_type":"text"},"source":["---\n","---\n","# [Part 6] Visualize Filters\n","You can visualize the first-layer convolutional filters from the trained network by running the following codes"]},{"cell_type":"markdown","metadata":{"id":"iVnKn1ks66DN","colab_type":"text"},"source":["First, download the provided helper functions  \n"]},{"cell_type":"code","metadata":{"id":"1a5W5bM668ww","colab_type":"code","colab":{}},"source":["!wget -q 'https://raw.githubusercontent.com/CNN-ADF/Task2019/master/resources/vis_utils.py'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8cSVw0YJ69Nq","colab_type":"text"},"source":["Run Visualization"]},{"cell_type":"code","metadata":{"id":"ZsXunqvQcZ34","colab_type":"code","colab":{}},"source":["from vis_utils import visualize_grid\n","\n","grid = visualize_grid(W_conv[0])\n","plt.imshow(grid.astype('uint8'))\n","plt.axis('off')\n","plt.gcf().set_size_inches(7, 7)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rz-VC_BrZpH0","colab_type":"text"},"source":["You could see that the filters in the first Conv layer formed some low feature templates such as **colors** and **lines**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oIrMySbpM3HW"},"source":["---\n","---\n","# Congratulation, You've Completed Exercise 4\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>\n"]},{"cell_type":"markdown","metadata":{"id":"W_NHYAoycZ35","colab_type":"text"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","<br>\n","<br>\n","\n","---\n","<font color='blue'><h2> ------------------------ These Parts Are Optional ------------------------ </h2>\n","</font>  \n","\n","---\n","---\n","\n","# [Part 7] Deep ConvNet API\n","\n","You've build and train a Three layer Convolutional Neural Network\n","\n","But looks like we're back at the begining where we build a specific training function for each architecture. Which is really not flexible if we want to try different architecture and depth of the network.\n","\n","So now we're going to build a Deep Convolutional Neural Network API, much more like the popular Deep Learning Framework available, so that we can design and train a more deep and complex ConvNet with much ease."]},{"cell_type":"markdown","metadata":{"id":"v1Bkxaa5mX3Q","colab_type":"text"},"source":["---\n","## 1 - Weight Initializer\n","\n","First, let's define the weight initialization function like in the Deep Neural Net API before\n","\n","But now, apart from the affine weight initializer, we add another weight initializer for the filter of the convolution layer"]},{"cell_type":"code","metadata":{"id":"sFPN3JCUm15b","colab_type":"code","colab":{}},"source":["def init_weights_affine(d_in, d_out, std=1e-3):\n","    \"\"\"\n","    Weight initialization for affine layer\n","    \n","    Inputs:\n","    - d_in  : int, number of input dimension\n","    - d_out : int, number of output dimension\n","    - std   : standard deviation for generating weights\n","    - seed  : random seed\n","    \n","    Outputs:\n","    - W: list of Weights\n","    - b: list of biases\n","    \"\"\"\n","    \n","    W = std * np.random.randn(d_in, d_out).astype(np.float32)\n","    b = np.zeros((1, d_out)).astype(np.float32)\n","\n","    return W, b\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yKVZY8Pnax0","colab_type":"code","colab":{}},"source":["\n","def init_weights_conv(filter_size, std=1e-3):\n","    \"\"\"\n","    Weight initialization for convolution layer\n","    \n","    Inputs:\n","    - d_filter_size  : tuple of filter size (nF, Hf, Wf, Cf)\n","    - std            : standard deviation for generating weights\n","    - seed           : random seed\n","    \n","    Outputs:\n","    - W: list of Weights\n","    - b: list of biases\n","    \"\"\"  \n","    \n","    num_filter, H_f, W_f, C_f = filter_size\n","\n","    W = std * np.random.randn(num_filter, H_f, W_f, C_f).astype(np.float32)\n","    b = np.zeros(num_filter).astype(np.float32)\n","\n","    return W, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Itws6LKGcEl0","colab_type":"text"},"source":["---\n","## 2 - Model Initializer\n","\n","Next, let's start with Model Initialization Function\n","\n","This function will receive a list of designed neural net **architecture**, and it will initialize weights according to the layer type and parameters given in the designed architecture.\n","\n","The architecture feed in is a list of layers that is stacked in a **Sequential Model** fashion. The Layer is also a list of named layer type followed by its required parameters.\n","\n","The Model initializer receive **4 kinds** of layer:\n","\n","<br>\n","<font size=10>\n","<table border=1 solid>\n","  <tr border=1>\n","    <th><font size=2>      Layer Name    </font></th>\n","    <th><font size=2>      Definition and Parameters    </font></th>\n","    <th colspan=2><font size=2>      Parameters    </font></th>\n","  </tr>\n","  <tr>\n","    <td><font size=3>      <pre>'input'</pre>    </font></td>\n","    <td><font size=2>      input layer, <br>to determine the input shape    </font></td>\n","    <td><font size=3>      <pre>(h, w, c)</pre>    </font></td>\n","    <td><font size=2>      tuple of input dimension     </font></td>\n","  </tr>\n","  <tr>\n","    <td><font size=3>      <pre>'affine'</pre>    </font></td>\n","    <td><font size=2>      affine layer, <br>followed by relu activation    </font></td>\n","    <td><font size=3>      <pre>d_out</pre>    </font></td>\n","    <td><font size=2>      integer, <br>number of output neurons     </font></td>\n","  </tr>\n","  <tr>\n","    <td rowspan=4><font size=3>      <pre>'conv'</pre>    </font></td>\n","    <td rowspan=4><font size=2>      convolution layer, <br>followed by relu activation    </td>\n","    <td><font size=3>      <pre>(h, w, c)</pre>    </font></td>\n","    <td><font size=2>      tuple of filter dimension     </font></td>\n","  </tr>\n","  <tr>\n","    <td><font size=3>      <pre>f</pre>    </font></td>\n","    <td><font size=2>      integer, <br>number of filter     </font></td>\n","  </tr>\n","  <tr>\n","    <td><font size=3>      <pre>s</pre>   </font></td>\n","    <td><font size=2>      integer, <br>size of stride     </font></td>\n","  </tr>\n","  <tr>\n","    <td><font size=3>      <pre>p</pre>    </font></td>\n","    <td><font size=2>      integer, <br>size of padding     </font></td>\n","  </tr>\n","  <tr></tr>\n","  <tr>\n","    <td rowspan=2><font size=3>      <pre>'pool'</pre>    </font></td>\n","    <td rowspan=2><font size=2>      pooling layer </font></td>\n","    <td><font size=3>      <pre>(w, h)</pre>   </font></td>\n","    <td><font size=2>      tuple of pool size    </font> </td>\n","  </tr>\n","  <tr>\n","    <td><font size=3>      <pre>s</pre>    </font></td>\n","    <td><font size=2>      integer, <br>size of stride     </font></td>\n","  </tr>\n","</table>\n","\n","</font>"]},{"cell_type":"code","metadata":{"id":"BStKpGcOcZ35","colab_type":"code","colab":{}},"source":["def init_model(architecture):    \n","  \n","    \"\"\"\n","    Inputs:\n","    - architecture  : list of layer name and paramters\n","    \n","    \n","    Outputs:\n","    - neural net model, a tuple of:\n","      - architecture: list of layer name and paramters\n","      - W           : list of trained Weights\n","      - b           : list of trained biases\n","      - params      : list of compact layer parameters\n","    \n","    \"\"\"\n","    \n","    \n","    # define output container\n","    W = {}\n","    b = {}\n","    params = {}\n","    \n","    # reset input shape from previous layer\n","    h, w, c = 0, 0, 0\n","    d_prev = 0\n","    \n","    # the first layer must be input layer\n","    assert(architecture[0][0]=='input')\n","    \n","    \n","    # loop read list of layer\n","    for i in range(len(architecture)):\n","      \n","        # read the layer information\n","        layer = architecture[i]\n","        \n","        # if the layer is input, get the input shape\n","        if layer[0]=='input':\n","            \n","            h, w, c = layer[1]\n","            \n","            \n","        # if the layer is affine layer,\n","        # initialize weight and bias\n","        elif layer[0]=='affine':\n","          \n","            # get the previous output shape\n","            if d_prev==0:\n","                d_prev = h*w*c\n","                \n","            # get this layer output shape\n","            d_out = layer[1]\n","            \n","            # initialize weight and bias\n","            W[i], b[i] = init_weights_affine(d_prev, d_out)\n","            \n","            # set this layer output shape as previous output shape \n","            # for the next layer\n","            d_prev = d_out\n","            \n","            \n","        # if the layer is convolution layer\n","        # initialize weight and bias\n","        elif layer[0]=='conv':\n","            \n","            # get conv layer parameter\n","            filter_size, nf, stride, pad = layer[1:]\n","            \n","            # get filter shape\n","            hf, wf = filter_size\n","            \n","            # calculate this layer output shape\n","            h = 1 + (h + 2 * pad - hf) // stride\n","            w = 1 + (w + 2 * pad - wf) // stride\n","            \n","            # initialize weight and bias\n","            W[i], b[i] = init_weights_conv((nf, hf, wf, c))\n","            \n","            # set this layer output shape as previous output shape \n","            c = nf\n","            \n","            # compact the parameters\n","            params[i] = {'stride': stride, 'pad':pad}\n","            \n","            \n","        # if the layer is pooling layer\n","        elif layer[0]=='pool':\n","            \n","            # get pool layer parameter\n","            pool_size, stride = layer[1:]\n","            \n","            # get pooling size\n","            hp, wp = pool_size\n","            \n","            # calculate this layer output shape\n","            h = (h - hp) // stride + 1\n","            w = (w - hp) // stride + 1            \n","            \n","            # compact the parameters\n","            params[i] = {'pool_height': pool_size[0], 'pool_width': pool_size[1], 'stride': stride}\n","            \n","            \n","    # combine the weights and its parameters into a tuple of model\n","    model = (architecture, W, b, params)\n","            \n","    # return the network model\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FrwTn3lAxK9B","colab_type":"text"},"source":["---\n","## 3 - Predict Function\n","\n","Now let's build the predict function \n","\n","you should already be very familiar with this implementation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_h_62pH_HH6W"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement Predict Function**\n","\n","    * loop call forward function for each hidden layer weights\n","    * call forward function for the last layer"]},{"cell_type":"code","metadata":{"id":"u_4IocKKcZ3-","colab_type":"code","colab":{}},"source":["def predict(model, X):    \n","    \"\"\"\n","    Inputs:\n","    - model      : Network model architecture and weights\n","    - X    : Input data, of shape(N, D)\n","    \n","    Output:\n","    - y_pred : list of class prediction\n","    \"\"\"\n","    \n","    # get the model architecture and weights\n","    architecture, W, b, params = model\n","    \n","    # get number of layers\n","    n_layer = len(architecture)\n","    \n","    # set input X as the first activation\n","    act = X    \n","    \n","    \n","    # loop i over n_layer-1\n","    for i in range(n_layer-1):\n","      \n","      # get layer architecture\n","      layer = architecture[i]\n","      \n","      \n","      # check layer type\n","      if layer[0]=='affine':\n","        # call affine relu forward function with input act, W[i], and b[i]\n","        act, _ = ??\n","        \n","      elif layer[0]=='conv':        \n","        # call conv relu forward function with input act, W[i], b[i], and params[i]\n","        act, _ = ??\n","        \n","      elif layer[0]=='pool':\n","        # call pool forward fast function with input act and params[i]\n","        # NOTE: USE FAST FUNCTION\n","        act, _ = ??\n","        \n","\n","    # calculate last layer score by calling affine forward function using act, W[i+1], and b[i+1]\n","    last_layer, _ = ??\n","    \n","    # take the maximum prediction from the last layer and use that column to get the class     \n","    # use np.argmax with axis=-1 \n","    y_pred = ??\n","    \n","    return y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4CLZ4NS5fVz","colab_type":"text"},"source":["---\n","## 4 - Training Function\n","\n","Now on to the training function\n","\n","you should already be very familiar with this implementation"]},{"cell_type":"markdown","metadata":{"id":"io7ROjdIHXK3","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement Training Function**\n","\n","there are **four steps** in this training function\n","\n","---\n","\n","**1. Forward Pass**\n","\n","    * loop over hidden layer [for len(W)-1]\n","        * call layer forward function acording to the architecure\n","    * call affine forward function for the last layer\n","    * call softmax score function\n","    \n","\n","**2. Calculate Loss**\n","\n","    * call softmax_loss function\n","    * loop over weights [for W]\n","        * calculate loss with regularization\n","\n","\n","**3. Backward Pass**\n","\n","    * call affine backward function for the last layer\n","    * loop over hidden layer [from len(W)-2 to 0]\n","        * call layer backward function acording to the architecure\n","\n","**4. Weight Update**\n","\n","    * loop over weights [for W]\n","        * implement weight update\n","    * calculate the training and validation accuracy"]},{"cell_type":"code","metadata":{"id":"Jv9HnCvQcZ3_","colab_type":"code","colab":{}},"source":["def train(model, X, y, X_val, y_val,\n","          lr=1e-4, lr_decay=0.95, \n","          reg=0.001, epochs=100, \n","          batch_size=50, \n","          verbose=True, print_every=100):\n","\n","  \n","    \"\"\"\n","    Inputs:\n","    - model      : Network model architecture and weights\n","    - X          : Array, shape (N, C, H, W) of training images\n","    - y          : Array, shape (N,) of labels for training images\n","    - X_val      : Array, shape (N_val, C, H, W) of validation images\n","    - y_val      : Array, shape (N_val,) of labels for validation images  \n","    - lr         : float, initial learning rate\n","    - lr_decay   : float, 0-1, decay rate to reduce learning rate each epoch\n","    - reg        : float, regularization rate\n","    - epochs     : int, number of training epoch\n","    - batch_size : int, number of batch used each step\n","    - verbose    : boolean, verbosity\n","    \n","    Outputs:\n","    - model      : Network model architecture and weights\n","    - history    : list of training history [loss, train_acc, val_acc]\n","    \n","    \"\"\"\n","      \n","    \n","    # extract input size\n","    num_train, H_in, W_in, C_in = X.shape\n","    \n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    \n","    # get the model architecture\n","    architecture, W, b, params = model        \n","    \n","    # get the layer number (including output layer)\n","    n_layer = len(architecture)\n","    \n","    \n","    # assume y takes values 0...K-1 where K is number of classes\n","    num_classes = np.max(y) + 1\n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","    \n","    \n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","    \n","    dW = {}\n","    db = {}\n","    dx = {}\n","    \n","                     \n","    ## ------------------------- start your code here --------------------------\n","    \n","    print('start training')\n","    \n","    for ep in range(epochs):\n","      \n","        # Shuffle data train index\n","        train_rows = np.arange(num_train)\n","        np.random.shuffle(train_rows)\n","        \n","        # split index into mini batches\n","        id_batch = np.split(train_rows, num_iter)\n","  \n","        for batch in id_batch:\n","      \n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","        \n","            # store all cache in dictionary\n","            cache = {}\n","\n","            # first layer activation input is X_batch\n","            act = X_batch\n","\n","\n","            # ------------------------------------------------\n","            # 1. Forward Pass\n","            # ------------------------------------------------\n","\n","            # loop i over n_layer-1\n","            for i in range(n_layer-1):\n","              \n","                # get layer architecture                \n","                layer = architecture[i]\n","                \n","                \n","                # check layer type\n","                if layer[0]=='affine':\n","                  # call affine relu forward function with input act, W[i], and b[i]\n","                  act, cache[i] = ??\n","\n","                elif layer[0]=='conv':        \n","                  # call conv relu forward function with input act, W[i], b[i], and params[i]\n","                  act, cache[i] = ??\n","\n","                elif layer[0]=='pool':\n","                  # call pool forward fast function with input act and params[i]\n","                  # NOTE: USE FAST FUNCTION\n","                  act, cache[i] = ??\n","\n"," \n","            # calculate last layer score by calling affine forward function using activation act, W[i+1], and b[i+1] \n","            last_layer, cache[i+1] = ??\n","\n","            # calculate softmax score by calling softmax function using last_layer score\n","            softmax_score = ??\n","\n","            # ------------------------------------------------\n","            # 2. Calculate Loss\n","            # ------------------------------------------------\n","\n","            # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n","            loss, dout = ??\n","\n","            # add regularization to the loss:\n","            #    for each weights, calculate the sum square, multiply regularization strength\n","            #    then add it to the loss\n","            for w in W:\n","                loss += reg * np.sum(w*w)\n","\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","\n","            # ------------------------------------------------\n","            # 3. Backward Pass\n","            # ------------------------------------------------    \n","\n","            # dictionary to contain all gradients\n","            dW = {}\n","            db = {}\n","\n","            # calculate last weights gradient by calling affine backward function using dout and cache[n_layer-1]\n","            dact, dW[n_layer-1], db[n_layer-1] = ??\n","\n","            #loop i from n_layer-2 down to 0\n","            for i in range(n_layer-2,-1,-1):\n","      \n","                # get layer architecture                                \n","                layer = architecture[i]\n","\n","                # check layer type\n","                if layer[0]=='affine':\n","                  # call affine relu backward function with input dact and cache[i]\n","                  dact, dW[i], db[i] = ??\n","                  \n","                    \n","                elif layer[0]=='conv':\n","                  # call conv relu backward function with input dact and cache[i]\n","                  dact, dW[i], db[i] = ??\n","\n","\n","                elif layer[0]=='pool':\n","                  # call pool backward fast function with input dact and cache[i]\n","                  # NOTE: USE FAST FUNCTION\n","                  dact = ??\n","\n","\n","            # ------------------------------------------------\n","            # 4. Weight Update\n","            # ------------------------------------------------ \n","      \n","      \n","            # add regularization to the gradient:\n","            #    for each gradient dW, add with twice of the weight multiplied by regularization strength\n","            #    dwi = dwi + 2 * reg * wi\n","            # loop over W\n","            for i in W.keys():   \n","                dW[i] += 2 * reg * W[i]\n","\n","\n","            # perform parameter update by subtracting W[i] and b[i] for each layer with a fraction of dW[i] and db[i]\n","            # according to the learning rate\n","            # loop over W\n","            for i in W.keys():    \n","                W[i] -= lr * dW[i]\n","                b[i] -= lr * db[i]\n","\n","\n","\n","            # iteration count\n","            it +=1\n","\n","            if verbose and it % print_every == 1:\n","                print ('iteration',it,'/',num_iter,'\\t(epoch', ep+1,'/',epochs, '): \\tloss =', loss)\n","                   \n","                \n","        # record model at this epoch    \n","        model_epoch = (architecture, W, b, params) \n","        \n","        # At the end of one epoch\n","        # 1. Check accuracy\n","        #    calculate the training accuracy by calling predict function on model_epoch and X_batch\n","        #    and compare it to y_batch. Then calculate the mean correct (accuracy in range 0-1)\n","        train_acc = (predict(model_epoch, X_batch) == y_batch).mean()\n","        train_acc_history.append(train_acc)\n","\n","        # 2. Calculate the training accuracy by calling predict function on model_epoch and X_val\n","        #    and compare it to y_val. Then calculate the mean correct (accuracy in range 0-1)\n","        val_acc = (predict(model_epoch, X_val) == y_val).mean()\n","        val_acc_history.append(val_acc)\n","\n","        # 3. Decay learning rate\n","        #    multiply learning rate with decay\n","        #    see sigmoid train function\n","        lr *= ??\n","\n","        \n","    ## ------------------------- your code end here ----------------------------\n","    \n","        \n","    # store trained weights in model\n","    model = (architecture, W, b, params)      \n","    \n","    # store all trining history\n","    history = [loss_history, train_acc_history, val_acc_history]\n","    \n","    # return model and training history\n","    return model, history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zy8qnCa2YHCU","colab_type":"text"},"source":["---\n","---\n","# [Part 8] Train Deep ConvNet"]},{"cell_type":"markdown","metadata":{"id":"fZ-o7rE5EvfM","colab_type":"text"},"source":["---\n","## 1 - Define Model\n","Now let's try our training function\n","\n","we'll use the same the three-layer convnet architecture as before"]},{"cell_type":"code","metadata":{"id":"6MBya5BzcZ4B","colab_type":"code","colab":{}},"source":["# architecture options:\n","# input , input_size\n","# conv  , conv_size , num_filter, stride, pad\n","# pool  , pool_size , stride\n","# affine, hidden_size\n","\n","architecture =[\n","    ['input' , (32, 32, 3)],\n","    ['conv'  , (7, 7), 36, 1, 3],\n","    ['pool'  , (2, 2), 2],\n","    ['affine', 500],\n","    ['affine', 10]\n","]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIMq_H5dFA0d","colab_type":"text"},"source":["Now print the architecture list"]},{"cell_type":"code","metadata":{"id":"GkthWmlFFIqL","colab_type":"code","colab":{}},"source":["for i in range(len(architecture)):\n","    layer = architecture[i]\n","    print('layer',i,':',layer)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehyHE8M0FFAM","colab_type":"text"},"source":["next, initialize the model"]},{"cell_type":"code","metadata":{"id":"nCC4F3HEcZ4C","colab_type":"code","colab":{}},"source":["model = init_model(architecture)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Q4Iw1JhFUr_","colab_type":"text"},"source":["---\n","## 2 -  Train Model\n","\n","Now we train the model\n","\n","you should get similar result as before"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BNUtQPJgHFzM"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Use the Training Function and train a **three layer convolutional network** \n"]},{"cell_type":"code","metadata":{"id":"AC2SPhHPcZ4F","colab_type":"code","colab":{}},"source":["t1 = time()\n","model, history2 = train(model, X_train, y_train, X_val, y_val, \n","                       lr=1e-2,  lr_decay=0.95, \n","                       reg=0.005, epochs=1, batch_size=100, \n","                       verbose=True, print_every=20)\n","                                                     \n","t2 = time()\n","\n","\n","training_time = (t2-t1)/60\n","print('\\ntraining time: %0.2f minutes' % training_time)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GqcK7CcuQ3to"},"source":["**Expected Output**:\n","<pre>\n","loss should start around 2.3 and end around 1.6\n","in around 25 minutes of training for just 1 epoch"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X4IEaQfpQ3tu"},"source":["---\n","## 3 - Training Accuracy\n","Calculate the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"HcwNwK_UuGjt","colab_type":"code","colab":{}},"source":["loss, train_acc, val_acc = history2\n","\n","plt.rcParams['figure.figsize'] = (10, 6)\n","plt.plot(loss)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ftPLSiKnQ3tz","colab":{}},"source":["y_pred = predict(model, X_val)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy = %.1f%%' % (accuracy*100))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gHZPZ-z1Q3uB"},"source":["**Expected Output**:\n","\n","<pre>You should also be able to get about <b>~45%</b> accuracy on validation set</pre>\n"]},{"cell_type":"markdown","metadata":{"id":"KtiRuQqccZ4L","colab_type":"text"},"source":["---\n","---\n","# Congratulation, You've Completed Exercise 5\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aqhn3S3Ex8me"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}