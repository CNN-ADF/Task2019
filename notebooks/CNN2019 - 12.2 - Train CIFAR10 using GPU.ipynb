{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN2019 - 12.2 - Train CIFAR10 using GPU.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hPwFEN34mNUP","colab_type":"text"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sjlcHwermNUS","colab_type":"text"},"source":["\n","# Task 12 part 2/3 - Train CIFAR10 using GPU\n","\n","In this second part of the assignment you will try to train a five layer CNN on CIFAR-10 dataset using GPU and feel the increased training speed compared to only using CPU\n","\n","In this assignment you will also try to train the network using Data Augmentation\n","\n","\n","The goals of this assignment are as follows:\n","\n","<pre>* train a five layer CNN using GPU from scratch\n","* train a five layer CNN using Data Augmentation \n"]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1","colab_type":"text"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"id":"_AgIza0ZupR2","colab_type":"text"},"source":["---\n","## 0 - Acceleration Setting\n","\n","This time we will check the ability of TensorFlow if it is run with GPU acceleration.\n","\n","For that, make sure that this Google Colab use **GPU** Runtime acceleration.\n","* Select the Runtime menu\n","* Change Runtime Type\n","* Choose **GPU**\n","\n","<img src = \"https://i.ibb.co/QX3Brf0/gpu.png\" align = \"center\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3iIKvXgjjCn-","colab_type":"text"},"source":["---\n","## 1 - Install TensorFlow 2\n","\n","If Tensorflow 2 is not already installed, install it first"]},{"cell_type":"code","metadata":{"id":"VitP-OLAjCGz","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu -q"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOPPPUZXi5EX","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","tf.__version__"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aIpLk-Iej1RC"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," '2.0.0'"]},{"cell_type":"code","metadata":{"id":"HyeXRIycxxeH","colab_type":"code","colab":{}},"source":["import torch\n","torch.__version__"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BE765j8Ixynl"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," '1.3.0'"]},{"cell_type":"markdown","metadata":{"id":"DvPSXMEIaFm1","colab_type":"text"},"source":["---\n","## 2 - Import Libraries\n","Import required libraries"]},{"cell_type":"code","metadata":{"id":"4KOPbytzogWG","colab_type":"code","colab":{}},"source":["import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import pprint\n","\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import Model\n","from tensorflow.keras.models import Sequential\n","\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.applications.vgg16 import VGG16\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.utils import plot_model\n","\n","%matplotlib inline\n","np.set_printoptions(precision=7)\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi","colab_type":"text"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk","colab_type":"code","colab":{}},"source":["## --- start your code here ----\n","\n","NIM = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv","colab_type":"text"},"source":["---\n","## 3 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw","colab_type":"code","colab":{}},"source":["(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz","colab_type":"text"},"source":["---\n","## 4 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","colab_type":"code","cellView":"both","colab":{}},"source":["X_val_ori = X_train_ori[-10000:,:]\n","y_val     = y_train[-10000:]\n","\n","X_train_ori = X_train_ori[:-10000, :]\n","y_train     = y_train[:-10000]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4","colab_type":"text"},"source":["---\n","## 5 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","colab_type":"code","cellView":"both","colab":{}},"source":["X_train = X_train_ori.astype('float32')\n","X_val   = X_val_ori.astype('float32')\n","X_test  = X_test_ori.astype('float32')\n","\n","mean_image = X_train.mean(axis=(0, 1, 2), keepdims=True)\n","std_image = X_train.std(axis=(0, 1, 2), keepdims=True)\n","\n","X_train = (X_train - mean_image) /std_image\n","X_val = (X_val - mean_image) /std_image\n","X_test = (X_test - mean_image) /std_image\n","\n","X_train = X_train.astype('float32')\n","X_val = X_val.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n","\n","y_train = y_train.ravel()\n","y_val   = y_val.ravel()\n","y_test  = y_test.ravel()\n","\n","print('\\ny_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpPgnChbl8jj","colab_type":"text"},"source":["one hot the label"]},{"cell_type":"code","metadata":{"id":"neQU8KqDIn1_","colab_type":"code","colab":{}},"source":["y_train_hot = to_categorical(y_train, 10)\n","y_val_hot   = to_categorical(y_val, 10)\n","y_test_hot  = to_categorical(y_test, 10)\n","\n","print('y_train_hot.shape =',y_train_hot.shape)\n","print('y_val_hot.shape   =',y_val_hot.shape)\n","print('y_test_hot.shape  =',y_test_hot.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKAWDMHpkSjy","colab_type":"text"},"source":["---\n","## 6 - Helper Function"]},{"cell_type":"code","metadata":{"id":"6qS-rDCAQCjL","colab_type":"code","colab":{}},"source":["def plot_history(history):\n","  plt.rcParams['figure.figsize'] = [12, 4]\n","  plt.subplots_adjust(wspace=0.2)\n","\n","  plt.subplot(121)\n","  # Plot training & validation accuracy values\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('Model accuracy')\n","  plt.ylabel('Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Val'])\n","\n","  plt.subplot(122)\n","  # Plot training & validation loss values\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('Model loss')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Val'])\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pKghE730YdkF"},"source":["---\n","---\n","# [Part 1] Device Acceleration Checking"]},{"cell_type":"markdown","metadata":{"id":"3XY9l5ODpJp5","colab_type":"text"},"source":["## 1 - GPU Availability Check\n","\n","Next, let's check TensorFlow and PyTorch's ability to utilize GPU capabilities\n","\n","**Because we set this Google Colab to run with GPU acceleration, Tensorflow will return the GPU device id**\n","\n","**If we're using PyTorch to check the deivice, we can see the GPU name**"]},{"cell_type":"code","metadata":{"id":"_4a2I2ZKpE4q","colab_type":"code","colab":{}},"source":["print('Using GPU:', tf.test.is_gpu_available())\n","\n","if tf.test.is_gpu_available():\n","  print('GPU name :', tf.test.gpu_device_name())\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Q65ikAWtxXOz"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    Using GPU: True\n","    GPU name : /device:GPU:0"]},{"cell_type":"code","metadata":{"id":"4XeTg2E9oxaD","colab_type":"code","colab":{}},"source":["print('Using GPU:',torch.cuda.is_available())\n","\n","if torch.cuda.is_available():\n","  print('GPU name :',torch.cuda.get_device_name(0))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O0vI9QBPxsRD"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    Using GPU: True\n","    GPU name : Tesla K80"]},{"cell_type":"markdown","metadata":{"id":"afwrUonUphEu","colab_type":"text"},"source":["## 2 - TPU Availability Check\n","\n","Then, let's check TensorFlow's ability to utilize TPU capabilities \n","\n","**Because we set this Google Colab to run with GPU acceleration, there will be no TPU acceleration that can be used**"]},{"cell_type":"code","metadata":{"id":"C3wzT-jVoxXd","colab_type":"code","colab":{}},"source":["if 'COLAB_TPU_ADDR' not in os.environ:\n","  print('Not connected to a TPU runtime')\n","  \n","else:\n","  \n","  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","  print ('TPU address is', tpu_address)\n","\n","  # for tf 2.x\n","  # tf.config.experimental_connect_to_host(tpu_address)\n","  # devices=tf.config.experimental_list_devices()\n","  \n","  # for tf 1.x  \n","  with tf.Session(tpu_address) as session:\n","    devices = session.list_devices()\n","    \n","  print('TPU devices:') \n","  print(*devices, sep=\"\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IWNvMRIMyNHW"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Not connected to a TPU runtime"]},{"cell_type":"markdown","metadata":{"id":"fNIfgK0sMm9V","colab_type":"text"},"source":["---\n","---\n","# [Part 2] Train 5-layer ConvNet\n","\n","Now let's build and train our model\n","\n"]},{"cell_type":"code","metadata":{"id":"zbwP7s5tipIX","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, GlobalAveragePooling2D\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"53aIJsYp3b1x","colab_type":"text"},"source":["---\n","## 1 - Model Builder"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EJG9Bgo3zEzK"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Define a ConvNet Model as follow**\n","\n","    * 32 @ Conv 3x3 using relu and padding same\n","    * 32 @ Conv 3x3 using relu and padding same\n","    * Max Pool\n","    * 64 @ Conv 3x3 using relu and padding same    \n","    * Max Pool\n","    * Flatten\n","    * Dense 200 using relu\n","    * Dense 10 using softmax\n","    \n","    then compile using categorical crossentropy and optimizer adam\n"]},{"cell_type":"code","metadata":{"id":"2RaUR-Efi-kZ","colab_type":"code","colab":{}},"source":["def FiveConvNet(name):\n","  \n","  model = Sequential([\n","\n","      ??, # conv\n","      ??, # conv\n","      ??, # pool\n","      ??, # conv\n","      ??, # pool\n","      ??, # flatten\n","      ??, # dense\n","      ??  # dense\n","      \n","  ], name = name) \n","  \n","  model.compile(??, ??, metrics=['accuracy'])\n","  \n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyQKN3hJz0MH","colab_type":"text"},"source":["---\n","## 2 - Define Model"]},{"cell_type":"code","metadata":{"id":"a_GWjiaQz-mC","colab_type":"code","colab":{}},"source":["model = FiveConvNet('using_GPU')\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_1PLi1YP0Fjh"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"using_GPU\"\n","conv2d (Conv2D)              (None, 32, 32, 32)        896       \n","...\n","dense_? (Dense)              (None, 10)                2010        \n","\n","Total params: 850,050\n","Trainable params: 850,050\n","Non-trainable params: 0\n","_________________________________________________________________"]},{"cell_type":"markdown","metadata":{"id":"0BjRQudj0Qdg","colab_type":"text"},"source":["---\n","## 3 - Train Model\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Cqz2YhpC5yJx"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Train the model for 8 epochs using batch size 200**\n","\n","**Don't forget to feed the validation data**\n","\n","\n","    "]},{"cell_type":"code","metadata":{"id":"YTMMAyn2jKDm","colab_type":"code","colab":{}},"source":["import time\n","\n","num_epochs = ??\n","batch_size = ??\n","\n","tic = time.time()\n","\n","history = model.fit(??, ??, \n","                    ??,\n","                    ??, \n","                    ??, \n","                    verbose=2)\n","\n","toc = time.time()\n","\n","print('\\n\\ntraining speed = %.2f seconds' % (toc-tic))\n","print('training speed = %.2f minutes' % ((toc-tic)/60))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9zkCeIn71Pdi"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Your model should run in about 5 seconds per epoch\n","it should be about 30x faster than using CPU"]},{"cell_type":"markdown","metadata":{"id":"c4NYoYbd0guE","colab_type":"text"},"source":["---\n","## 4 - Visualize Training"]},{"cell_type":"code","metadata":{"id":"-gUP_tmW0eBZ","colab_type":"code","colab":{}},"source":["plot_history(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZRYQUCc70kuB","colab_type":"text"},"source":["---\n","## 5 - Evaluate Model"]},{"cell_type":"code","metadata":{"id":"UWby1QQhkeL-","colab_type":"code","colab":{}},"source":["val_scores   = model.evaluate(X_val, y_val_hot, verbose=2)\n","test_scores  = model.evaluate(X_test, y_test_hot, verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lNsYt9oG67vd"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","You should get around 72% accuracy on data test\n","tough the model is overfit"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9kmjelGV9v-5"},"source":["---\n","---\n","# [Part 3] Train ConvNet using Data Augmentation\n","\n","In this part, we'll use the same 5 layer convnet model, but let's train it using Data Augmentation\n","\n","We'll use the `ImageDataGenerator` function provided by `tf.keras`\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"raumBBzr-BWQ"},"source":["---\n","## 1 - Define Model\n","\n","Initiate new model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"45BrRww7-BWX","colab":{}},"source":["model_aug = FiveConvNet('Augmented')\n","\n","model_aug.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rX9g239_-SEA","colab_type":"text"},"source":["---\n","## 2 - Define Augmentation\n","\n","Keras already provided a set of function to automatically perform Data Augmentation on the run while we train the data. This way, we don't need to manually  modify and store the data\n","\n","you can read more detailed example [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n","\n","The first step is by defining what type of Augmentation we want to perform to our data"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Bj0-cQia-sKD"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Define your augmentation scheme**\n","\n","    Common setting:\n","    * width and height shift 0.1 to 0.2\n","    * horizontal flip true\n","    * vertical flip false\n","    * zoom 0.1 to 0.2\n","    * rotation\n","\n","\n","    "]},{"cell_type":"code","metadata":{"id":"D05R_2nBP57z","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","datagen = ImageDataGenerator(\n","    \n","    rotation_range=??,\n","    \n","    width_shift_range=??,\n","    \n","    height_shift_range=??,\n","    \n","    brightness_range=[??, ??],\n","    \n","    shear_range=??,\n","    \n","    zoom_range=??,\n","    \n","    channel_shift_range=??,\n","    \n","    horizontal_flip=??,\n","    \n","    vertical_flip=??,\n","    \n","    rescale=??,\n","    \n",")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRhhpJt6yIct","colab_type":"text"},"source":["Prepare the data generator to fit the train data"]},{"cell_type":"code","metadata":{"id":"GoWUUbGgRmzu","colab_type":"code","colab":{}},"source":["datagen.fit(X_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CJUIM1ZF-MzB"},"source":["---\n","## 3 - Train Model\n","\n","To train the model using Data Augmentation, we're not going to use `.fit()` function, \n","\n","but instead, we use `.fit_generator()` function\n","\n","when passing the data train to the `.fit_generator()`  function, we'll use `datagen.flow()` to perform the augmentation on the run\n","\n","the use is as follow\n","\n","```python\n","augmented = datagen.flow(data, target, batch_size=??)\n","\n","history = model.fit_generator( augmented, validation=??, epoch=??)\n","\n","```"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BHi7XjLm-MzJ"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Train the model for 8 epochs using batch size 200**\n","\n","**Don't forget to feed the validation data**\n","\n","\n","    "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r8KAegvU-MzN","colab":{}},"source":["import time\n","\n","num_epochs = ??\n","batch_size = ??\n","\n","tic = time.time()\n","\n","# call datagen.flow() function with input X_train, y_train_hot, and batch_size\n","augmented_train = datagen.flow(\n","    ??, ??, ??\n",")\n","\n","\n","# call model_aug.fit_generator() function with input augmented_train,\n","# validation_data = (X_val, y_val_hot), and epoch\n","history_aug = model_aug.fit_generator(\n","    ??,\n","    ??\n","    ??, \n","    verbose=2)\n","\n","toc = time.time()\n","\n","print('\\n\\ntraining speed = %.2f seconds' % (toc-tic))\n","print('training speed = %.2f minutes' % ((toc-tic)/60))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Qi_5S9-3-MzW"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Your model should run in about 30-50 seconds per epoch\n","depending on your Augmentation scheme"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b16As5eK-MzZ"},"source":["---\n","## 4 - Visualize Training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zi9RNiT8-Mza","colab":{}},"source":["plot_history(history_aug)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vXL99nar-Mzg"},"source":["---\n","## 5 - Evaluate Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-OavOiV_-Mzh","colab":{}},"source":["val_scores   = model_aug.evaluate(X_val, y_val_hot, verbose=2)\n","test_scores  = model_aug.evaluate(X_test, y_test_hot, verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XpJBd0vj-Mzm"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","You should get around 76% accuracy on data test\n","\n","You should also see that using Data Augmentation prevent Overfitting"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_LuEQe63EsWH"},"source":["---\n","---\n","# [Part 4] Train CIFAR10 using VGG\n","\n","Our 5-layer ConvNet already runs fast with 5 seconds per epoch\n","\n","Not much from here that can be accelerated anymore\n","\n","So instead, let's define a bigger model using VGG16 so we can later compare it with TPU accelerated computation\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s_64TzyVEsWY"},"source":["---\n","## 1 - Model Builder\n","\n","Load empty VGG16, cut it up to the 4th block, then add a Global Average Pooling and a couple of Dense Layer"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GDpHC2dOPD_P"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Define a ConvNet Model as follow**\n","\n","    * vgg16 model with weights=None, include_top=False, and input_shape=(32,32,3)\n","    * get layer 'block4_conv3' output\n","    * GlobalAveragePooling2D\n","    * Dense 200 using relu\n","    * Dense 10 using softmax\n","    \n","    use Functional API,\n","    then compile using categorical crossentropy and optimizer adam\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t53VFCqLEsWe","colab":{}},"source":["from tensorflow.keras.applications.vgg16 import VGG16\n","\n","def CifarVGG(name):\n","  \n","  # load empty vgg16 no top with input (32,32,3)\n","  model = VGG16(weights=None, include_top=False, input_shape=(32, 32, 3))\n","\n","  # get 'block4_conv3' output\n","  x = model.get_layer('block4_conv3').output\n","  \n","  # global average pooling\n","  x = ??\n","\n","  # dense 200\n","  x = ??\n","\n","  # dense 10\n","  prediction = ??\n","  \n","  # instantiate model\n","  myModel = Model(inputs=model.input, outputs=prediction, name=name)\n","\n","  myModel.compile(??,  ??, metrics=['accuracy'])\n","  \n","  return myModel\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"byzN39tEEsWj"},"source":["---\n","## 2 - Define Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QgS0fYAYEsWk","colab":{}},"source":["model = CifarVGG('VGG_using_GPU')\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"smukVtarPhxZ"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"VGG_using_GPU\"\n","input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n","...\n","dense_? (Dense)              (None, 200)               102600    \n","prediction (Dense)           (None, 10)                2010      \n","\n","Total params: 7,739,874\n","Trainable params: 7,739,874\n","Non-trainable params: 0\n","_________________________________________________________________"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vqR50hkmEsWr"},"source":["---\n","## 3 - Train Model\n","\n","Now train the model for 10 epoch\n","\n","For greater comparison, we'll use batch size of 1000\n","\n","But this time, notice that we're not passing validation set\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4C5ZSb50EsWu","colab":{}},"source":["import time\n","\n","num_epochs = 10\n","batch_size = 1000\n","\n","tic = time.time()\n","\n","history = model.fit(X_train, y_train_hot, \n","                    epochs=num_epochs, \n","                    batch_size=batch_size, \n","                    verbose=2)\n","\n","toc = time.time()\n","\n","print('\\n\\ntraining speed = %.2f seconds' % (toc-tic))\n","print('training speed = %.2f minutes' % ((toc-tic)/60))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IuZeH70eEsW0"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Your model should run in about 20 seconds per epoch"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8xOOW3g0EsW7"},"source":["---\n","## 4 - Evaluate Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"suf2M5ZjEsW8","colab":{}},"source":["val_scores   = model.evaluate(X_val, y_val_hot, verbose=2)\n","test_scores  = model.evaluate(X_test, y_test_hot, verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"t29orKJ8EsXC"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","You should get around 70% accuracy on data test\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 12 part 2/3\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bvBk-u7_92ub"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}